{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8069a5d6",
   "metadata": {},
   "source": [
    "# CIFAR10 EfficientNetB7-ResidualMLP Neural Architecture Search\n",
    "\n",
    "## Evaluation of best model from 2022-02-12_14_48 run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af99681f",
   "metadata": {},
   "source": [
    "### For a description of the ResidualMLP and my neural architecture search algorithym, refer to: https://github.com/david-thrower/residual_MLP_Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc6571",
   "metadata": {},
   "source": [
    "## The following cells are the NAS and hyperparameter search task run:\n",
    "\n",
    "File 1: ### The task trigger, 'ultraparameters', and hyperparameter search space for the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b9cb717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"Runs task.py, is a more convenient way to set up the parameters than\r\n",
      "manually writing a shell script.\"\"\"\r\n",
      "\r\n",
      "import subprocess\r\n",
      "preliminary_install_commands = [\"pip3 install --upgrade pip\",\r\n",
      "                                \"pip3 install pendulum\"]\r\n",
      "for cmd in preliminary_install_commands:\r\n",
      "    subprocess.run(cmd,\r\n",
      "                   shell=True,\r\n",
      "                   check=True)\r\n",
      "import pendulum\r\n",
      "\r\n",
      "# Configure the run with this dict.\r\n",
      "# Always enter boolean and floats as strings.\r\n",
      "\r\n",
      "TIME_STAMP = pendulum.now().\\\r\n",
      "                    \t__str__().\\\r\n",
      "                    \treplace('T','_').\\\r\n",
      "                    \treplace(':','-')[:16]\r\n",
      "\r\n",
      "HPARAMS = {\r\n",
      "    'project_name':\"CIFAR10_EfficientNetB7-ResidualMLP_NAS_THIRD_PASS_SEARCH\",\r\n",
      "    'problem_type':\"classification\",\r\n",
      "    'number_of_classes':10,\r\n",
      "    'training_set_size':50000,\r\n",
      "    'patience':25,\r\n",
      "    'patience_min_delta':0.00001,\r\n",
      "    'batch_size':50,\r\n",
      "    'max_epochs':150,\r\n",
      "    'minimum_learning_rate':0.0001,\r\n",
      "    'maximum_learning_rate':0.02,\r\n",
      "    'number_of_learning_rates_to_try':7,\r\n",
      "    'minimum_number_of_blocks':2,\r\n",
      "    'maximum_number_of_blocks':3,\r\n",
      "    'minimum_number_of_layers_per_block':2,\r\n",
      "    'maximum_number_of_layers_per_block':4,\r\n",
      "    'minimum_neurons_per_block_layer':70,\r\n",
      "    'maximum_neurons_per_block_layer':140,\r\n",
      "    'n_options_of_neurons_per_layer_to_try':7,\r\n",
      "    'minimum_neurons_per_block_layer_decay':15,\r\n",
      "    'maximum_neurons_per_block_layer_decay':35,\r\n",
      "    'minimum_dropout_rate_for_bypass_layers':0.2,\r\n",
      "    'maximim_dropout_rate_for_bypass_layers':.6,\r\n",
      "    'n_options_dropout_rate_for_bypass_layers':3,\r\n",
      "    'minimum_inter_block_layers_per_block':0,\r\n",
      "    'maximum_inter_block_layers_per_block':85,\r\n",
      "    'n_options_inter_block_layers_per_block':7,\r\n",
      "    'minimum_dropout_rate':0.2,\r\n",
      "    'maximum_dropout_rate':.6,\r\n",
      "    'n_options_dropout_rate':5,\r\n",
      "    'minimum_final_dense_layers':33,\r\n",
      "    'maximum_final_dense_layers':300,\r\n",
      "    'n_options_final_dense_layers':7\r\n",
      "}\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    \r\n",
      "    BASE_FILE_NAME = f\"{TIME_STAMP}_CIFAR10_EfficientNetB7-ResidualMLP_NAS_SECOND_PASS_SEARCH\"\r\n",
      "    SHELL_LOGS_FILE_NAME = f\"{BASE_FILE_NAME}_python3_shell_log.txt\"\r\n",
      "    SHELL_SCRIPT_NAME = f\"{BASE_FILE_NAME}_task_trigger.sh\"\r\n",
      "    \r\n",
      "\r\n",
      "    back_slash = \"\\\\\"\r\n",
      "    shell_script_content = f\"python3 task.py {back_slash}\" + \"\\n\"\r\n",
      "    \r\n",
      "    for key, value in HPARAMS.items():\r\n",
      "        shell_script_content += f\"    --{key} '{value}'{back_slash}\" + \"\\n\"\r\n",
      "    shell_script_content = shell_script_content[:-2] + \"\\n\"\r\n",
      "    print(shell_script_content)\r\n",
      "\r\n",
      "    \r\n",
      "    with open(SHELL_SCRIPT_NAME,'w',encoding=\"utf8\") as f:\r\n",
      "        f.write(shell_script_content)\r\n",
      "    \r\n",
      "    command = f\"sh {SHELL_SCRIPT_NAME} >> {SHELL_LOGS_FILE_NAME} &\"\r\n",
      "    subprocess.run(command,\r\n",
      "                   shell=True,\r\n",
      "                   check=True)"
     ]
    }
   ],
   "source": [
    "! cat task_trigger.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90b40d1",
   "metadata": {},
   "source": [
    "## File 2: The parameterized training task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66489a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\r\n",
      "import subprocess\r\n",
      "import argparse\r\n",
      "# Add parser for common params\r\n",
      "\r\n",
      "try:\r\n",
      "    import pendulum\r\n",
      "except ModuleNotFoundError as not_found:\r\n",
      "    print(\"We're missing a module:\")\r\n",
      "    print(not_found)\r\n",
      "    print(\"No problem, we install it...\")\r\n",
      "    subprocess.run(\"pip3 install pendulum\", \r\n",
      "    \t           shell=True, \r\n",
      "    \t           check=True)\r\n",
      "    import pendulum\r\n",
      "\r\n",
      "try:\r\n",
      "    import pandas as pd\r\n",
      "except ModuleNotFoundError as not_found:\r\n",
      "    print(\"We're missing a module:\")\r\n",
      "    print(not_found)\r\n",
      "    print(\"No problem, we install it...\")\r\n",
      "    subprocess.run(\"pip3 install pandas\", \r\n",
      "    \t           shell=True, \r\n",
      "    \t           check=True)\r\n",
      "    import pandas as pd\r\n",
      "\r\n",
      "try:\r\n",
      "    import keras_tuner as kt\r\n",
      "except ModuleNotFoundError as not_found:\r\n",
      "    print(\"We're missing a module:\")\r\n",
      "    print(not_found)\r\n",
      "    print(\"No problem, we install it...\")\r\n",
      "    subprocess.run(\"pip3 install keras_tuner\", \r\n",
      "    \t           shell=True, \r\n",
      "    \t           check=True)\r\n",
      "    import keras_tuner as kt\r\n",
      "\r\n",
      "\r\n",
      "try:\r\n",
      "    import tensorflow as tf\r\n",
      "except ModuleNotFoundError as not_found:\r\n",
      "    print(\"We're missing a module:\")\r\n",
      "    print(not_found)\r\n",
      "    print(\"No problem, we install it...\")\r\n",
      "    subprocess.run(\"pip3 install tensorflow\", \r\n",
      "    \t           shell=True, \r\n",
      "    \t           check=True)\r\n",
      "    import tensorflow as tf\r\n",
      "\r\n",
      "from residualmlp.residual_mlp import ResidualMLP\r\n",
      "\r\n",
      "parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "    \"--project_name\", type=str, \r\n",
      "    help=\"Name for this project.\",\r\n",
      "    default=\"CIFAR10_EfficientNetB7-ResidualMLP_NAS_AUGMENTED_SPACE\")\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "    \"--number_of_classes\", \r\n",
      "    type = int,\r\n",
      "    help=\"Number of categorical classes. If a regression or \"\r\n",
      "    \"logstic regression problem set this to 1.\",\r\n",
      "    default=10)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "    \"--problem_type\",\r\n",
      "    type=str,\r\n",
      "    help=\"'classification' | 'regression'. \"\r\n",
      "         \"Default:'classification'\",\r\n",
      "    default=\"classification\")\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "    \"--training_set_size\", \r\n",
      "    type=int,\r\n",
      "    help=\"Training set size (how many observations).\",\r\n",
      "    default=50000)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "    \"--patience\",\r\n",
      "    type=int,\r\n",
      "    help=\"How many epochs with no improved performance before the \"\r\n",
      "        \"early stopping callback stops further training?\",\r\n",
      "    default=25)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "    \"--patience_min_delta\",\r\n",
      "    type=float,\r\n",
      "    help=\"How sensitive should the early stopping callback be\"\r\n",
      "        \"  to change?\",\r\n",
      "    default=0.00001)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "    \"--batch_size\",\r\n",
      "    type=int,\r\n",
      "    help=\"How many observations to train with...\",\r\n",
      "    default=50)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "    \"--max_epochs\",\r\n",
      "    type=int,\r\n",
      "    help=\"max_epochs: Integer, the maximum number of epochs to train one \"\r\n",
      "         \"model. It is recommended to set this to a value slightly higher \"\r\n",
      "         \"than the expected epochs to convergence for your largest Model, \"\r\n",
      "         \"and to use early stopping during training (for example, via .\",\r\n",
      "         default=50)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--minimum_learning_rate\",\r\n",
      "        type=float,\r\n",
      "        help=\"Lowest learning rate to try?\",\r\n",
      "        default = 0.00007)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--maximum_learning_rate\",\r\n",
      "        type=float,\r\n",
      "        help=\"Highest learning rate to try?\",\r\n",
      "        default = 0.7)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--number_of_learning_rates_to_try\",\r\n",
      "        type=int,\r\n",
      "        help=\"How many learning rate to try (maximum)?\",\r\n",
      "        default = 7)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--minimum_number_of_blocks\",\r\n",
      "        type=int,\r\n",
      "        help=\"Minimum number of ResidualMLP blocks in neural architectures \"\r\n",
      "             \"to try?\",\r\n",
      "        default = 1)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--maximum_number_of_blocks\",\r\n",
      "        type=int,\r\n",
      "        help=\"Maximum number of ResidualMLP blocks in neural architectures \"\r\n",
      "              \"to try?\",\r\n",
      "        default = 8)\r\n",
      "\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--minimum_number_of_layers_per_block\",\r\n",
      "        type=int,\r\n",
      "        help=\"Minimum number of layers to try in each ResidualMLP block in \"\r\n",
      "             \"the neural architectures to try?\",\r\n",
      "        default = 1)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--maximum_number_of_layers_per_block\",\r\n",
      "        type=int,\r\n",
      "        help=\"Maximum number of layers to try in each ResidualMLP block in \"\r\n",
      "             \"the neural architectures to try?\",\r\n",
      "        default = 8)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--minimum_neurons_per_block_layer\",\r\n",
      "        type=int,\r\n",
      "        help=\"Minimum number of neurons to try in the Dense layers in \"\r\n",
      "             \"each ResidualMLP block in the neural architectures to tried?\",\r\n",
      "        default = 30)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--maximum_neurons_per_block_layer\",\r\n",
      "        type=int,\r\n",
      "        help=\"Maximum number of neurons to try in the Dense layers in \"\r\n",
      "             \"each ResidualMLP block in the neural architectures to tried?\",\r\n",
      "        default = 130)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--n_options_of_neurons_per_layer_to_try\",\r\n",
      "        type=int,\r\n",
      "        help=\"How many different numbers of neurons (at most) to try in \"\r\n",
      "             \"the Dense layers try in each ResidualMLP block in the neural \"\r\n",
      "             \"architectures to tried?\",\r\n",
      "        default = 7)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--minimum_neurons_per_block_layer_decay\",\r\n",
      "        type=int,\r\n",
      "        help=\"Lowest decay in number of neurons (n less neurons than the \"\r\n",
      "             \"last layer) to try in the Dense layers try in each \"\r\n",
      "             \"ResidualMLP block in the neural \"\r\n",
      "             \"architectures to tried?\",\r\n",
      "        default = 0)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--maximum_neurons_per_block_layer_decay\",\r\n",
      "        type=int,\r\n",
      "        help=\"Highest decay in number of neurons (n less neurons than the \"\r\n",
      "             \"last layer) to try in the Dense layers try in each \"\r\n",
      "             \"ResidualMLP block in the neural \"\r\n",
      "             \"architectures to tried?\",\r\n",
      "        default = 50)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--minimum_dropout_rate_for_bypass_layers\",\r\n",
      "        type=float,\r\n",
      "        help=\"Lowest dropout rate to try for dropout layers located in \"\r\n",
      "             \"residual byass layers?\",\r\n",
      "        default = 0.01)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--maximim_dropout_rate_for_bypass_layers\",\r\n",
      "        type=float,\r\n",
      "        help=\"Highest dropout rate to try for dropout layers located in \"\r\n",
      "             \"residual byass layers?\",\r\n",
      "        default = 0.7)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--n_options_dropout_rate_for_bypass_layers\",\r\n",
      "        type=int,\r\n",
      "        help=\"How many dropout rates to try for dropout layers located in \"\r\n",
      "             \"residual byass layers?\",\r\n",
      "        default = 7)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--minimum_inter_block_layers_per_block\",\r\n",
      "        type=int,\r\n",
      "        help=\"Minimum number of neurons per Dense layer for Dense layers \"\r\n",
      "             \"inserted between ResidualMLP blocks? 0 Will not insert a \"\r\n",
      "             \"layer if selected.\",\r\n",
      "        default = 0)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--maximum_inter_block_layers_per_block\",\r\n",
      "        type=int,\r\n",
      "        help=\"Maximum number of neurons per Dense layer for Dense layers \"\r\n",
      "             \"inserted between ResidualMLP blocks? 0 Will not insert a \"\r\n",
      "             \"layer if selected.\",\r\n",
      "        default = 150)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--n_options_inter_block_layers_per_block\",\r\n",
      "        type=int,\r\n",
      "        help=\"How many different numbers of neurons per Dense layer for \"\r\n",
      "             \"Dense layers inserted between ResidualMLP blocks will we try \"\r\n",
      "             \"(at most)?\",\r\n",
      "        default = 7)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--minimum_dropout_rate\",\r\n",
      "        type=float,\r\n",
      "        help=\"Minimum dropout rate for final Dense layers?\",\r\n",
      "        default = 0.01)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--maximum_dropout_rate\",\r\n",
      "        type=float,\r\n",
      "        help=\"Maximum dropout rate for final Dense layers?\",\r\n",
      "        default = 0.7)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--n_options_dropout_rate\",\r\n",
      "        type=int,\r\n",
      "        help=\"How many dropout rates (at max) to try for final Dense layers?\",\r\n",
      "        default = 7)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--minimum_final_dense_layers\",\r\n",
      "        type=int,\r\n",
      "        help=\"Lowest number of neurons to try for final Dense layers, after \"\r\n",
      "             \"the last ResidualMLP block, before the very last Dense layer \"\r\n",
      "             \"returning an output? \"\r\n",
      "             \"(0 doesn't create a layer')\",\r\n",
      "        default = 0)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--maximum_final_dense_layers\",\r\n",
      "        type=int,\r\n",
      "        help=\"Highest number of neurons to try for final Dense layers, after \"\r\n",
      "             \"the last ResidualMLP block, before the very last Dense layer \"\r\n",
      "             \"returning an output? \"\r\n",
      "             \"(0 doesn't create a layer')\",\r\n",
      "        default = 150)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--n_options_final_dense_layers\",\r\n",
      "        type=int,\r\n",
      "        help=\"How many options for neurons to try for final Dense layers, \"\r\n",
      "             \"after the last ResidualMLP block, before the very last \"\r\n",
      "             \"Dense layer returning an output?\",\r\n",
      "        default = 7)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "args, _ = parser.parse_known_args()\r\n",
      "hparams = args.__dict__\r\n",
      "\r\n",
      "# Boilerplate args\r\n",
      "\r\n",
      "DATE = pendulum.now().__str__()[:16].replace(\"T\",\"_\").replace(\":\",\"_\")\r\n",
      "PROJECT_NAME = hparams[\"project_name\"]\r\n",
      "TRAINING_SET_SIZE = hparams[\"training_set_size\"]\r\n",
      "NUMBER_OF_CLASSES = hparams[\"number_of_classes\"]\r\n",
      "PROBLEM_TYPE = hparams[\"problem_type\"]\r\n",
      "\r\n",
      "# Keras tuner search & fit args\r\n",
      "\r\n",
      "PATIENCE = hparams[\"patience\"]\r\n",
      "PATIENCE_MIN_DELTA = hparams[\"patience_min_delta\"]\r\n",
      "BATCH_SIZE = hparams[\"batch_size\"]\r\n",
      "MAX_EPOCHS = hparams[\"max_epochs\"]\r\n",
      "RESULTS_DIR_FOR_SEARCH =\\\r\n",
      "    f'{DATE}_{PROJECT_NAME}_SEARCH_RUN'\r\n",
      "\r\n",
      "\r\n",
      "# Base model args\r\n",
      "\r\n",
      "BASE_MODEL_INPUT_SHAPE = (600,600,3)\r\n",
      "\r\n",
      "# ResidualMLP model args\r\n",
      "\r\n",
      "INPUT_SHAPE = (32,32,3)\r\n",
      "MINIMUM_LEARNING_RATE = hparams[\"minimum_learning_rate\"]\r\n",
      "MAXIMUM_LEARNING_RATE = hparams[\"maximum_learning_rate\"]\r\n",
      "NUMBER_OF_LEARNING_RATES_TO_TRY = hparams[\"number_of_learning_rates_to_try\"]\r\n",
      "MINIMUM_NUMBER_OF_BLOCKS = hparams[\"minimum_number_of_blocks\"]\r\n",
      "MAXIMUM_NUMBER_OF_BLOCKS = hparams[\"maximum_number_of_blocks\"]\r\n",
      "MINIMUM_NUMBER_OF_LAYERS_PER_BLOCK =\\\r\n",
      "    hparams[\"minimum_number_of_layers_per_block\"]\r\n",
      "MAXIMUM_NUMBER_OF_LAYERS_PER_BLOCK =\\\r\n",
      "    hparams[\"maximum_number_of_layers_per_block\"]\r\n",
      "MINIMUM_NEURONS_PER_BLOCK_LAYER = hparams[\"minimum_neurons_per_block_layer\"]\r\n",
      "MAXIMUM_NEURONS_PER_BLOCK_LAYER = hparams[\"maximum_neurons_per_block_layer\"]\r\n",
      "N_OPTIONS_OF_NEURONS_PER_LAYER_TO_TRY =\\\r\n",
      "    hparams[\"n_options_of_neurons_per_layer_to_try\"]\r\n",
      "MINIMUM_NEURONS_PER_BLOCK_LAYER_DECAY =\\\r\n",
      "    hparams[\"minimum_neurons_per_block_layer_decay\"]\r\n",
      "MAXIMUM_NEURONS_PER_BLOCK_LAYER_DECAY =\\\r\n",
      "    hparams[\"maximum_neurons_per_block_layer_decay\"]\r\n",
      "MINIMUM_DROPOUT_RATE_FOR_BYPASS_LAYERS =\\\r\n",
      "    hparams[\"minimum_dropout_rate_for_bypass_layers\"]\r\n",
      "MAXIMIM_DROPOUT_RATE_FOR_BYPASS_LAYERS =\\\r\n",
      "    hparams[\"maximim_dropout_rate_for_bypass_layers\"]\r\n",
      "N_OPTIONS_DROPOUT_RATE_FOR_BYPASS_LAYERS =\\\r\n",
      "    hparams[\"n_options_dropout_rate_for_bypass_layers\"]\r\n",
      "MINIMUM_INTER_BLOCK_LAYERS_PER_BLOCK =\\\r\n",
      "    hparams[\"minimum_inter_block_layers_per_block\"]\r\n",
      "MAXIMUM_INTER_BLOCK_LAYERS_PER_BLOCK =\\\r\n",
      "    hparams[\"maximum_inter_block_layers_per_block\"]\r\n",
      "N_OPTIONS_INTER_BLOCK_LAYERS_PER_BLOCK =\\\r\n",
      "    hparams[\"n_options_inter_block_layers_per_block\"]\r\n",
      "MINIMUM_DROPOUT_RATE = hparams[\"minimum_dropout_rate\"]\r\n",
      "MAXIMUM_DROPOUT_RATE = hparams[\"maximum_dropout_rate\"]\r\n",
      "N_OPTIONS_DROPOUT_RATE = hparams[\"n_options_dropout_rate\"]\r\n",
      "MINIMUM_FINAL_DENSE_LAYERS = hparams[\"minimum_final_dense_layers\"]\r\n",
      "MAXIMUM_FINAL_DENSE_LAYERS = hparams[\"maximum_final_dense_layers\"]\r\n",
      "N_OPTIONS_FINAL_DENSE_LAYERS = hparams[\"n_options_final_dense_layers\"]\r\n",
      "\r\n",
      "# CIFAR10_EfficientNetB7-ResidualMLP_NAS\r\n",
      "\r\n",
      "RESULTS_DIR_FOR_FINAL_MODEL =\\\r\n",
      "    f'{DATE}_{PROJECT_NAME}_FINAL_MODEL'\r\n",
      "\r\n",
      "FINAL_MODEL_PATH =\\\r\n",
      "    f\"{DATE}_{PROJECT_NAME}_FINAL_EXPORTED_MODEL\"\r\n",
      "\r\n",
      "# Print header info to shell script logs:\r\n",
      "\r\n",
      "print(\"# To reproduce the environment the NAS task was run in, copy and paste \"\r\n",
      "      \"the following information into the evaluation script environent \"\r\n",
      "      \"(excluding this line). PLEASE NOTE!!: If you do not reporduce these \"\r\n",
      "      \"variables, the hyperband will not reproduce the correct model!\")    \r\n",
      "print(f\"Date = {DATE}\")\r\n",
      "print(f\"PROBLEM_TYPE = {PROBLEM_TYPE}\")\r\n",
      "print(f\"NUMBER_OF_CLASSES = {NUMBER_OF_CLASSES}\")\r\n",
      "print(f\"INPUT_SHAPE = {INPUT_SHAPE}\")\r\n",
      "print(f\"BASE_MODEL_INPUT_SHAPE = {BASE_MODEL_INPUT_SHAPE}\")\r\n",
      "print(f\"PROJECT_NAME = {PROJECT_NAME}\")\r\n",
      "print(f\"TRAINING_SET_SIZE = {TRAINING_SET_SIZE}\")\r\n",
      "print(f\"PATIENCE = {PATIENCE}\")\r\n",
      "print(f\"PATIENCE_MIN_DELTA = {PATIENCE_MIN_DELTA}\")\r\n",
      "print(f\"BATCH_SIZE = {BATCH_SIZE}\")\r\n",
      "print(f\"MAX_EPOCHS = {MAX_EPOCHS}\")\r\n",
      "print(f\"RESULTS_DIR_FOR_SEARCH = {RESULTS_DIR_FOR_SEARCH}\")\r\n",
      "print(f\"MINIMUM_LEARNING_RATE = {MINIMUM_LEARNING_RATE}\")\r\n",
      "print(f\"MAXIMUM_LEARNING_RATE = {MAXIMUM_LEARNING_RATE}\")\r\n",
      "print(f\"NUMBER_OF_LEARNING_RATES_TO_TRY = {NUMBER_OF_LEARNING_RATES_TO_TRY}\")\r\n",
      "print(f\"MINIMUM_NUMBER_OF_BLOCKS = {MINIMUM_NUMBER_OF_BLOCKS}\")\r\n",
      "print(f\"MAXIMUM_NUMBER_OF_BLOCKS = {MAXIMUM_NUMBER_OF_BLOCKS}\")\r\n",
      "print(\"MINIMUM_NUMBER_OF_LAYERS_PER_BLOCK = \"\r\n",
      "      f\"{MINIMUM_NUMBER_OF_LAYERS_PER_BLOCK}\")\r\n",
      "print(\"MAXIMUM_NUMBER_OF_LAYERS_PER_BLOCK = \"\r\n",
      "      f\"{MAXIMUM_NUMBER_OF_LAYERS_PER_BLOCK}\")\r\n",
      "print(f\"MINIMUM_NEURONS_PER_BLOCK_LAYER = {MINIMUM_NEURONS_PER_BLOCK_LAYER}\")\r\n",
      "print(f\"MAXIMUM_NEURONS_PER_BLOCK_LAYER = {MAXIMUM_NEURONS_PER_BLOCK_LAYER}\")\r\n",
      "print(\"N_OPTIONS_OF_NEURONS_PER_LAYER_TO_TRY = \"\r\n",
      "      f\"{N_OPTIONS_OF_NEURONS_PER_LAYER_TO_TRY}\")\r\n",
      "print(\"MINIMUM_NEURONS_PER_BLOCK_LAYER_DECAY = \"\r\n",
      "      f\"{MINIMUM_NEURONS_PER_BLOCK_LAYER_DECAY}\")\r\n",
      "print(\"MAXIMUM_NEURONS_PER_BLOCK_LAYER_DECAY = \"\r\n",
      "      f\"{MAXIMUM_NEURONS_PER_BLOCK_LAYER_DECAY}\")\r\n",
      "print(\"MINIMUM_DROPOUT_RATE_FOR_BYPASS_LAYERS = \"\r\n",
      "      f\"{MINIMUM_DROPOUT_RATE_FOR_BYPASS_LAYERS}\")\r\n",
      "print(\"MAXIMIM_DROPOUT_RATE_FOR_BYPASS_LAYERS = \"\r\n",
      "      f\"{MAXIMIM_DROPOUT_RATE_FOR_BYPASS_LAYERS}\")\r\n",
      "print(\"N_OPTIONS_DROPOUT_RATE_FOR_BYPASS_LAYERS = \"\r\n",
      "      f\"{N_OPTIONS_DROPOUT_RATE_FOR_BYPASS_LAYERS}\")\r\n",
      "print(\"MINIMUM_INTER_BLOCK_LAYERS_PER_BLOCK = \"\r\n",
      "      f\"{MINIMUM_INTER_BLOCK_LAYERS_PER_BLOCK}\")\r\n",
      "print(\"MAXIMUM_INTER_BLOCK_LAYERS_PER_BLOCK = \"\r\n",
      "      f\"{MAXIMUM_INTER_BLOCK_LAYERS_PER_BLOCK}\")\r\n",
      "print(\"N_OPTIONS_INTER_BLOCK_LAYERS_PER_BLOCK = \"\r\n",
      "      f\"{N_OPTIONS_INTER_BLOCK_LAYERS_PER_BLOCK}\")\r\n",
      "print(f\"MINIMUM_DROPOUT_RATE = {MINIMUM_DROPOUT_RATE}\")\r\n",
      "print(f\"MAXIMUM_DROPOUT_RATE = {MAXIMUM_DROPOUT_RATE}\")\r\n",
      "print(f\"N_OPTIONS_DROPOUT_RATE = {N_OPTIONS_DROPOUT_RATE}\")\r\n",
      "print(f\"MINIMUM_FINAL_DENSE_LAYERS = {MINIMUM_FINAL_DENSE_LAYERS}\")\r\n",
      "print(f\"MAXIMUM_FINAL_DENSE_LAYERS = {MAXIMUM_FINAL_DENSE_LAYERS}\")\r\n",
      "print(f\"N_OPTIONS_FINAL_DENSE_LAYERS = {N_OPTIONS_FINAL_DENSE_LAYERS}\")\r\n",
      "\r\n",
      "print(\"\"\"\r\n",
      "\r\n",
      "cifar = tf.keras.datasets.cifar10.load_data()\r\n",
      "(x_train, y_train), (x_test, y_test) = cifar\r\n",
      "\r\n",
      "y_train_ohe = tf.one_hot([i[0] for i in  y_train],10)\r\n",
      "indexes_for_rows = tf.range(0,y_train.shape[0])\r\n",
      "shuffled_indexes = tf.random.shuffle(indexes_for_rows)\r\n",
      "selected_indexes = shuffled_indexes[:TRAINING_SET_SIZE]\r\n",
      "selected_x_train = x_train[selected_indexes,:,:,:]\r\n",
      "selected_y_train_ohe = y_train_ohe.numpy()[selected_indexes,:]\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "mod_with_fc_raw = tf.keras.applications.efficientnet.EfficientNetB7(\r\n",
      "    include_top=True, weights='imagenet', input_tensor=None,\r\n",
      "    input_shape = BASE_MODEL_INPUT_SHAPE, pooling='max', classes=1000\r\n",
      ")\r\n",
      "\r\n",
      "# Make the deepest conv2d layer trainable, leave everything else\r\n",
      "# as not trainable\r\n",
      "for layer in mod_with_fc_raw.layers:\r\n",
      "    layer.trainable = False\r\n",
      "# Last conv2d layer. This we want to train .\r\n",
      "mod_with_fc_raw.layers[-6].trainable = True\r\n",
      "\r\n",
      "# Create the final base model\r\n",
      "# (remove the final Dense and BatchNormalization layers ...) \r\n",
      "efficient_net_b_7_transferable_base_model =\\\r\n",
      "    tf.keras.Model(inputs=mod_with_fc_raw.layers[0].input, \r\n",
      "                    outputs=mod_with_fc_raw.layers[-3].output)\r\n",
      "\r\n",
      "model_builder = ResidualMLP(\r\n",
      "                    problem_type = PROBLEM_TYPE,\r\n",
      "                    minimum_learning_rate = MINIMUM_LEARNING_RATE, \r\n",
      "                    maximum_learning_rate = MAXIMUM_LEARNING_RATE, \r\n",
      "                    number_of_learning_rates_to_try =\r\n",
      "                        NUMBER_OF_LEARNING_RATES_TO_TRY, \r\n",
      "                    input_shape = INPUT_SHAPE, \r\n",
      "                    bw_images = False, \r\n",
      "                    base_model = \r\n",
      "                        efficient_net_b_7_transferable_base_model, \r\n",
      "                    base_model_input_shape = BASE_MODEL_INPUT_SHAPE, \r\n",
      "                    flatten_after_base_model = False, \r\n",
      "                    minimum_number_of_blocks = MINIMUM_NUMBER_OF_BLOCKS, \r\n",
      "                    maximum_number_of_blocks = MAXIMUM_NUMBER_OF_BLOCKS, \r\n",
      "                    minimum_number_of_layers_per_block =\r\n",
      "                        MINIMUM_NUMBER_OF_LAYERS_PER_BLOCK, \r\n",
      "                    maximum_number_of_layers_per_block =\r\n",
      "                        MAXIMUM_NUMBER_OF_LAYERS_PER_BLOCK,\r\n",
      "                    minimum_neurons_per_block_layer =\r\n",
      "                        MINIMUM_NEURONS_PER_BLOCK_LAYER, \r\n",
      "                    maximum_neurons_per_block_layer =\r\n",
      "                        MAXIMUM_NEURONS_PER_BLOCK_LAYER, \r\n",
      "                    n_options_of_neurons_per_layer_to_try =\r\n",
      "                        N_OPTIONS_OF_NEURONS_PER_LAYER_TO_TRY, \r\n",
      "                    minimum_neurons_per_block_layer_decay =\r\n",
      "                        MINIMUM_NEURONS_PER_BLOCK_LAYER_DECAY, \r\n",
      "                    maximum_neurons_per_block_layer_decay = \r\n",
      "                        MAXIMUM_NEURONS_PER_BLOCK_LAYER_DECAY, \r\n",
      "                    minimum_dropout_rate_for_bypass_layers =\r\n",
      "                        MINIMUM_DROPOUT_RATE_FOR_BYPASS_LAYERS, \r\n",
      "                    maximim_dropout_rate_for_bypass_layers =\r\n",
      "                        MAXIMIM_DROPOUT_RATE_FOR_BYPASS_LAYERS, \r\n",
      "                    n_options_dropout_rate_for_bypass_layers =\r\n",
      "                        N_OPTIONS_DROPOUT_RATE_FOR_BYPASS_LAYERS,\r\n",
      "                    minimum_inter_block_layers_per_block =\r\n",
      "                        MINIMUM_INTER_BLOCK_LAYERS_PER_BLOCK, \r\n",
      "                    maximum_inter_block_layers_per_block =\r\n",
      "                        MAXIMUM_INTER_BLOCK_LAYERS_PER_BLOCK,\r\n",
      "                    n_options_inter_block_layers_per_block =\\\r\n",
      "                        N_OPTIONS_INTER_BLOCK_LAYERS_PER_BLOCK,\r\n",
      "                    minimum_dropout_rate = MINIMUM_DROPOUT_RATE, \r\n",
      "                    maximum_dropout_rate = MAXIMUM_DROPOUT_RATE,\r\n",
      "                    n_options_dropout_rate = N_OPTIONS_DROPOUT_RATE, \r\n",
      "                    minimum_final_dense_layers =\r\n",
      "                        MINIMUM_FINAL_DENSE_LAYERS,\r\n",
      "                    maximum_final_dense_layers =\r\n",
      "                        MAXIMUM_FINAL_DENSE_LAYERS, \r\n",
      "                    n_options_final_dense_layers =\r\n",
      "                        N_OPTIONS_FINAL_DENSE_LAYERS, \r\n",
      "                    number_of_classes = NUMBER_OF_CLASSES,\r\n",
      "                    final_activation = tf.keras.activations.softmax)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "logdir_for_search = os.path.join(\"logs\", RESULTS_DIR_FOR_SEARCH + \"_TB\")\r\n",
      "tensorboard_callback_search =\\\r\n",
      "    tf.keras.callbacks.TensorBoard(logdir_for_search, histogram_freq=1)\r\n",
      "\r\n",
      "tuner = kt.Hyperband(\r\n",
      "    model_builder.build_auto_residual_mlp,\r\n",
      "    objective='val_loss',\r\n",
      "    project_name = PROJECT_NAME,\r\n",
      "    max_epochs = MAX_EPOCHS,\r\n",
      "    hyperband_iterations = 2)\r\n",
      "\r\n",
      "\"\"\")\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "    cifar = tf.keras.datasets.cifar10.load_data()\r\n",
      "    (x_train, y_train), (x_test, y_test) = cifar\r\n",
      "    \r\n",
      "    y_train_ohe = tf.one_hot([i[0] for i in  y_train],10)\r\n",
      "    indexes_for_rows = tf.range(0,y_train.shape[0])\r\n",
      "    shuffled_indexes = tf.random.shuffle(indexes_for_rows)\r\n",
      "    selected_indexes = shuffled_indexes[:TRAINING_SET_SIZE]\r\n",
      "    selected_x_train = x_train[selected_indexes,:,:,:]\r\n",
      "    selected_y_train_ohe = y_train_ohe.numpy()[selected_indexes,:]\r\n",
      "    \r\n",
      "    \r\n",
      "    \r\n",
      "    \r\n",
      "    mod_with_fc_raw = tf.keras.applications.efficientnet.EfficientNetB7(\r\n",
      "        include_top=True, weights='imagenet', input_tensor=None,\r\n",
      "        input_shape = BASE_MODEL_INPUT_SHAPE, pooling='max', classes=1000\r\n",
      "    )\r\n",
      "    \r\n",
      "    # Make the deepest conv2d layer trainable, leave everything else\r\n",
      "    # as not trainable\r\n",
      "    for layer in mod_with_fc_raw.layers:\r\n",
      "        layer.trainable = False\r\n",
      "    # Last conv2d layer. This we want to train .\r\n",
      "    mod_with_fc_raw.layers[-6].trainable = True\r\n",
      "    \r\n",
      "    # Create the final base model\r\n",
      "    # (remove the final Dense and BatchNormalization layers ...) \r\n",
      "    efficient_net_b_7_transferable_base_model =\\\r\n",
      "        tf.keras.Model(inputs=mod_with_fc_raw.layers[0].input, \r\n",
      "                        outputs=mod_with_fc_raw.layers[-3].output)\r\n",
      "    \r\n",
      "    model_builder = ResidualMLP(\r\n",
      "                        problem_type = PROBLEM_TYPE,\r\n",
      "                        minimum_learning_rate = MINIMUM_LEARNING_RATE, \r\n",
      "                        maximum_learning_rate = MAXIMUM_LEARNING_RATE, \r\n",
      "                        number_of_learning_rates_to_try =\r\n",
      "                            NUMBER_OF_LEARNING_RATES_TO_TRY, \r\n",
      "                        input_shape = INPUT_SHAPE, \r\n",
      "                        bw_images = False, \r\n",
      "                        base_model = \r\n",
      "                            efficient_net_b_7_transferable_base_model, \r\n",
      "                        base_model_input_shape = BASE_MODEL_INPUT_SHAPE, \r\n",
      "                        flatten_after_base_model = False, \r\n",
      "                        minimum_number_of_blocks = MINIMUM_NUMBER_OF_BLOCKS, \r\n",
      "                        maximum_number_of_blocks = MAXIMUM_NUMBER_OF_BLOCKS, \r\n",
      "                        minimum_number_of_layers_per_block =\r\n",
      "                            MINIMUM_NUMBER_OF_LAYERS_PER_BLOCK, \r\n",
      "                        maximum_number_of_layers_per_block =\r\n",
      "                            MAXIMUM_NUMBER_OF_LAYERS_PER_BLOCK,\r\n",
      "                        minimum_neurons_per_block_layer =\r\n",
      "                            MINIMUM_NEURONS_PER_BLOCK_LAYER, \r\n",
      "                        maximum_neurons_per_block_layer =\r\n",
      "                            MAXIMUM_NEURONS_PER_BLOCK_LAYER, \r\n",
      "                        n_options_of_neurons_per_layer_to_try =\r\n",
      "                            N_OPTIONS_OF_NEURONS_PER_LAYER_TO_TRY, \r\n",
      "                        minimum_neurons_per_block_layer_decay =\r\n",
      "                            MINIMUM_NEURONS_PER_BLOCK_LAYER_DECAY, \r\n",
      "                        maximum_neurons_per_block_layer_decay = \r\n",
      "                            MAXIMUM_NEURONS_PER_BLOCK_LAYER_DECAY, \r\n",
      "                        minimum_dropout_rate_for_bypass_layers =\r\n",
      "                            MINIMUM_DROPOUT_RATE_FOR_BYPASS_LAYERS, \r\n",
      "                        maximim_dropout_rate_for_bypass_layers =\r\n",
      "                            MAXIMIM_DROPOUT_RATE_FOR_BYPASS_LAYERS, \r\n",
      "                        n_options_dropout_rate_for_bypass_layers =\r\n",
      "                            N_OPTIONS_DROPOUT_RATE_FOR_BYPASS_LAYERS,\r\n",
      "                        minimum_inter_block_layers_per_block =\r\n",
      "                            MINIMUM_INTER_BLOCK_LAYERS_PER_BLOCK, \r\n",
      "                        maximum_inter_block_layers_per_block =\r\n",
      "                            MAXIMUM_INTER_BLOCK_LAYERS_PER_BLOCK,\r\n",
      "                        n_options_inter_block_layers_per_block =\\\r\n",
      "                            N_OPTIONS_INTER_BLOCK_LAYERS_PER_BLOCK,\r\n",
      "                        minimum_dropout_rate = MINIMUM_DROPOUT_RATE, \r\n",
      "                        maximum_dropout_rate = MAXIMUM_DROPOUT_RATE,\r\n",
      "                        n_options_dropout_rate = N_OPTIONS_DROPOUT_RATE, \r\n",
      "                        minimum_final_dense_layers =\r\n",
      "                            MINIMUM_FINAL_DENSE_LAYERS,\r\n",
      "                        maximum_final_dense_layers =\r\n",
      "                            MAXIMUM_FINAL_DENSE_LAYERS, \r\n",
      "                        n_options_final_dense_layers =\r\n",
      "                            N_OPTIONS_FINAL_DENSE_LAYERS, \r\n",
      "                        number_of_classes = NUMBER_OF_CLASSES,\r\n",
      "                        final_activation = tf.keras.activations.softmax)\r\n",
      "    \r\n",
      "    \r\n",
      "    \r\n",
      "    logdir_for_search = os.path.join(\"logs\", RESULTS_DIR_FOR_SEARCH + \"_TB\")\r\n",
      "    tensorboard_callback_search =\\\r\n",
      "        tf.keras.callbacks.TensorBoard(logdir_for_search, histogram_freq=1)\r\n",
      "    \r\n",
      "    tuner = kt.Hyperband(\r\n",
      "        model_builder.build_auto_residual_mlp,\r\n",
      "        objective='val_loss',\r\n",
      "        project_name = PROJECT_NAME,\r\n",
      "        max_epochs = MAX_EPOCHS,\r\n",
      "        hyperband_iterations = 3)\r\n",
      "    \r\n",
      "    \r\n",
      "    tuner.search(x=selected_x_train,  \r\n",
      "                 y=selected_y_train_ohe,\r\n",
      "                 epochs=MAX_EPOCHS,\r\n",
      "                 batch_size=BATCH_SIZE, \r\n",
      "                 callbacks=[\r\n",
      "                        tf.keras.callbacks.EarlyStopping(\r\n",
      "                            monitor=\"val_loss\",\r\n",
      "                            patience=PATIENCE,\r\n",
      "                            min_delta=PATIENCE_MIN_DELTA,\r\n",
      "                            restore_best_weights=True,\r\n",
      "                        ),\r\n",
      "                        tensorboard_callback_search,\r\n",
      "                    ],\r\n",
      "                 validation_split=0.3)\r\n",
      "    \r\n",
      "    print(\"These are the best params and results:\")\r\n",
      "    tuner.results_summary(num_trials=10)\r\n",
      "    \r\n",
      "    # final_model = tuner.get_best_models(num_models=1)[0]\r\n",
      "    \r\n",
      "    best_hp = tuner.get_best_hyperparameters()[0]\r\n",
      "    final_model = tuner.hypermodel.build(best_hp)\r\n",
      "\r\n",
      "    \r\n",
      "    \r\n",
      "    logdir_final_model = os.path.join(\"logs\",\r\n",
      "                                      RESULTS_DIR_FOR_FINAL_MODEL + \"_TB\")\r\n",
      "    tensorboard_callback_final =\\\r\n",
      "        tf.keras.callbacks.TensorBoard(logdir_final_model, histogram_freq=1)\r\n",
      "    \r\n",
      "    history = final_model.fit(x=selected_x_train,  \r\n",
      "                        y=selected_y_train_ohe, \r\n",
      "                        batch_size=BATCH_SIZE, \r\n",
      "                        epochs=150,      \r\n",
      "                        verbose='auto', \r\n",
      "                        callbacks=[tf.keras.callbacks.\\\r\n",
      "                                   EarlyStopping(monitor='val_loss',\r\n",
      "                                                 patience=PATIENCE,\r\n",
      "                                                 min_delta=PATIENCE_MIN_DELTA,\r\n",
      "                                                 restore_best_weights=True),\r\n",
      "                                tensorboard_callback_final], \r\n",
      "                        validation_split=0.3,\r\n",
      "                        validation_data=None,\r\n",
      "                        shuffle=True,\r\n",
      "                        class_weight=None, \r\n",
      "                        sample_weight=None, \r\n",
      "                        initial_epoch=0, \r\n",
      "                        steps_per_epoch=None, \r\n",
      "                        validation_steps=None, \r\n",
      "                        validation_batch_size=10, \r\n",
      "                        validation_freq=1, \r\n",
      "                        max_queue_size=10, \r\n",
      "                        workers=5, \r\n",
      "                        use_multiprocessing=True)\r\n",
      "    \r\n",
      "    \r\n",
      "    \r\n",
      "    hy = pd.DataFrame(history.history)\r\n",
      "    hy.to_csv(f'{DATE}_test_history.csv')\r\n",
      "    hy.to_json(f'{DATE}_test_history.json')\r\n",
      "    \r\n",
      "    final_model.save(FINAL_MODEL_PATH)\r\n",
      "    print(\"Successful Run!\")\r\n"
     ]
    }
   ],
   "source": [
    "! cat task.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adcb9b0",
   "metadata": {},
   "source": [
    "## File 3: My python package > module residualmlp.residual_mlp that was used to run the neural architcture search task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fef59e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try:\r\n",
      "    import keras_tuner as kt\r\n",
      "except Exception as exc:\r\n",
      "    print(\"Importing Keras tuner appears to be unsuccesful. \"\r\n",
      "          \"keras tuner may need to be installed $ pip install -q -U \"\r\n",
      "          \"keras-tuner. The auto-ml features are disabled until this is \"\r\n",
      "           \"fixed, but ResidualMLP will work. A more detailed error is: \"\r\n",
      "           f\"{exc}\")\r\n",
      "import tensorflow as tf\r\n",
      "import numpy as np\r\n",
      "import pandas as pd\r\n",
      "import pendulum\r\n",
      "\r\n",
      "# Becomes a layer to convert BW images to RGB\r\n",
      "\r\n",
      "class ResidualMLP:\r\n",
      "    \r\n",
      "    def __init__(self, problem_type = 'classification',\r\n",
      "                      learning_rate = .0007,\r\n",
      "                      minimum_learning_rate = 0.00007,\r\n",
      "                      maximum_learning_rate = 0.7,\r\n",
      "                      number_of_learning_rates_to_try = 5,\r\n",
      "                      input_shape = (32,32,3),\r\n",
      "                      bw_images = False,\r\n",
      "                      base_model = '',\r\n",
      "                      base_model_input_shape = (600,600,3),\r\n",
      "                      base_model_hyperparameters = {},\r\n",
      "                      flatten_after_base_model = True,\r\n",
      "                          # 2D ARRAY: Each row: \r\n",
      "                          # [number_of_layers,\r\n",
      "                          #  first_layer_neurons, \r\n",
      "                          #  decay_of_n_Dense_units_per_layer]\r\n",
      "                      blocks = [[5,400,50]],\r\n",
      "                      minimum_number_of_blocks = 1,\r\n",
      "                      maximum_number_of_blocks = 7,\r\n",
      "                      minimum_number_of_layers_per_block = 1,\r\n",
      "                      maximum_number_of_layers_per_block = 7,\r\n",
      "                      minimum_neurons_per_block_layer = 3,\r\n",
      "                      maximum_neurons_per_block_layer = 30,\r\n",
      "                      n_options_of_neurons_per_layer_to_try = 7,\r\n",
      "                      minimum_neurons_per_block_layer_decay = 1,\r\n",
      "                      maximum_neurons_per_block_layer_decay = 7,\r\n",
      "                      residual_bypass_dense_layers = list(),\r\n",
      "                      b_norm_or_dropout_residual_bypass_layers = 'dropout',\r\n",
      "                      dropout_rate_for_bypass_layers = .35,\r\n",
      "                      minimum_dropout_rate_for_bypass_layers = 0.01,\r\n",
      "                      maximim_dropout_rate_for_bypass_layers = 0.7,\r\n",
      "                      n_options_dropout_rate_for_bypass_layers = 7,\r\n",
      "                      inter_block_layers_per_block = list(),\r\n",
      "                      minimum_inter_block_layers_per_block = 3,\r\n",
      "                      maximum_inter_block_layers_per_block = 30,\r\n",
      "                      n_options_inter_block_layers_per_block = 7,\r\n",
      "                      b_norm_or_dropout_last_layers = 'dropout', # | 'bnorm'\r\n",
      "                      dropout_rate = .2, #\r\n",
      "                      minimum_dropout_rate = 0.01,\r\n",
      "                      maximum_dropout_rate = 0.7,\r\n",
      "                      n_options_dropout_rate = 7,\r\n",
      "                      activation = tf.keras.activations.relu,\r\n",
      "                      final_dense_layers = [75,35],\r\n",
      "                      minimum_final_dense_layers = 0,\r\n",
      "                      maximum_final_dense_layers = 30,\r\n",
      "                      n_options_final_dense_layers = 2,\r\n",
      "                      number_of_classes = 10, # 1 if a regression problem\r\n",
      "                      final_activation = tf.keras.activations.softmax,\r\n",
      "                      loss = tf.keras.losses.CategoricalCrossentropy(\r\n",
      "                          from_logits=False)):\r\n",
      "        self.problem_type = problem_type\r\n",
      "        self.learning_rate = learning_rate\r\n",
      "        self.minimum_learning_rate = minimum_learning_rate\r\n",
      "        self.maximum_learning_rate = maximum_learning_rate\r\n",
      "        self.number_of_learning_rates_to_try = number_of_learning_rates_to_try\r\n",
      "        self.input_shape = input_shape\r\n",
      "        self.bw_images = bw_images\r\n",
      "        self.base_model = base_model\r\n",
      "        self.base_model_input_shape = base_model_input_shape\r\n",
      "        self.flatten_after_base_model = flatten_after_base_model\r\n",
      "        self.blocks = blocks\r\n",
      "        self.minimum_number_of_blocks = minimum_number_of_blocks\r\n",
      "        self.maximum_number_of_blocks = maximum_number_of_blocks\r\n",
      "        self.minimum_number_of_layers_per_block =\\\r\n",
      "            minimum_number_of_layers_per_block\r\n",
      "        self.maximum_number_of_layers_per_block =\\\r\n",
      "            maximum_number_of_layers_per_block\r\n",
      "        self.minimum_neurons_per_block_layer = minimum_neurons_per_block_layer\r\n",
      "        self.maximum_neurons_per_block_layer = maximum_neurons_per_block_layer\r\n",
      "        self.n_options_of_neurons_per_layer_to_try =\\\r\n",
      "            n_options_of_neurons_per_layer_to_try\r\n",
      "        self.minimum_neurons_per_block_layer_decay =\\\r\n",
      "            minimum_neurons_per_block_layer_decay\r\n",
      "        self.maximum_neurons_per_block_layer_decay =\\\r\n",
      "            maximum_neurons_per_block_layer_decay\r\n",
      "        # residual_bypass_dense_layers\r\n",
      "        # Screen for nonsense combinations of the parameters 'blocks' and \r\n",
      "        # 'residual_bypass_dense_layers'\r\n",
      "        if not isinstance(residual_bypass_dense_layers,list):\r\n",
      "            raise ValueError(\"The parameter residual_bypass_dense_layers \"\r\n",
      "                             \"should be one of these 2: 1. a 2d list, one 1d \"\r\n",
      "                             \"list of positive    integers for each list in \"\r\n",
      "                             \"blocks, or 2. an empty list.\")\r\n",
      "        if len(residual_bypass_dense_layers) != 0:\r\n",
      "            if len(blocks) != len(residual_bypass_dense_layers):\r\n",
      "                raise ValueError(\"The parameter 'blocks' and \"\r\n",
      "                                 \"'residual_bypass_dense_layers are 2d \"\r\n",
      "                                 \"arrays' must have the same number of 1d \"\r\n",
      "                                 \"arrays nested within them OR be aan empty \"\r\n",
      "                                 \"list or left default. To fix this error \"\r\n",
      "                                 \"do one of the following: Fix 1: Make them \" \r\n",
      "                                 \"the same number of 1d arrays, for example \"\r\n",
      "                                 \"blocks = [[5,20,2],[5,20,2],[5,20,2]] ...\"\r\n",
      "                                 \" (3 nested 1d arrays) \"\r\n",
      "                                 \"residual_bypass_dense_layers = \"\r\n",
      "                                 \"[[10,7],[],[10]] also 3. \"\r\n",
      "                                 \"the ith array nested in \"\r\n",
      "                                 \"residual_bypass_dense_layers will be \"\r\n",
      "                                 \"associated with the i_th block in blocks. \"\r\n",
      "                                 \" each j_th item in the i_th nested 1d \"\r\n",
      "                                 \"array will make one dense layer in the \"\r\n",
      "                                 \"tensor that bypasses the main block of \"\r\n",
      "                                 \"dense layers in the ith block in the \"\r\n",
      "                                 \"residual MLP. This is probably a bit \"\r\n",
      "                                 \"confusing to read. Please refer to the \"\r\n",
      "                                 \"tutorials and documentation. Note \"\r\n",
      "                                 \"the order I referred to the tutorials and \"\r\n",
      "                                 \"documentation in. Fix 2: leave \"\r\n",
      "                                 \"residual_bypass_dense_layers default / \"\r\n",
      "                                 \"set it to an empty list.\")\r\n",
      "            else:\r\n",
      "                self.residual_bypass_dense_layers =\\\r\n",
      "                    residual_bypass_dense_layers\r\n",
      "        else:\r\n",
      "            self.residual_bypass_dense_layers = [[] for block in blocks]\r\n",
      "        self.b_norm_or_dropout_residual_bypass_layers =\\\r\n",
      "            b_norm_or_dropout_residual_bypass_layers\r\n",
      "        self.dropout_rate_for_bypass_layers = dropout_rate_for_bypass_layers\r\n",
      "        self.minimum_dropout_rate_for_bypass_layers =\\\r\n",
      "            minimum_dropout_rate_for_bypass_layers\r\n",
      "        self.maximim_dropout_rate_for_bypass_layers =\\\r\n",
      "            maximim_dropout_rate_for_bypass_layers\r\n",
      "        self.n_options_dropout_rate_for_bypass_layers =\\\r\n",
      "            n_options_dropout_rate_for_bypass_layers\r\n",
      "        self.inter_block_layers_per_block = inter_block_layers_per_block\r\n",
      "        self.minimum_inter_block_layers_per_block =\\\r\n",
      "            minimum_inter_block_layers_per_block\r\n",
      "        self.maximum_inter_block_layers_per_block =\\\r\n",
      "            maximum_inter_block_layers_per_block\r\n",
      "        self.n_options_inter_block_layers_per_block =\\\r\n",
      "            n_options_inter_block_layers_per_block\r\n",
      "        self.b_norm_or_dropout_last_layers = b_norm_or_dropout_last_layers\r\n",
      "        self.dropout_rate  = dropout_rate\r\n",
      "        self.minimum_dropout_rate = minimum_dropout_rate\r\n",
      "        self.maximum_dropout_rate = maximum_dropout_rate\r\n",
      "        self.n_options_dropout_rate = n_options_dropout_rate\r\n",
      "        self.activation = activation\r\n",
      "        self.final_dense_layers = final_dense_layers\r\n",
      "        self.minimum_final_dense_layers = minimum_final_dense_layers\r\n",
      "        self.maximum_final_dense_layers = maximum_final_dense_layers\r\n",
      "        self.n_options_final_dense_layers = n_options_final_dense_layers\r\n",
      "        self.number_of_classes = number_of_classes\r\n",
      "        self.final_activation = final_activation\r\n",
      "        self.loss = loss\r\n",
      "        \r\n",
      "    \r\n",
      "    def grayscale_to_rgb(images, channel_axis=-1):\r\n",
      "        images= tf.expand_dims(images, axis=channel_axis)\r\n",
      "        tiling = [1] * 4    # 4 dimensions: B, H, W, C\r\n",
      "        tiling[channel_axis] *= 3\r\n",
      "        images= tf.tile(images, tiling)\r\n",
      "        im = tf.keras.preprocessing.image.smart_resize(images,(224,224))\r\n",
      "        return im\r\n",
      "\r\n",
      "\r\n",
      "    # builds and compiles a tandem model given these params and\r\n",
      "    # selected base model:\r\n",
      "    def make_tandem_model(self):\r\n",
      "        if self.problem_type == 'classification':\r\n",
      "            precision = tf.keras.metrics.Precision(),\r\n",
      "            recall = tf.keras.metrics.Recall()\r\n",
      "            accuracy = tf.keras.metrics.Accuracy()\r\n",
      "        if self.problem_type == 'classification' and\\\r\n",
      "                self.number_of_classes > 1:\r\n",
      "            metrics = [tf.keras.metrics.TopKCategoricalAccuracy(\r\n",
      "                k = k,\r\n",
      "                name=f'top_{k}_'\r\n",
      "                'categorical_'\r\n",
      "                'accuracy',\r\n",
      "                dtype=None)\r\n",
      "                           for k in np.arange(1,self.number_of_classes)\\\r\n",
      "                               if k < 10]\r\n",
      "            metrics.append(precision)\r\n",
      "            metrics.append(recall)\r\n",
      "            metrics.append(accuracy)\r\n",
      "        elif self.problem_type == 'classification' and\\\r\n",
      "                self.number_of_classes == 1:    \r\n",
      "            metrics = [precision, recall, accuracy]\r\n",
      "        else:\r\n",
      "            rmse = tf.keras.metrics.RootMeanSquaredError()\r\n",
      "            mae = tf.keras.metrics.MeanAbsoluteError()\r\n",
      "            metrics = [rmse, mae]\r\n",
      "    \r\n",
      "        inp = tf.keras.layers.Input(shape = self.input_shape) \r\n",
      "        # Start with input layer that fits. \r\n",
      "        # The keras fucntional API requires an explicit input layer\r\n",
      "        if self.bw_images:\r\n",
      "            x = self.grayscale_to_rgb(inp)\r\n",
      "        else:\r\n",
      "            x = inp\r\n",
      "        if self.base_model != '':\r\n",
      "            x = tf.keras.layers.Resizing(self.base_model_input_shape[0],\r\n",
      "                                         self.base_model_input_shape[1])(x)\r\n",
      "            x = self.base_model(x)\r\n",
      "        if self.flatten_after_base_model:\r\n",
      "            x = tf.keras.layers.Flatten()(x)\r\n",
      "        initializer = tf.keras.initializers.GlorotNormal()\r\n",
      "        for bl in np.arange(len(self.blocks)):\r\n",
      "            block = self.blocks[bl]\r\n",
      "            bypass_block = self.residual_bypass_dense_layers[bl]\r\n",
      "            \r\n",
      "            \r\n",
      "            x = tf.keras.layers.Dense(block[1],\r\n",
      "                                      self.activation,\r\n",
      "                                      kernel_initializer=initializer)(x)\r\n",
      "            y = x\r\n",
      "            x = tf.keras.layers.BatchNormalization()(x)\r\n",
      "            # x proceeds sequentially to the \r\n",
      "            # next Dense layer.\r\n",
      "            \r\n",
      "            if self.b_norm_or_dropout_residual_bypass_layers == 'dropout':\r\n",
      "                y = tf.keras.layers\\\r\n",
      "                    .Dropout(self.dropout_rate_for_bypass_layers)(y)\r\n",
      "            elif self.b_norm_or_dropout_residual_bypass_layers == 'bnorm':\r\n",
      "                y = tf.keras.layers.BatchNormalization()(y)\r\n",
      "            else:\r\n",
      "                raise ValueError(\"The parameter: \"\r\n",
      "                                 \"'b_norm_or_dropout_residual_bypass_\"\r\n",
      "                                 \"layers'\"\r\n",
      "                                 \" must be left default '', or be \"\r\n",
      "                                 \"'dropout' or may be 'bnorm'.\")\r\n",
      "            for bypass_layer in bypass_block:\r\n",
      "                y = tf.keras.layers.Dense(bypass_layer,\r\n",
      "                                          self.activation,\r\n",
      "                                          kernel_initializer=initializer)(y)\r\n",
      "                if self.b_norm_or_dropout_residual_bypass_layers == 'dropout':\r\n",
      "                    y = tf.keras.layers\\\r\n",
      "                        .Dropout(self.dropout_rate_for_bypass_layers)(y)\r\n",
      "                elif self.b_norm_or_dropout_residual_bypass_layers == 'bnorm':\r\n",
      "                    y = tf.keras.layers.BatchNormalization()(y)\r\n",
      "                else:\r\n",
      "                    raise ValueError(\"The parameter: \"\r\n",
      "                                     \"'b_norm_or_dropout_residual_bypass_\"\r\n",
      "                                     \"layers' must be left default '', or be \"\r\n",
      "                                     \"'dropout' or may be 'bnorm'.\")\r\n",
      "            # y does NOT proceed sequentially\r\n",
      "            # to the next layer. This bypasses \r\n",
      "            # several layers and give a memory \r\n",
      "            # that attenuates some of the \r\n",
      "            # deleterious effects of a deeper \r\n",
      "            # network and lets us capture more \r\n",
      "            # complex interactions before \r\n",
      "            # overfitting becomes an issue than \r\n",
      "            # the textbook sequential multi - \r\n",
      "            # layer perceptron ...\r\n",
      "            for j in np.arange(block[0]): \r\n",
      "                x = tf.keras.layers.Dense(block[1] - block[2] * j,\r\n",
      "                                          self.activation,\r\n",
      "                                          kernel_initializer=initializer)(x) \r\n",
      "                x = tf.keras.layers.BatchNormalization()(x)\r\n",
      "    \r\n",
      "            x = tf.keras.layers.Concatenate(axis=1)([x, y])\r\n",
      "            \r\n",
      "            if bl != np.arange(len(self.blocks)).max():\r\n",
      "                for inter_block_layer in self.inter_block_layers_per_block:\r\n",
      "                    x = tf.keras.layers.Dense(inter_block_layer,\r\n",
      "                                          self.activation,\r\n",
      "                                          kernel_initializer=initializer)(x)\r\n",
      "                    x = tf.keras.layers.BatchNormalization()(x)\r\n",
      "    \r\n",
      "        for i in self.final_dense_layers:\r\n",
      "            x = tf.keras.layers.Dense(i,\r\n",
      "                                      self.activation,\r\n",
      "                                      kernel_initializer=initializer)(x) \r\n",
      "            if self.b_norm_or_dropout_last_layers == 'bnorm':\r\n",
      "                x = tf.keras.layers.BatchNormalization()(x)\r\n",
      "            elif self.b_norm_or_dropout_last_layers == 'dropout':\r\n",
      "                x = tf.keras.layers.Dropout(self.dropout_rate)(x)\r\n",
      "            else:\r\n",
      "                raise ValueError(\"For b_norm_or_dropout_last_layers, \" \r\n",
      "                                 \"you must pick either 'dropout' or 'bnorm'\")\r\n",
      "        out = tf.keras.layers.Dense(self.number_of_classes,\r\n",
      "                                    self.final_activation,\r\n",
      "                                    kernel_initializer=initializer)(x)\r\n",
      "    \r\n",
      "        # Declare the graph for our model ...\r\n",
      "        modelo_final = tf.keras.Model(inputs=inp,outputs = out)\r\n",
      "        \r\n",
      "        modelo_final\\\r\n",
      "            .compile(optimizer=\\\r\n",
      "                     tf.keras.optimizers.Adam(\r\n",
      "                         learning_rate=self.learning_rate, \r\n",
      "                         clipnorm=1.0),\r\n",
      "                         loss=self.loss, \r\n",
      "                         metrics=metrics)\r\n",
      "        return modelo_final\r\n",
      "    \r\n",
      "    \r\n",
      "    def parse_block(self,number_of_blocks,\r\n",
      "                        layers_per_block,\r\n",
      "                        neurons_per_block_layer,\r\n",
      "                        neurons_per_block_layer_decay):\r\n",
      "        blocks_0 = []\r\n",
      "        for i in np.arange(number_of_blocks):\r\n",
      "            block_0 = [layers_per_block,\r\n",
      "                       neurons_per_block_layer,\r\n",
      "                       neurons_per_block_layer_decay]\r\n",
      "            blocks_0.append(block_0)\r\n",
      "        return blocks_0\r\n",
      "    \r\n",
      "    def build_auto_residual_mlp(self,hp):\r\n",
      "        \r\n",
      "        self.learning_rate = hp.Float(name='learning_rate',\r\n",
      "                                      min_value=self.minimum_learning_rate, \r\n",
      "                                      max_value=self.maximum_learning_rate,\r\n",
      "                                      sampling='log')\r\n",
      "\r\n",
      "        permutations_of_blocks_array = np.array([[i,j,k,l]\r\n",
      "         for i in np.arange(self.minimum_number_of_blocks,\r\n",
      "                            self.maximum_number_of_blocks + 1)\r\n",
      "         for j in np.arange(self.minimum_number_of_layers_per_block,\r\n",
      "                            self.maximum_number_of_layers_per_block + 1)\r\n",
      "         for k in np.linspace(self.minimum_neurons_per_block_layer,\r\n",
      "                              self.maximum_neurons_per_block_layer, \r\n",
      "                              self.n_options_of_neurons_per_layer_to_try,\r\n",
      "                              dtype=int)\r\n",
      "         for l in np.arange(self.minimum_neurons_per_block_layer_decay,\r\n",
      "                            self.maximum_neurons_per_block_layer_decay + 1)])\r\n",
      "        \r\n",
      "        permutations_of_blocks_df = pd.DataFrame(permutations_of_blocks_array)\r\n",
      "        permutations_of_blocks_df.columns = ['number_of_blocks',\r\n",
      "                                             'layers_per_block',\r\n",
      "                                             'neurons_per_block_layer',\r\n",
      "                                             'neurons_per_block_layer_decay']\r\n",
      "        print(\"All permutations:\")\r\n",
      "        print(permutations_of_blocks_df)\r\n",
      "\r\n",
      "        # Filter out any invalid permutations that would try creating Dense\r\n",
      "        # layers with 0 units or a negative number of units.\r\n",
      "        permutations_of_blocks_df['valid_block'] =\\\r\n",
      "            permutations_of_blocks_df['neurons_per_block_layer'] >\\\r\n",
      "            permutations_of_blocks_df[\"layers_per_block\"] *\\\r\n",
      "            permutations_of_blocks_df[\"neurons_per_block_layer_decay\"]\r\n",
      "        valid_permutations_df =\\\r\n",
      "            permutations_of_blocks_df.query(\"valid_block == True\")\\\r\n",
      "            .reset_index(drop=True)\r\n",
      "        print(\"Valid permutations\")\r\n",
      "        print(valid_permutations_df)\r\n",
      "        \r\n",
      "        list_of_blocks_args = list()\r\n",
      "        for i in np.arange(valid_permutations_df.shape[0]):\r\n",
      "            blocks_arg = self.parse_block(\r\n",
      "                valid_permutations_df.loc[i]['number_of_blocks'],\r\n",
      "                valid_permutations_df.loc[i]['layers_per_block'],\r\n",
      "                valid_permutations_df.loc[i]['neurons_per_block_layer'],\r\n",
      "                valid_permutations_df.loc[i]['neurons_per_block_layer_decay'])\r\n",
      "            list_of_blocks_args.append(blocks_arg)\r\n",
      "        \r\n",
      "        valid_permutations_df['blocks'] = list_of_blocks_args\r\n",
      "        \r\n",
      "        print(\"Valid permutations with blocks column\")\r\n",
      "        print(valid_permutations_df)\r\n",
      " \r\n",
      "        valid_permutations_df.sort_values(['layers_per_block',\r\n",
      "                                           'neurons_per_block_layer'],\r\n",
      "                                            ascending=True)\\\r\n",
      "            .reset_index(drop=True)\r\n",
      "        \r\n",
      "        # for reeference, the list of block options is saved as a csv\r\n",
      "        date = pendulum.now().__str__()[:16].replace(\"T\",\"_\").replace(\":\",\"_\")\r\n",
      "        valid_permutations_df.to_csv(f'{date}_blocks_permutations.csv')\r\n",
      "        \r\n",
      "        list_to_choose_blocks_option_from =\\\r\n",
      "            [int(i) for i in np.arange(valid_permutations_df.shape[0])]\r\n",
      "        blocks_index_chosen = hp.Choice(\r\n",
      "                    name='blocks',\r\n",
      "                    values=list_to_choose_blocks_option_from,\r\n",
      "                    ordered=True)\r\n",
      "        \r\n",
      "        \r\n",
      "        self.blocks = valid_permutations_df.loc[blocks_index_chosen]['blocks']\r\n",
      "        print(self.blocks)\r\n",
      "        \r\n",
      "        bypass_layers_units = hp.Choice(name='bypass_layers_units',\r\n",
      "                        values=[int(i) \r\n",
      "                                for i in np.linspace(\r\n",
      "                                self.minimum_neurons_per_block_layer ,\r\n",
      "                                self.maximum_neurons_per_block_layer,\r\n",
      "                                self.n_options_of_neurons_per_layer_to_try,\r\n",
      "                                dtype=int)],\r\n",
      "                        ordered=True)\r\n",
      "        \r\n",
      "        if bypass_layers_units == 0:\r\n",
      "            self.residual_bypass_dense_layers =\\\r\n",
      "                [list() for _ in np.arange(len(self.blocks))]\r\n",
      "        else:\r\n",
      "            self.residual_bypass_dense_layers =\\\r\n",
      "                [[bypass_layers_units] for _ in np.arange(len(self.blocks))]\r\n",
      "        \r\n",
      "        inter_block_layers_per_block_options =\\\r\n",
      "            [int(i) for i in\r\n",
      "                np.linspace(self.minimum_inter_block_layers_per_block,\r\n",
      "                            self.maximum_inter_block_layers_per_block,\r\n",
      "                            self.n_options_inter_block_layers_per_block,\r\n",
      "                            dtype=int)]\r\n",
      "        inter_block_layers_per_block_choice =\\\r\n",
      "            hp.Choice(name='inter_block_layers',\r\n",
      "                      values=inter_block_layers_per_block_options,\r\n",
      "                      ordered=True)\r\n",
      "\r\n",
      "        self.inter_block_layers_per_block = list()\r\n",
      "        if inter_block_layers_per_block_choice > 1:\r\n",
      "            for i in range(len(self.blocks) -1):\r\n",
      "                self.inter_block_layers_per_block\\\r\n",
      "                    .append(inter_block_layers_per_block_choice)\r\n",
      "\r\n",
      "        final_dense_layers_options = \\\r\n",
      "            [int(i) for i in np.linspace(self.minimum_final_dense_layers,\r\n",
      "                                         self.maximum_final_dense_layers,\r\n",
      "                                         self.n_options_final_dense_layers,\r\n",
      "                                         dtype=int)]\r\n",
      "        final_dense_layers_choice = hp.Choice(\r\n",
      "                                            name='final_dense_layers',\r\n",
      "                                            values=final_dense_layers_options,\r\n",
      "                                            ordered=True)\r\n",
      "        if final_dense_layers_choice == 0:\r\n",
      "            self.final_dense_layers = []\r\n",
      "        else:\r\n",
      "            self.final_dense_layers = [final_dense_layers_choice]\r\n",
      "\r\n",
      "        self.b_norm_or_dropout_residual_bypass_layers =\\\r\n",
      "            hp.Choice(name=\"b_norm_or_dropout_residual_bypass_layers\",\r\n",
      "                      values=['dropout','bnorm'],\r\n",
      "                      ordered=False)\r\n",
      "        dropout_rate_for_bypass_layers_choices =\\\r\n",
      "            [float(i) for i in\r\n",
      "                np.linspace(self.minimum_dropout_rate_for_bypass_layers,\r\n",
      "                        self.maximim_dropout_rate_for_bypass_layers,\r\n",
      "                        self.n_options_dropout_rate_for_bypass_layers,\r\n",
      "                        dtype=float)]\r\n",
      "        self.dropout_rate_for_bypass_layers =\\\r\n",
      "            hp.Choice(name='dropout_rate_for_bypass_layers',\r\n",
      "                      values=dropout_rate_for_bypass_layers_choices,\r\n",
      "                      ordered=True)\r\n",
      "\r\n",
      "        self.b_norm_or_dropout_last_layers =\\\r\n",
      "                        hp.Choice(name='b_norm_or_dropout_last_layers',\r\n",
      "                                  values=['dropout','bnorm'],\r\n",
      "                                  ordered=False)\r\n",
      "        \r\n",
      "        dropout_rate_options = [float(i) for i in \r\n",
      "                                np.linspace(self.minimum_dropout_rate, \r\n",
      "                                            self.maximum_dropout_rate,\r\n",
      "                                            self.n_options_dropout_rate,\r\n",
      "                                            dtype=float)]\r\n",
      "        self.dropout_rate = hp.Choice(name='dropout_rate',\r\n",
      "                                      values=dropout_rate_options,\r\n",
      "                                      ordered=True)\r\n",
      "\r\n",
      "        if self.problem_type == 'classification':\r\n",
      "            precision = tf.keras.metrics.Precision(),\r\n",
      "            recall = tf.keras.metrics.Recall()\r\n",
      "            accuracy = tf.keras.metrics.Accuracy()\r\n",
      "        if self.problem_type == 'classification' and\\\r\n",
      "                self.number_of_classes > 1:\r\n",
      "            metrics = [tf.keras.metrics.TopKCategoricalAccuracy(\r\n",
      "                k = k,\r\n",
      "                name=f'top_{k}_'\r\n",
      "                'categorical_'\r\n",
      "                'accuracy',\r\n",
      "                dtype=None)\r\n",
      "                           for k in np.arange(1,self.number_of_classes)\\\r\n",
      "                               if k < 10]\r\n",
      "            metrics.append(precision)\r\n",
      "            metrics.append(recall)\r\n",
      "            metrics.append(accuracy)\r\n",
      "        elif self.problem_type == 'classification' and\\\r\n",
      "                self.number_of_classes == 1:    \r\n",
      "            metrics = [precision, recall, accuracy]\r\n",
      "        else:\r\n",
      "            rmse = tf.keras.metrics.RootMeanSquaredError()\r\n",
      "            mae = tf.keras.metrics.MeanAbsoluteError()\r\n",
      "            metrics = [rmse, mae]\r\n",
      "    \r\n",
      "        inp = tf.keras.layers.Input(shape = self.input_shape) \r\n",
      "        # Start with input layer that fits. \r\n",
      "        # The keras fucntional API requires an explicit input layer\r\n",
      "        if self.bw_images:\r\n",
      "            x = self.grayscale_to_rgb(inp)\r\n",
      "        else:\r\n",
      "            x = inp\r\n",
      "        if self.base_model != '':\r\n",
      "            x = tf.keras.layers.Resizing(self.base_model_input_shape[0],\r\n",
      "                                         self.base_model_input_shape[1])(x)\r\n",
      "            x = self.base_model(x)\r\n",
      "        if self.flatten_after_base_model:\r\n",
      "            x = tf.keras.layers.Flatten()(x)\r\n",
      "        initializer = tf.keras.initializers.GlorotNormal()\r\n",
      "        for bl in np.arange(len(self.blocks)):\r\n",
      "            block = self.blocks[bl]\r\n",
      "            bypass_block = self.residual_bypass_dense_layers[bl]\r\n",
      "            \r\n",
      "            \r\n",
      "            x = tf.keras.layers.Dense(block[1],\r\n",
      "                                      self.activation,\r\n",
      "                                      kernel_initializer=initializer)(x)\r\n",
      "            y = x\r\n",
      "            x = tf.keras.layers.BatchNormalization()(x)\r\n",
      "            # x proceeds sequentially to the \r\n",
      "            # next Dense layer.\r\n",
      "            \r\n",
      "            if self.b_norm_or_dropout_residual_bypass_layers == 'dropout':\r\n",
      "                y = tf.keras.layers\\\r\n",
      "                    .Dropout(self.dropout_rate_for_bypass_layers)(y)\r\n",
      "            elif self.b_norm_or_dropout_residual_bypass_layers == 'bnorm':\r\n",
      "                y = tf.keras.layers.BatchNormalization()(y)\r\n",
      "            else:\r\n",
      "                raise ValueError(\"The parameter: \"\r\n",
      "                                 \"'b_norm_or_dropout_residual_bypass_\"\r\n",
      "                                 \"layers'\"\r\n",
      "                                 \" must be left default '', or be \"\r\n",
      "                                 \"'dropout' or may be 'bnorm'.\")\r\n",
      "            for bypass_layer in bypass_block:\r\n",
      "                y = tf.keras.layers.Dense(bypass_layer,\r\n",
      "                                          self.activation,\r\n",
      "                                          kernel_initializer=initializer)(y)\r\n",
      "                if self.b_norm_or_dropout_residual_bypass_layers == 'dropout':\r\n",
      "                    y = tf.keras.layers\\\r\n",
      "                        .Dropout(self.dropout_rate_for_bypass_layers)(y)\r\n",
      "                elif self.b_norm_or_dropout_residual_bypass_layers == 'bnorm':\r\n",
      "                    y = tf.keras.layers.BatchNormalization()(y)\r\n",
      "                else:\r\n",
      "                    raise ValueError(\"The parameter: \"\r\n",
      "                                     \"'b_norm_or_dropout_residual_bypass_\"\r\n",
      "                                     \"layers' must be left default '', or be \"\r\n",
      "                                     \"'dropout' or may be 'bnorm'.\")\r\n",
      "            # y does NOT proceed sequentially\r\n",
      "            # to the next layer. This bypasses \r\n",
      "            # several layers and give a memory \r\n",
      "            # that attenuates some of the \r\n",
      "            # deleterious effects of a deeper \r\n",
      "            # network and lets us capture more \r\n",
      "            # complex interactions before \r\n",
      "            # overfitting becomes an issue than \r\n",
      "            # the textbook sequential multi - \r\n",
      "            # layer perceptron ...\r\n",
      "            for j in np.arange(block[0]): \r\n",
      "                x = tf.keras.layers.Dense(block[1] - block[2] * j,\r\n",
      "                                          self.activation,\r\n",
      "                                          kernel_initializer=initializer)(x) \r\n",
      "                x = tf.keras.layers.BatchNormalization()(x)\r\n",
      "    \r\n",
      "            x = tf.keras.layers.Concatenate(axis=1)([x, y])\r\n",
      "            \r\n",
      "            if bl != np.arange(len(self.blocks)).max():\r\n",
      "                for inter_block_layer in self.inter_block_layers_per_block:\r\n",
      "                    x = tf.keras.layers.Dense(inter_block_layer,\r\n",
      "                                          self.activation,\r\n",
      "                                          kernel_initializer=initializer)(x)\r\n",
      "                    x = tf.keras.layers.BatchNormalization()(x)\r\n",
      "    \r\n",
      "        for i in self.final_dense_layers:\r\n",
      "            x = tf.keras.layers.Dense(i,\r\n",
      "                                      self.activation,\r\n",
      "                                      kernel_initializer=initializer)(x) \r\n",
      "            if self.b_norm_or_dropout_last_layers == 'bnorm':\r\n",
      "                x = tf.keras.layers.BatchNormalization()(x)\r\n",
      "            elif self.b_norm_or_dropout_last_layers == 'dropout':\r\n",
      "                x = tf.keras.layers.Dropout(self.dropout_rate)(x)\r\n",
      "            else:\r\n",
      "                raise ValueError(\"For b_norm_or_dropout_last_layers, \" \r\n",
      "                                 \"you must pick either 'dropout' or 'bnorm'\")\r\n",
      "        out = tf.keras.layers.Dense(self.number_of_classes,\r\n",
      "                                    self.final_activation,\r\n",
      "                                    kernel_initializer=initializer)(x)\r\n",
      "    \r\n",
      "        # Declare the graph for our model ...\r\n",
      "        modelo_final = tf.keras.Model(inputs=inp,outputs = out)\r\n",
      "        \r\n",
      "        modelo_final\\\r\n",
      "            .compile(optimizer=\\\r\n",
      "                     tf.keras.optimizers.Adam(\r\n",
      "                         learning_rate=self.learning_rate, \r\n",
      "                         clipnorm=1.0),\r\n",
      "                         loss=self.loss, \r\n",
      "                         metrics=metrics)\r\n",
      "        return modelo_final\r\n"
     ]
    }
   ],
   "source": [
    "! cat residualmlp/residual_mlp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b6f2f0",
   "metadata": {},
   "source": [
    "## File 4: The first 150 lines from the console logs from the task run: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "029071ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\r\n",
      "  Downloading pandas-1.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\r\n",
      "      11.7/11.7 MB 21.0 MB/s eta 0:00:00\r\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.19.4)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2021.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\r\n",
      "Installing collected packages: pandas\r\n",
      "Successfully installed pandas-1.4.1\r\n",
      "Collecting keras_tuner\r\n",
      "  Downloading keras_tuner-1.1.0-py3-none-any.whl (98 kB)\r\n",
      "      98.0/98.0 KB 14.4 MB/s eta 0:00:00\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from keras_tuner) (2.26.0)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from keras_tuner) (1.4.1)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from keras_tuner) (21.0)\r\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (from keras_tuner) (7.27.0)\r\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.8/dist-packages (from keras_tuner) (2.6.0)\r\n",
      "Collecting kt-legacy\r\n",
      "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from keras_tuner) (1.19.4)\r\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (0.1.3)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (0.18.0)\r\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (58.1.0)\r\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (3.0.20)\r\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (5.1.0)\r\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (0.7.5)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (4.7.0)\r\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (0.2.0)\r\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (2.10.0)\r\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (5.1.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->keras_tuner) (2.4.7)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->keras_tuner) (2021.5.30)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests->keras_tuner) (2.0.6)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->keras_tuner) (1.26.7)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->keras_tuner) (3.2)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (2.0.1)\r\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (1.35.0)\r\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (0.37.0)\r\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (1.39.0)\r\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (3.17.3)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (1.8.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (3.3.4)\r\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (0.12.0)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (0.6.1)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (0.4.6)\r\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from absl-py>=0.4->tensorboard->keras_tuner) (1.15.0)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras_tuner) (4.2.2)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras_tuner) (0.2.8)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras_tuner) (4.7.2)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras_tuner) (1.3.0)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.16->ipython->keras_tuner) (0.8.2)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect>4.3->ipython->keras_tuner) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras_tuner) (0.2.5)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->keras_tuner) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras_tuner) (3.1.1)\r\n",
      "Installing collected packages: kt-legacy, keras_tuner\r\n",
      "Successfully installed keras_tuner-1.1.0 kt-legacy-1.0.4\r\n",
      "We're missing a module:\r\n",
      "No module named 'pandas'\r\n",
      "No problem, we install it...\r\n",
      "We're missing a module:\r\n",
      "No module named 'keras_tuner'\r\n",
      "No problem, we install it...\r\n",
      "# To reproduce the environment the NAS task was run in, copy and paste the following information into the evaluation script environent (excluding this line). PLEASE NOTE!!: If you do not reporduce these variables, the hyperband will not reproduce the correct model!\r\n",
      "Date = 2022-02-12_14_48\r\n",
      "PROBLEM_TYPE = classification\r\n",
      "NUMBER_OF_CLASSES = 10\r\n",
      "INPUT_SHAPE = (32, 32, 3)\r\n",
      "BASE_MODEL_INPUT_SHAPE = (600, 600, 3)\r\n",
      "PROJECT_NAME = CIFAR10_EfficientNetB7-ResidualMLP_NAS_THIRD_PASS_SEARCH\r\n",
      "TRAINING_SET_SIZE = 50000\r\n",
      "PATIENCE = 25\r\n",
      "PATIENCE_MIN_DELTA = 1e-05\r\n",
      "BATCH_SIZE = 50\r\n",
      "MAX_EPOCHS = 150\r\n",
      "RESULTS_DIR_FOR_SEARCH = 2022-02-12_14_48_CIFAR10_EfficientNetB7-ResidualMLP_NAS_THIRD_PASS_SEARCH_SEARCH_RUN\r\n",
      "MINIMUM_LEARNING_RATE = 0.0001\r\n",
      "MAXIMUM_LEARNING_RATE = 0.02\r\n",
      "NUMBER_OF_LEARNING_RATES_TO_TRY = 7\r\n",
      "MINIMUM_NUMBER_OF_BLOCKS = 2\r\n",
      "MAXIMUM_NUMBER_OF_BLOCKS = 3\r\n",
      "MINIMUM_NUMBER_OF_LAYERS_PER_BLOCK = 2\r\n",
      "MAXIMUM_NUMBER_OF_LAYERS_PER_BLOCK = 4\r\n",
      "MINIMUM_NEURONS_PER_BLOCK_LAYER = 70\r\n",
      "MAXIMUM_NEURONS_PER_BLOCK_LAYER = 140\r\n",
      "N_OPTIONS_OF_NEURONS_PER_LAYER_TO_TRY = 7\r\n",
      "MINIMUM_NEURONS_PER_BLOCK_LAYER_DECAY = 15\r\n",
      "MAXIMUM_NEURONS_PER_BLOCK_LAYER_DECAY = 35\r\n",
      "MINIMUM_DROPOUT_RATE_FOR_BYPASS_LAYERS = 0.2\r\n",
      "MAXIMIM_DROPOUT_RATE_FOR_BYPASS_LAYERS = 0.6\r\n",
      "N_OPTIONS_DROPOUT_RATE_FOR_BYPASS_LAYERS = 3\r\n",
      "MINIMUM_INTER_BLOCK_LAYERS_PER_BLOCK = 0\r\n",
      "MAXIMUM_INTER_BLOCK_LAYERS_PER_BLOCK = 85\r\n",
      "N_OPTIONS_INTER_BLOCK_LAYERS_PER_BLOCK = 7\r\n",
      "MINIMUM_DROPOUT_RATE = 0.2\r\n",
      "MAXIMUM_DROPOUT_RATE = 0.6\r\n",
      "N_OPTIONS_DROPOUT_RATE = 5\r\n",
      "MINIMUM_FINAL_DENSE_LAYERS = 33\r\n",
      "MAXIMUM_FINAL_DENSE_LAYERS = 300\r\n",
      "N_OPTIONS_FINAL_DENSE_LAYERS = 7\r\n",
      "\r\n",
      "\r\n",
      "cifar = tf.keras.datasets.cifar10.load_data()\r\n",
      "(x_train, y_train), (x_test, y_test) = cifar\r\n",
      "\r\n",
      "y_train_ohe = tf.one_hot([i[0] for i in  y_train],10)\r\n",
      "indexes_for_rows = tf.range(0,y_train.shape[0])\r\n",
      "shuffled_indexes = tf.random.shuffle(indexes_for_rows)\r\n",
      "selected_indexes = shuffled_indexes[:TRAINING_SET_SIZE]\r\n",
      "selected_x_train = x_train[selected_indexes,:,:,:]\r\n",
      "selected_y_train_ohe = y_train_ohe.numpy()[selected_indexes,:]\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "mod_with_fc_raw = tf.keras.applications.efficientnet.EfficientNetB7(\r\n",
      "    include_top=True, weights='imagenet', input_tensor=None,\r\n",
      "    input_shape = BASE_MODEL_INPUT_SHAPE, pooling='max', classes=1000\r\n",
      ")\r\n",
      "\r\n",
      "# Make the deepest conv2d layer trainable, leave everything else\r\n",
      "# as not trainable\r\n",
      "for layer in mod_with_fc_raw.layers:\r\n",
      "    layer.trainable = False\r\n",
      "# Last conv2d layer. This we want to train .\r\n",
      "mod_with_fc_raw.layers[-6].trainable = True\r\n",
      "\r\n",
      "# Create the final base model\r\n",
      "# (remove the final Dense and BatchNormalization layers ...) \r\n",
      "efficient_net_b_7_transferable_base_model =    tf.keras.Model(inputs=mod_with_fc_raw.layers[0].input, \r\n",
      "                    outputs=mod_with_fc_raw.layers[-3].output)\r\n",
      "\r\n",
      "model_builder = ResidualMLP(\r\n",
      "                    problem_type = PROBLEM_TYPE,\r\n",
      "                    minimum_learning_rate = MINIMUM_LEARNING_RATE, \r\n",
      "                    maximum_learning_rate = MAXIMUM_LEARNING_RATE, \r\n",
      "                    number_of_learning_rates_to_try =\r\n",
      "                        NUMBER_OF_LEARNING_RATES_TO_TRY, \r\n",
      "                    input_shape = INPUT_SHAPE, \r\n",
      "                    bw_images = False, \r\n",
      "                    base_model = \r\n",
      "                        efficient_net_b_7_transferable_base_model, \r\n",
      "                    base_model_input_shape = BASE_MODEL_INPUT_SHAPE, \r\n",
      "                    flatten_after_base_model = False, \r\n",
      "                    minimum_number_of_blocks = MINIMUM_NUMBER_OF_BLOCKS, \r\n",
      "                    maximum_number_of_blocks = MAXIMUM_NUMBER_OF_BLOCKS, \r\n",
      "                    minimum_number_of_layers_per_block =\r\n",
      "                        MINIMUM_NUMBER_OF_LAYERS_PER_BLOCK, \r\n",
      "                    maximum_number_of_layers_per_block =\r\n",
      "                        MAXIMUM_NUMBER_OF_LAYERS_PER_BLOCK,\r\n",
      "                    minimum_neurons_per_block_layer =\r\n",
      "                        MINIMUM_NEURONS_PER_BLOCK_LAYER, \r\n",
      "                    maximum_neurons_per_block_layer =\r\n",
      "                        MAXIMUM_NEURONS_PER_BLOCK_LAYER, \r\n",
      "                    n_options_of_neurons_per_layer_to_try =\r\n",
      "                        N_OPTIONS_OF_NEURONS_PER_LAYER_TO_TRY, \r\n",
      "                    minimum_neurons_per_block_layer_decay =\r\n",
      "                        MINIMUM_NEURONS_PER_BLOCK_LAYER_DECAY, \r\n",
      "                    maximum_neurons_per_block_layer_decay = \r\n",
      "                        MAXIMUM_NEURONS_PER_BLOCK_LAYER_DECAY, \r\n",
      "                    minimum_dropout_rate_for_bypass_layers =\r\n",
      "                        MINIMUM_DROPOUT_RATE_FOR_BYPASS_LAYERS, \r\n",
      "                    maximim_dropout_rate_for_bypass_layers =\r\n",
      "                        MAXIMIM_DROPOUT_RATE_FOR_BYPASS_LAYERS, \r\n",
      "                    n_options_dropout_rate_for_bypass_layers =\r\n",
      "                        N_OPTIONS_DROPOUT_RATE_FOR_BYPASS_LAYERS,\r\n",
      "                    minimum_inter_block_layers_per_block =\r\n",
      "                        MINIMUM_INTER_BLOCK_LAYERS_PER_BLOCK, \r\n",
      "                    maximum_inter_block_layers_per_block =\r\n",
      "                        MAXIMUM_INTER_BLOCK_LAYERS_PER_BLOCK,\r\n",
      "                    n_options_inter_block_layers_per_block =                        N_OPTIONS_INTER_BLOCK_LAYERS_PER_BLOCK,\r\n",
      "                    minimum_dropout_rate = MINIMUM_DROPOUT_RATE, \r\n",
      "                    maximum_dropout_rate = MAXIMUM_DROPOUT_RATE,\r\n",
      "                    n_options_dropout_rate = N_OPTIONS_DROPOUT_RATE, \r\n",
      "                    minimum_final_dense_layers =\r\n",
      "                        MINIMUM_FINAL_DENSE_LAYERS,\r\n",
      "                    maximum_final_dense_layers =\r\n",
      "                        MAXIMUM_FINAL_DENSE_LAYERS, \r\n",
      "                    n_options_final_dense_layers =\r\n",
      "                        N_OPTIONS_FINAL_DENSE_LAYERS, \r\n",
      "                    number_of_classes = NUMBER_OF_CLASSES,\r\n",
      "                    final_activation = tf.keras.activations.softmax)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "logdir_for_search = os.path.join(\"logs\", RESULTS_DIR_FOR_SEARCH + \"_TB\")\r\n",
      "tensorboard_callback_search =    tf.keras.callbacks.TensorBoard(logdir_for_search, histogram_freq=1)\r\n",
      "\r\n",
      "tuner = kt.Hyperband(\r\n",
      "    model_builder.build_auto_residual_mlp,\r\n",
      "    objective='val_loss',\r\n",
      "    project_name = PROJECT_NAME,\r\n",
      "    max_epochs = MAX_EPOCHS,\r\n",
      "    hyperband_iterations = 2)\r\n",
      "\r\n",
      "\r\n",
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\r\n",
      "\r",
      "    16384/170498071 [..............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "    90112/170498071 [..............................] - ETA: 1:42\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "   434176/170498071 [..............................] - ETA: 42s \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  1335296/170498071 [..............................] - ETA: 23s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  2973696/170498071 [..............................] - ETA: 13s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  4022272/170498071 [..............................] - ETA: 11s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  5152768/170498071 [..............................] - ETA: 10s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  6217728/170498071 [>.............................] - ETA: 10s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  7233536/170498071 [>.............................] - ETA: 9s \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  8364032/170498071 [>.............................] - ETA: 9s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  9478144/170498071 [>.............................] - ETA: 9s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 10559488/170498071 [>.............................] - ETA: 8s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 11689984/170498071 [=>............................] - ETA: 8s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 12804096/170498071 [=>............................] - ETA: 8s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 13950976/170498071 [=>............................] - ETA: 8s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 15114240/170498071 [=>............................] - ETA: 8s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 16293888/170498071 [=>............................] - ETA: 7s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 17457152/170498071 [==>...........................] - ETA: 7s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 18571264/170498071 [==>...........................] - ETA: 7s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 19750912/170498071 [==>...........................] - ETA: 7s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 20897792/170498071 [==>...........................] - ETA: 7s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 22028288/170498071 [==>...........................] - ETA: 7s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 22978560/170498071 [===>..........................] - ETA: 7s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 24141824/170498071 [===>..........................] - ETA: 7s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 25337856/170498071 [===>..........................] - ETA: 7s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 26533888/170498071 [===>..........................] - ETA: 7s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 27729920/170498071 [===>..........................] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 28942336/170498071 [====>.........................] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 30138368/170498071 [====>.........................] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 31137792/170498071 [====>.........................] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 32317440/170498071 [====>.........................] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 33513472/170498071 [====>.........................] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 34709504/170498071 [=====>........................] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 35889152/170498071 [=====>........................] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 37101568/170498071 [=====>........................] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 38297600/170498071 [=====>........................] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 39477248/170498071 [=====>........................] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 40181760/170498071 [======>.......................] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 41754624/170498071 [======>.......................] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 42639360/170498071 [======>.......................] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 43540480/170498071 [======>.......................] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 44425216/170498071 [======>.......................] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 45359104/170498071 [======>.......................] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 46276608/170498071 [=======>......................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 47226880/170498071 [=======>......................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 48128000/170498071 [=======>......................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 49029120/170498071 [=======>......................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 49930240/170498071 [=======>......................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 50814976/170498071 [=======>......................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 51748864/170498071 [========>.....................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 52699136/170498071 [========>.....................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 53665792/170498071 [========>.....................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 54616064/170498071 [========>.....................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 55582720/170498071 [========>.....................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 56532992/170498071 [========>.....................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 57384960/170498071 [=========>....................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 58417152/170498071 [=========>....................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 59367424/170498071 [=========>....................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 60383232/170498071 [=========>....................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 61333504/170498071 [=========>....................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 62316544/170498071 [=========>....................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 63332352/170498071 [==========>...................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 64315392/170498071 [==========>...................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 65298432/170498071 [==========>...................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 66314240/170498071 [==========>...................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 67280896/170498071 [==========>...................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 68231168/170498071 [===========>..................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 69214208/170498071 [===========>..................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 70262784/170498071 [===========>..................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 71229440/170498071 [===========>..................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 72278016/170498071 [===========>..................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 73228288/170498071 [===========>..................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 74276864/170498071 [============>.................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 75259904/170498071 [============>.................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 76062720/170498071 [============>.................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 77144064/170498071 [============>.................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 78209024/170498071 [============>.................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 79175680/170498071 [============>.................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 80060416/170498071 [=============>................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 81100800/170498071 [=============>................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 82092032/170498071 [=============>................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 83124224/170498071 [=============>................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 84189184/170498071 [=============>................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 84959232/170498071 [=============>................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 86007808/170498071 [==============>...............] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 86892544/170498071 [==============>...............] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 88006656/170498071 [==============>...............] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 89038848/170498071 [==============>...............] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 89645056/170498071 [==============>...............] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 90923008/170498071 [==============>...............] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 91643904/170498071 [===============>..............] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 92446720/170498071 [===============>..............] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 93233152/170498071 [===============>..............] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 93970432/170498071 [===============>..............] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 94609408/170498071 [===============>..............] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 95379456/170498071 [===============>..............] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 96231424/170498071 [===============>..............] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 97099776/170498071 [================>.............] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 97951744/170498071 [================>.............] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 98721792/170498071 [================>.............] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 99557376/170498071 [================>.............] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "100376576/170498071 [================>.............] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "101122048/170498071 [================>.............] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "101916672/170498071 [================>.............] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "102735872/170498071 [=================>............] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "103620608/170498071 [=================>............] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "104439808/170498071 [=================>............] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "105144320/170498071 [=================>............] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "105979904/170498071 [=================>............] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "106815488/170498071 [=================>............] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "107716608/170498071 [=================>............] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "108617728/170498071 [==================>...........] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "109404160/170498071 [==================>...........] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "110239744/170498071 [==================>...........] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "110993408/170498071 [==================>...........] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "111796224/170498071 [==================>...........] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "112697344/170498071 [==================>...........] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "113631232/170498071 [==================>...........] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "114499584/170498071 [===================>..........] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "115335168/170498071 [===================>..........] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "116170752/170498071 [===================>..........] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "117071872/170498071 [===================>..........] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "117989376/170498071 [===================>..........] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "118841344/170498071 [===================>..........] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "119676928/170498071 [====================>.........] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "120528896/170498071 [====================>.........] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "121413632/170498071 [====================>.........] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "122265600/170498071 [====================>.........] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "123232256/170498071 [====================>.........] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "124166144/170498071 [====================>.........] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "125050880/170498071 [=====================>........] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "125902848/170498071 [=====================>........] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "126771200/170498071 [=====================>........] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "127655936/170498071 [=====================>........] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "128573440/170498071 [=====================>........] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "129425408/170498071 [=====================>........] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "130310144/170498071 [=====================>........] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "130981888/170498071 [======================>.......] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "131833856/170498071 [======================>.......] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "132554752/170498071 [======================>.......] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "133431296/170498071 [======================>.......] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "134340608/170498071 [======================>.......] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "135225344/170498071 [======================>.......] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "136142848/170498071 [======================>.......] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "137027584/170498071 [=======================>......] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "137961472/170498071 [=======================>......] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "138846208/170498071 [=======================>......] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "139681792/170498071 [=======================>......] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "140648448/170498071 [=======================>......] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "141516800/170498071 [=======================>......] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "142434304/170498071 [========================>.....] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "143302656/170498071 [========================>.....] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "144203776/170498071 [========================>.....] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "145137664/170498071 [========================>.....] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "146038784/170498071 [========================>.....] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "147021824/170498071 [========================>.....] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "147963904/170498071 [=========================>....] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "148840448/170498071 [=========================>....] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "149741568/170498071 [=========================>....] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "150642688/170498071 [=========================>....] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "151543808/170498071 [=========================>....] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "152412160/170498071 [=========================>....] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "153378816/170498071 [=========================>....] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "154279936/170498071 [==========================>...] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "155197440/170498071 [==========================>...] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "156164096/170498071 [==========================>...] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "157016064/170498071 [==========================>...] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "157982720/170498071 [==========================>...] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "158801920/170498071 [==========================>...] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "159784960/170498071 [===========================>..] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "160768000/170498071 [===========================>..] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "161652736/170498071 [===========================>..] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "162652160/170498071 [===========================>..] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "163651584/170498071 [===========================>..] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "164552704/170498071 [===========================>..] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "165437440/170498071 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "166420480/170498071 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "167436288/170498071 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "168353792/170498071 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "169336832/170498071 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "170287104/170498071 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "170500096/170498071 [==============================] - 9s 0us/step\r\n",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "170508288/170498071 [==============================] - 9s 0us/step\r\n",
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb7.h5\r\n",
      "cat: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!cat 2022-02-12_14-48_CIFAR10_EfficientNetB7-ResidualMLP_NAS_SECOND_PASS_SEARCH_python3_shell_log.txt | head -n 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8572b21a",
   "metadata": {},
   "source": [
    "## Now to evauluate the best model found by the task:\n",
    "\n",
    "### First make sure the container we are evaluating this in has all its dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a42ae72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're missing a module:\n",
      "No module named 'pendulum'\n",
      "No problem, we install it...\n",
      "Collecting pendulum\n",
      "  Downloading pendulum-2.1.2-cp38-cp38-manylinux1_x86_64.whl (155 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0,>=2.6 in /usr/local/lib/python3.8/dist-packages (from pendulum) (2.8.2)\n",
      "Collecting pytzdata>=2020.1\n",
      "  Downloading pytzdata-2020.1-py2.py3-none-any.whl (489 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil<3.0,>=2.6->pendulum) (1.15.0)\n",
      "Installing collected packages: pytzdata, pendulum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "WARNING: You are using pip version 21.2.4; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed pendulum-2.1.2 pytzdata-2020.1\n",
      "We're missing a module:\n",
      "No module named 'pandas'\n",
      "No problem, we install it...\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.19.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "WARNING: You are using pip version 21.2.4; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're missing a module:\n",
      "No module named 'keras_tuner'\n",
      "No problem, we install it...\n",
      "Collecting keras_tuner\n",
      "  Downloading keras_tuner-1.1.0-py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from keras_tuner) (21.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from keras_tuner) (1.4.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from keras_tuner) (1.19.4)\n",
      "Collecting kt-legacy\n",
      "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from keras_tuner) (2.26.0)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (from keras_tuner) (7.27.0)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.8/dist-packages (from keras_tuner) (2.6.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (3.0.20)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (5.1.0)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (2.10.0)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (0.1.3)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (5.1.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (0.18.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (58.1.0)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (4.7.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (0.7.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.16->ipython->keras_tuner) (0.8.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect>4.3->ipython->keras_tuner) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras_tuner) (0.2.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->keras_tuner) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->keras_tuner) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->keras_tuner) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->keras_tuner) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests->keras_tuner) (2.0.6)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (0.4.6)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (1.35.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (0.37.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (2.0.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (1.39.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (0.12.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (1.8.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (0.6.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (3.17.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from absl-py>=0.4->tensorboard->keras_tuner) (1.15.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras_tuner) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras_tuner) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras_tuner) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras_tuner) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->keras_tuner) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras_tuner) (3.1.1)\n",
      "Installing collected packages: kt-legacy, keras-tuner\n",
      "Successfully installed keras-tuner-1.1.0 kt-legacy-1.0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "WARNING: You are using pip version 21.2.4; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import argparse\n",
    "# Add parser for common params\n",
    "\n",
    "try:\n",
    "    import pendulum\n",
    "except ModuleNotFoundError as not_found:\n",
    "    print(\"We're missing a module:\")\n",
    "    print(not_found)\n",
    "    print(\"No problem, we install it...\")\n",
    "    subprocess.run(\"pip3 install pendulum\", \n",
    "    \t           shell=True, \n",
    "    \t           check=True)\n",
    "    import pendulum\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ModuleNotFoundError as not_found:\n",
    "    print(\"We're missing a module:\")\n",
    "    print(not_found)\n",
    "    print(\"No problem, we install it...\")\n",
    "    subprocess.run(\"pip3 install pandas\", \n",
    "    \t           shell=True, \n",
    "    \t           check=True)\n",
    "    import pandas as pd\n",
    "\n",
    "try:\n",
    "    import keras_tuner as kt\n",
    "except ModuleNotFoundError as not_found:\n",
    "    print(\"We're missing a module:\")\n",
    "    print(not_found)\n",
    "    print(\"No problem, we install it...\")\n",
    "    subprocess.run(\"pip3 install keras_tuner\", \n",
    "    \t           shell=True, \n",
    "    \t           check=True)\n",
    "    import keras_tuner as kt\n",
    "\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except ModuleNotFoundError as not_found:\n",
    "    print(\"We're missing a module:\")\n",
    "    print(not_found)\n",
    "    print(\"No problem, we install it...\")\n",
    "    subprocess.run(\"pip3 install tensorflow\", \n",
    "    \t           shell=True, \n",
    "    \t           check=True)\n",
    "    import tensorflow as tf\n",
    "\n",
    "from residualmlp.residual_mlp import ResidualMLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9978f729",
   "metadata": {},
   "source": [
    "## Second: Reproduce the environment the task was run in: (This is a crirical step. Some of the, what I like to call 'ultraparmeters' - hyperparameters that fundamentally alter the behavior of the HYPERmodel, not just the models it returns) must be set to the same settings as they were during training, otherwise, our call to Tuner.get_best_model() will NOT reproduce the correct model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d66b0904",
   "metadata": {},
   "outputs": [],
   "source": [
    "Date = '2022-02-12_14_48'\n",
    "PROBLEM_TYPE = 'classification'\n",
    "NUMBER_OF_CLASSES = 10\n",
    "INPUT_SHAPE = (32, 32, 3)\n",
    "BASE_MODEL_INPUT_SHAPE = (600, 600, 3)\n",
    "PROJECT_NAME = 'CIFAR10_EfficientNetB7-ResidualMLP_NAS_THIRD_PASS_SEARCH'\n",
    "TRAINING_SET_SIZE = 50000\n",
    "PATIENCE = 25\n",
    "PATIENCE_MIN_DELTA = 1e-05\n",
    "BATCH_SIZE = 50\n",
    "MAX_EPOCHS = 150\n",
    "RESULTS_DIR_FOR_SEARCH = '2022-02-12_14_48_CIFAR10_EfficientNetB7-ResidualMLP_NAS_THIRD_PASS_SEARCH_SEARCH_RUN'\n",
    "MINIMUM_LEARNING_RATE = 0.0001\n",
    "MAXIMUM_LEARNING_RATE = 0.02\n",
    "NUMBER_OF_LEARNING_RATES_TO_TRY = 7\n",
    "MINIMUM_NUMBER_OF_BLOCKS = 2\n",
    "MAXIMUM_NUMBER_OF_BLOCKS = 3\n",
    "MINIMUM_NUMBER_OF_LAYERS_PER_BLOCK = 2\n",
    "MAXIMUM_NUMBER_OF_LAYERS_PER_BLOCK = 4\n",
    "MINIMUM_NEURONS_PER_BLOCK_LAYER = 70\n",
    "MAXIMUM_NEURONS_PER_BLOCK_LAYER = 140\n",
    "N_OPTIONS_OF_NEURONS_PER_LAYER_TO_TRY = 7\n",
    "MINIMUM_NEURONS_PER_BLOCK_LAYER_DECAY = 15\n",
    "MAXIMUM_NEURONS_PER_BLOCK_LAYER_DECAY = 35\n",
    "MINIMUM_DROPOUT_RATE_FOR_BYPASS_LAYERS = 0.2\n",
    "MAXIMIM_DROPOUT_RATE_FOR_BYPASS_LAYERS = 0.6\n",
    "N_OPTIONS_DROPOUT_RATE_FOR_BYPASS_LAYERS = 3\n",
    "MINIMUM_INTER_BLOCK_LAYERS_PER_BLOCK = 0\n",
    "MAXIMUM_INTER_BLOCK_LAYERS_PER_BLOCK = 85\n",
    "N_OPTIONS_INTER_BLOCK_LAYERS_PER_BLOCK = 7\n",
    "MINIMUM_DROPOUT_RATE = 0.2\n",
    "MAXIMUM_DROPOUT_RATE = 0.6\n",
    "N_OPTIONS_DROPOUT_RATE = 5\n",
    "MINIMUM_FINAL_DENSE_LAYERS = 33\n",
    "MAXIMUM_FINAL_DENSE_LAYERS = 300\n",
    "N_OPTIONS_FINAL_DENSE_LAYERS = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d740839",
   "metadata": {},
   "source": [
    "## Third, reproduce the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18d7fa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 8s 0us/step\n",
      "170508288/170498071 [==============================] - 8s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-13 21:10:08.938581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-13 21:10:08.976250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-13 21:10:08.977179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-13 21:10:08.979524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-13 21:10:08.980386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-13 21:10:08.981139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-13 21:10:09.741206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-13 21:10:09.742090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-13 21:10:09.742846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-13 21:10:09.743605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7261 MB memory:  -> device: 0, name: Quadro P4000, pci bus id: 0000:00:05.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cifar = tf.keras.datasets.cifar10.load_data()\n",
    "(x_train, y_train), (x_test, y_test) = cifar\n",
    "\n",
    "y_train_ohe = tf.one_hot([i[0] for i in  y_train],10)\n",
    "indexes_for_rows = tf.range(0,y_train.shape[0])\n",
    "shuffled_indexes = tf.random.shuffle(indexes_for_rows)\n",
    "selected_indexes = shuffled_indexes[:TRAINING_SET_SIZE]\n",
    "selected_x_train = x_train[selected_indexes,:,:,:]\n",
    "selected_y_train_ohe = y_train_ohe.numpy()[selected_indexes,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547816c8",
   "metadata": {},
   "source": [
    "## Fourth, reproduce the EfficientNetB7 base model used for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "022bbe47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb7.h5\n",
      "268328960/268326632 [==============================] - 12s 0us/step\n",
      "268337152/268326632 [==============================] - 12s 0us/step\n"
     ]
    }
   ],
   "source": [
    "mod_with_fc_raw = tf.keras.applications.efficientnet.EfficientNetB7(\n",
    "    include_top=True, weights='imagenet', input_tensor=None,\n",
    "    input_shape = BASE_MODEL_INPUT_SHAPE, pooling='max', classes=1000\n",
    ")\n",
    "\n",
    "# Make the deepest conv2d layer trainable, leave everything else\n",
    "# as not trainable\n",
    "for layer in mod_with_fc_raw.layers:\n",
    "    layer.trainable = False\n",
    "# Last conv2d layer. This we want to train .\n",
    "mod_with_fc_raw.layers[-6].trainable = True\n",
    "\n",
    "# Create the final base model\n",
    "# (remove the final Dense and BatchNormalization layers ...) \n",
    "efficient_net_b_7_transferable_base_model =    tf.keras.Model(inputs=mod_with_fc_raw.layers[0].input, \n",
    "                    outputs=mod_with_fc_raw.layers[-3].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237ced65",
   "metadata": {},
   "source": [
    "## Fifth: Reporduce the hypermodel and tuner used to perform the neural architecture and hyperparmaeter search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49696af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-13 21:10:35.429367: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2022-02-13 21:10:35.429426: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2022-02-13 21:10:35.430607: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project ./CIFAR10_EfficientNetB7-ResidualMLP_NAS_THIRD_PASS_SEARCH/oracle.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-13 21:10:35.605893: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2022-02-13 21:10:35.606135: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1749] CUPTI activity buffer flushed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All permutations:\n",
      "     number_of_blocks  layers_per_block  neurons_per_block_layer  \\\n",
      "0                   2                 2                       70   \n",
      "1                   2                 2                       70   \n",
      "2                   2                 2                       70   \n",
      "3                   2                 2                       70   \n",
      "4                   2                 2                       70   \n",
      "..                ...               ...                      ...   \n",
      "877                 3                 4                      140   \n",
      "878                 3                 4                      140   \n",
      "879                 3                 4                      140   \n",
      "880                 3                 4                      140   \n",
      "881                 3                 4                      140   \n",
      "\n",
      "     neurons_per_block_layer_decay  \n",
      "0                               15  \n",
      "1                               16  \n",
      "2                               17  \n",
      "3                               18  \n",
      "4                               19  \n",
      "..                             ...  \n",
      "877                             31  \n",
      "878                             32  \n",
      "879                             33  \n",
      "880                             34  \n",
      "881                             35  \n",
      "\n",
      "[882 rows x 4 columns]\n",
      "Valid permutations\n",
      "     number_of_blocks  layers_per_block  neurons_per_block_layer  \\\n",
      "0                   2                 2                       70   \n",
      "1                   2                 2                       70   \n",
      "2                   2                 2                       70   \n",
      "3                   2                 2                       70   \n",
      "4                   2                 2                       70   \n",
      "..                ...               ...                      ...   \n",
      "689                 3                 4                      140   \n",
      "690                 3                 4                      140   \n",
      "691                 3                 4                      140   \n",
      "692                 3                 4                      140   \n",
      "693                 3                 4                      140   \n",
      "\n",
      "     neurons_per_block_layer_decay  valid_block  \n",
      "0                               15         True  \n",
      "1                               16         True  \n",
      "2                               17         True  \n",
      "3                               18         True  \n",
      "4                               19         True  \n",
      "..                             ...          ...  \n",
      "689                             30         True  \n",
      "690                             31         True  \n",
      "691                             32         True  \n",
      "692                             33         True  \n",
      "693                             34         True  \n",
      "\n",
      "[694 rows x 5 columns]\n",
      "Valid permutations with blocks column\n",
      "     number_of_blocks  layers_per_block  neurons_per_block_layer  \\\n",
      "0                   2                 2                       70   \n",
      "1                   2                 2                       70   \n",
      "2                   2                 2                       70   \n",
      "3                   2                 2                       70   \n",
      "4                   2                 2                       70   \n",
      "..                ...               ...                      ...   \n",
      "689                 3                 4                      140   \n",
      "690                 3                 4                      140   \n",
      "691                 3                 4                      140   \n",
      "692                 3                 4                      140   \n",
      "693                 3                 4                      140   \n",
      "\n",
      "     neurons_per_block_layer_decay  valid_block  \\\n",
      "0                               15         True   \n",
      "1                               16         True   \n",
      "2                               17         True   \n",
      "3                               18         True   \n",
      "4                               19         True   \n",
      "..                             ...          ...   \n",
      "689                             30         True   \n",
      "690                             31         True   \n",
      "691                             32         True   \n",
      "692                             33         True   \n",
      "693                             34         True   \n",
      "\n",
      "                                         blocks  \n",
      "0                    [[2, 70, 15], [2, 70, 15]]  \n",
      "1                    [[2, 70, 16], [2, 70, 16]]  \n",
      "2                    [[2, 70, 17], [2, 70, 17]]  \n",
      "3                    [[2, 70, 18], [2, 70, 18]]  \n",
      "4                    [[2, 70, 19], [2, 70, 19]]  \n",
      "..                                          ...  \n",
      "689  [[4, 140, 30], [4, 140, 30], [4, 140, 30]]  \n",
      "690  [[4, 140, 31], [4, 140, 31], [4, 140, 31]]  \n",
      "691  [[4, 140, 32], [4, 140, 32], [4, 140, 32]]  \n",
      "692  [[4, 140, 33], [4, 140, 33], [4, 140, 33]]  \n",
      "693  [[4, 140, 34], [4, 140, 34], [4, 140, 34]]  \n",
      "\n",
      "[694 rows x 6 columns]\n",
      "[[2, 70, 15], [2, 70, 15]]\n",
      "INFO:tensorflow:Reloading Tuner from ./CIFAR10_EfficientNetB7-ResidualMLP_NAS_THIRD_PASS_SEARCH/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "model_builder = ResidualMLP(\n",
    "                    problem_type = PROBLEM_TYPE,\n",
    "                    minimum_learning_rate = MINIMUM_LEARNING_RATE, \n",
    "                    maximum_learning_rate = MAXIMUM_LEARNING_RATE, \n",
    "                    number_of_learning_rates_to_try =\n",
    "                        NUMBER_OF_LEARNING_RATES_TO_TRY, \n",
    "                    input_shape = INPUT_SHAPE, \n",
    "                    bw_images = False, \n",
    "                    base_model = \n",
    "                        efficient_net_b_7_transferable_base_model, \n",
    "                    base_model_input_shape = BASE_MODEL_INPUT_SHAPE, \n",
    "                    flatten_after_base_model = False, \n",
    "                    minimum_number_of_blocks = MINIMUM_NUMBER_OF_BLOCKS, \n",
    "                    maximum_number_of_blocks = MAXIMUM_NUMBER_OF_BLOCKS, \n",
    "                    minimum_number_of_layers_per_block =\n",
    "                        MINIMUM_NUMBER_OF_LAYERS_PER_BLOCK, \n",
    "                    maximum_number_of_layers_per_block =\n",
    "                        MAXIMUM_NUMBER_OF_LAYERS_PER_BLOCK,\n",
    "                    minimum_neurons_per_block_layer =\n",
    "                        MINIMUM_NEURONS_PER_BLOCK_LAYER, \n",
    "                    maximum_neurons_per_block_layer =\n",
    "                        MAXIMUM_NEURONS_PER_BLOCK_LAYER, \n",
    "                    n_options_of_neurons_per_layer_to_try =\n",
    "                        N_OPTIONS_OF_NEURONS_PER_LAYER_TO_TRY, \n",
    "                    minimum_neurons_per_block_layer_decay =\n",
    "                        MINIMUM_NEURONS_PER_BLOCK_LAYER_DECAY, \n",
    "                    maximum_neurons_per_block_layer_decay = \n",
    "                        MAXIMUM_NEURONS_PER_BLOCK_LAYER_DECAY, \n",
    "                    minimum_dropout_rate_for_bypass_layers =\n",
    "                        MINIMUM_DROPOUT_RATE_FOR_BYPASS_LAYERS, \n",
    "                    maximim_dropout_rate_for_bypass_layers =\n",
    "                        MAXIMIM_DROPOUT_RATE_FOR_BYPASS_LAYERS, \n",
    "                    n_options_dropout_rate_for_bypass_layers =\n",
    "                        N_OPTIONS_DROPOUT_RATE_FOR_BYPASS_LAYERS,\n",
    "                    minimum_inter_block_layers_per_block =\n",
    "                        MINIMUM_INTER_BLOCK_LAYERS_PER_BLOCK, \n",
    "                    maximum_inter_block_layers_per_block =\n",
    "                        MAXIMUM_INTER_BLOCK_LAYERS_PER_BLOCK,\n",
    "                    n_options_inter_block_layers_per_block =                        N_OPTIONS_INTER_BLOCK_LAYERS_PER_BLOCK,\n",
    "                    minimum_dropout_rate = MINIMUM_DROPOUT_RATE, \n",
    "                    maximum_dropout_rate = MAXIMUM_DROPOUT_RATE,\n",
    "                    n_options_dropout_rate = N_OPTIONS_DROPOUT_RATE, \n",
    "                    minimum_final_dense_layers =\n",
    "                        MINIMUM_FINAL_DENSE_LAYERS,\n",
    "                    maximum_final_dense_layers =\n",
    "                        MAXIMUM_FINAL_DENSE_LAYERS, \n",
    "                    n_options_final_dense_layers =\n",
    "                        N_OPTIONS_FINAL_DENSE_LAYERS, \n",
    "                    number_of_classes = NUMBER_OF_CLASSES,\n",
    "                    final_activation = tf.keras.activations.softmax)\n",
    "\n",
    "\n",
    "\n",
    "logdir_for_search = os.path.join(\"logs\", RESULTS_DIR_FOR_SEARCH + \"_TB\")\n",
    "tensorboard_callback_search =    tf.keras.callbacks.TensorBoard(logdir_for_search, histogram_freq=1)\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    model_builder.build_auto_residual_mlp,\n",
    "    objective='val_loss',\n",
    "    project_name = PROJECT_NAME,\n",
    "    max_epochs = MAX_EPOCHS,\n",
    "    hyperband_iterations = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52be059a",
   "metadata": {},
   "source": [
    "## Get info about the best models from the search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a1e0367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in ./CIFAR10_EfficientNetB7-ResidualMLP_NAS_THIRD_PASS_SEARCH\n",
      "Showing 10 best trials\n",
      "Objective(name='val_loss', direction='min')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001245791948180787\n",
      "blocks: 79\n",
      "bypass_layers_units: 81\n",
      "inter_block_layers: 85\n",
      "final_dense_layers: 166\n",
      "b_norm_or_dropout_residual_bypass_layers: bnorm\n",
      "dropout_rate_for_bypass_layers: 0.2\n",
      "b_norm_or_dropout_last_layers: bnorm\n",
      "dropout_rate: 0.2\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.23214344680309296\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0006127124845172436\n",
      "blocks: 270\n",
      "bypass_layers_units: 105\n",
      "inter_block_layers: 0\n",
      "final_dense_layers: 33\n",
      "b_norm_or_dropout_residual_bypass_layers: bnorm\n",
      "dropout_rate_for_bypass_layers: 0.4\n",
      "b_norm_or_dropout_last_layers: bnorm\n",
      "dropout_rate: 0.4\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.2366037219762802\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.00014524140478609202\n",
      "blocks: 34\n",
      "bypass_layers_units: 128\n",
      "inter_block_layers: 85\n",
      "final_dense_layers: 211\n",
      "b_norm_or_dropout_residual_bypass_layers: bnorm\n",
      "dropout_rate_for_bypass_layers: 0.6\n",
      "b_norm_or_dropout_last_layers: bnorm\n",
      "dropout_rate: 0.2\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.23940517008304596\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.00023077561751748773\n",
      "blocks: 277\n",
      "bypass_layers_units: 81\n",
      "inter_block_layers: 0\n",
      "final_dense_layers: 211\n",
      "b_norm_or_dropout_residual_bypass_layers: dropout\n",
      "dropout_rate_for_bypass_layers: 0.2\n",
      "b_norm_or_dropout_last_layers: bnorm\n",
      "dropout_rate: 0.5\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.24102962017059326\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.00023947195749221193\n",
      "blocks: 8\n",
      "bypass_layers_units: 128\n",
      "inter_block_layers: 85\n",
      "final_dense_layers: 33\n",
      "b_norm_or_dropout_residual_bypass_layers: bnorm\n",
      "dropout_rate_for_bypass_layers: 0.2\n",
      "b_norm_or_dropout_last_layers: dropout\n",
      "dropout_rate: 0.2\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.24898336827754974\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.00030913073725729744\n",
      "blocks: 453\n",
      "bypass_layers_units: 105\n",
      "inter_block_layers: 85\n",
      "final_dense_layers: 300\n",
      "b_norm_or_dropout_residual_bypass_layers: bnorm\n",
      "dropout_rate_for_bypass_layers: 0.4\n",
      "b_norm_or_dropout_last_layers: dropout\n",
      "dropout_rate: 0.3\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.25448015332221985\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0010054114650258042\n",
      "blocks: 64\n",
      "bypass_layers_units: 116\n",
      "inter_block_layers: 70\n",
      "final_dense_layers: 211\n",
      "b_norm_or_dropout_residual_bypass_layers: dropout\n",
      "dropout_rate_for_bypass_layers: 0.2\n",
      "b_norm_or_dropout_last_layers: dropout\n",
      "dropout_rate: 0.5\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.25815945863723755\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0003236462676517471\n",
      "blocks: 2\n",
      "bypass_layers_units: 140\n",
      "inter_block_layers: 42\n",
      "final_dense_layers: 77\n",
      "b_norm_or_dropout_residual_bypass_layers: dropout\n",
      "dropout_rate_for_bypass_layers: 0.4\n",
      "b_norm_or_dropout_last_layers: dropout\n",
      "dropout_rate: 0.4\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.26008978486061096\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0002703715159192416\n",
      "blocks: 591\n",
      "bypass_layers_units: 116\n",
      "inter_block_layers: 56\n",
      "final_dense_layers: 77\n",
      "b_norm_or_dropout_residual_bypass_layers: bnorm\n",
      "dropout_rate_for_bypass_layers: 0.2\n",
      "b_norm_or_dropout_last_layers: dropout\n",
      "dropout_rate: 0.5\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.26252010464668274\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.00017980360673973948\n",
      "blocks: 683\n",
      "bypass_layers_units: 93\n",
      "inter_block_layers: 56\n",
      "final_dense_layers: 33\n",
      "b_norm_or_dropout_residual_bypass_layers: bnorm\n",
      "dropout_rate_for_bypass_layers: 0.6\n",
      "b_norm_or_dropout_last_layers: dropout\n",
      "dropout_rate: 0.2\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.26301461458206177\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a24f3a0",
   "metadata": {},
   "source": [
    "## Let's examine the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88c5f959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All permutations:\n",
      "     number_of_blocks  layers_per_block  neurons_per_block_layer  \\\n",
      "0                   2                 2                       70   \n",
      "1                   2                 2                       70   \n",
      "2                   2                 2                       70   \n",
      "3                   2                 2                       70   \n",
      "4                   2                 2                       70   \n",
      "..                ...               ...                      ...   \n",
      "877                 3                 4                      140   \n",
      "878                 3                 4                      140   \n",
      "879                 3                 4                      140   \n",
      "880                 3                 4                      140   \n",
      "881                 3                 4                      140   \n",
      "\n",
      "     neurons_per_block_layer_decay  \n",
      "0                               15  \n",
      "1                               16  \n",
      "2                               17  \n",
      "3                               18  \n",
      "4                               19  \n",
      "..                             ...  \n",
      "877                             31  \n",
      "878                             32  \n",
      "879                             33  \n",
      "880                             34  \n",
      "881                             35  \n",
      "\n",
      "[882 rows x 4 columns]\n",
      "Valid permutations\n",
      "     number_of_blocks  layers_per_block  neurons_per_block_layer  \\\n",
      "0                   2                 2                       70   \n",
      "1                   2                 2                       70   \n",
      "2                   2                 2                       70   \n",
      "3                   2                 2                       70   \n",
      "4                   2                 2                       70   \n",
      "..                ...               ...                      ...   \n",
      "689                 3                 4                      140   \n",
      "690                 3                 4                      140   \n",
      "691                 3                 4                      140   \n",
      "692                 3                 4                      140   \n",
      "693                 3                 4                      140   \n",
      "\n",
      "     neurons_per_block_layer_decay  valid_block  \n",
      "0                               15         True  \n",
      "1                               16         True  \n",
      "2                               17         True  \n",
      "3                               18         True  \n",
      "4                               19         True  \n",
      "..                             ...          ...  \n",
      "689                             30         True  \n",
      "690                             31         True  \n",
      "691                             32         True  \n",
      "692                             33         True  \n",
      "693                             34         True  \n",
      "\n",
      "[694 rows x 5 columns]\n",
      "Valid permutations with blocks column\n",
      "     number_of_blocks  layers_per_block  neurons_per_block_layer  \\\n",
      "0                   2                 2                       70   \n",
      "1                   2                 2                       70   \n",
      "2                   2                 2                       70   \n",
      "3                   2                 2                       70   \n",
      "4                   2                 2                       70   \n",
      "..                ...               ...                      ...   \n",
      "689                 3                 4                      140   \n",
      "690                 3                 4                      140   \n",
      "691                 3                 4                      140   \n",
      "692                 3                 4                      140   \n",
      "693                 3                 4                      140   \n",
      "\n",
      "     neurons_per_block_layer_decay  valid_block  \\\n",
      "0                               15         True   \n",
      "1                               16         True   \n",
      "2                               17         True   \n",
      "3                               18         True   \n",
      "4                               19         True   \n",
      "..                             ...          ...   \n",
      "689                             30         True   \n",
      "690                             31         True   \n",
      "691                             32         True   \n",
      "692                             33         True   \n",
      "693                             34         True   \n",
      "\n",
      "                                         blocks  \n",
      "0                    [[2, 70, 15], [2, 70, 15]]  \n",
      "1                    [[2, 70, 16], [2, 70, 16]]  \n",
      "2                    [[2, 70, 17], [2, 70, 17]]  \n",
      "3                    [[2, 70, 18], [2, 70, 18]]  \n",
      "4                    [[2, 70, 19], [2, 70, 19]]  \n",
      "..                                          ...  \n",
      "689  [[4, 140, 30], [4, 140, 30], [4, 140, 30]]  \n",
      "690  [[4, 140, 31], [4, 140, 31], [4, 140, 31]]  \n",
      "691  [[4, 140, 32], [4, 140, 32], [4, 140, 32]]  \n",
      "692  [[4, 140, 33], [4, 140, 33], [4, 140, 33]]  \n",
      "693  [[4, 140, 34], [4, 140, 34], [4, 140, 34]]  \n",
      "\n",
      "[694 rows x 6 columns]\n",
      "[[2, 105, 32], [2, 105, 32]]\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "best_model = tuner.get_best_models(1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7ebac8",
   "metadata": {},
   "source": [
    "## It appears that the best model is:\n",
    "\n",
    "Blocks option: [[2,105,32],[2,105,32]]. This means:\n",
    "1. There are 2 ResidualMLP blocks of 2 block body layers.\n",
    "2. Each block having a head layer Dense(105) within the body of the residual block and Dense(73) on the second layer.\n",
    "3. The residual bypass path for both ResidualMLP blocks has one Dense(81) layer.\n",
    "4. There is one Dense(85) layer in between the 2 residual blocks.\n",
    "5. There is one Dense(166) layer after the last block before the output layer.\n",
    "6. All sequential Dense layers were striated with BatchNormalization() layers. \n",
    "7. The learning rate was 0.00012457\n",
    "8. It appears the model only needed to train for 2 epochs to reach the accuracy it did!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f14eb57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "resizing (Resizing)             (None, 600, 600, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model (Functional)              (None, 2560)         64097687    resizing[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 105)          268905      model[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 105)          420         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 105)          11130       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 105)          420         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 105)          420         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 73)           7738        batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 81)           8586        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 73)           292         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 81)           324         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 154)          0           batch_normalization_4[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 85)           13175       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 85)           340         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 105)          9030        batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 105)          420         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 105)          11130       batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 105)          420         dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 105)          420         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 73)           7738        batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 81)           8586        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 73)           292         dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 81)           324         dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 154)          0           batch_normalization_10[0][0]     \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 166)          25730       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 166)          664         dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 10)           1670        batch_normalization_11[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 64,475,861\n",
      "Trainable params: 2,014,196\n",
      "Non-trainable params: 62,461,665\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a7a584",
   "metadata": {},
   "source": [
    "## Let's evalate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6e12a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-13 21:33:50.882766: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-02-13 21:33:58.064038: I tensorflow/stream_executor/cuda/cuda_dnn.cc:381] Loaded cuDNN version 8204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 955s 3s/step - loss: 0.2548 - top_1_categorical_accuracy: 0.9280 - top_2_categorical_accuracy: 0.9750 - top_3_categorical_accuracy: 0.9882 - top_4_categorical_accuracy: 0.9934 - top_5_categorical_accuracy: 0.9960 - top_6_categorical_accuracy: 0.9973 - top_7_categorical_accuracy: 0.9983 - top_8_categorical_accuracy: 0.9990 - top_9_categorical_accuracy: 0.9995 - precision: 0.9365 - recall: 0.9220 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.25477176904678345,\n",
       " 0.9279999732971191,\n",
       " 0.9750000238418579,\n",
       " 0.9882000088691711,\n",
       " 0.993399977684021,\n",
       " 0.9959999918937683,\n",
       " 0.9973000288009644,\n",
       " 0.9983000159263611,\n",
       " 0.9990000128746033,\n",
       " 0.9994999766349792,\n",
       " 0.9365159869194031,\n",
       " 0.921999990940094,\n",
       " 0.0]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_ohe = tf.one_hot([i[0] for i in  y_test],10)\n",
    "best_model.evaluate(x_test,y_test_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b882050b",
   "metadata": {},
   "source": [
    "## It looks like I almost reached 93% top-1-categorical-accuracy: 0.9279999732971191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df69b5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-13 22:25:13.475689: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: 2022-02-12_14_48_interim_best_model_EXPORTED_MODEL/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    }
   ],
   "source": [
    "DATE = '2022-02-12_14_48' # Date of training task execution...\n",
    "best_model.save(f'{DATE}_interim_best_model_EXPORTED_MODEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eec64d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
