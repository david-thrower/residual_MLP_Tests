{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d19c0e83",
   "metadata": {},
   "source": [
    "# Cold Start Neural Architecture Search Example: EfficientNetB7-ResidualMLP using hyperband (on CIFAR10 dataset): Best Model Evaluation:\n",
    "\n",
    "## By cold start I mean the search does not rely on a memory of or aggregate data from a history of past NAS projects.\n",
    "\n",
    "1. Ran: task_trigger.py (seen in next cell)\n",
    "2. The process task_trigger.py parsed and executed the shell script: 2022-02-09_04-29_CIFAR10_EfficientNetB7-ResidualMLP_NAS_SECOND_PASS_SEARCH_task_trigger.sh (The next cell)\n",
    "3. The shell script passed the configured parameters to and executed task.py, our Neural Architecture Search and training task.\n",
    "4. This used my python package > module > Class > method: residualmlp.residual_mlp.ResidualMLP.build_auto_residual_mlp. This module is seen in the next cell. The method .build_auto_residual_mlp abstracts away and parameterizes the ResidualMLP neural archtiecture into a selection from a single ordered list of positive integers, in ascending order by model complexity. The ResidualMLP architecture is MLP architecture with strategically arranged skip connections and other nonlinear paths through a series Dense layers. This method described is used to to recursively create tandem EfficientNetB7-ResidualMLP via an integration with the hyperparameter optimization engine keras_tuner (this is done in in task.py) which runs a Hyperband neural architecture search and hyperparameter optimization.\n",
    "\n",
    "## Here, we evaluate the best model returned from the successful trials of training / search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8463e4b4",
   "metadata": {},
   "source": [
    "## task_trigger.py (First step in the cascade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fb40354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"Runs task.py, is a more convenient way to set up the parameters than\r\n",
      "manually writing a shell script.\"\"\"\r\n",
      "\r\n",
      "import subprocess\r\n",
      "preliminary_install_commands = [\"pip3 install --upgrade pip\",\r\n",
      "                                \"pip3 install pendulum\"]\r\n",
      "for cmd in preliminary_install_commands:\r\n",
      "    subprocess.run(cmd,\r\n",
      "                   shell=True,\r\n",
      "                   check=True)\r\n",
      "import pendulum\r\n",
      "\r\n",
      "# Configure the run with this dict.\r\n",
      "# Always enter boolean and floats as strings.\r\n",
      "\r\n",
      "TIME_STAMP = pendulum.now().\\\r\n",
      "                    \t__str__().\\\r\n",
      "                    \treplace('T','_').\\\r\n",
      "                    \treplace(':','-')[:16]\r\n",
      "\r\n",
      "HPARAMS = {\r\n",
      "    'project_name':\"CIFAR10_EfficientNetB7-ResidualMLP_NAS_SECOND_PASS_SEARCH\",\r\n",
      "    'training_set_size':50000,\r\n",
      "    'patience':25,\r\n",
      "    'patience_min_delta':0.00001,\r\n",
      "    'batch_size':50,\r\n",
      "    'max_epochs':150,\r\n",
      "    'minimum_learning_rate':0.00007,\r\n",
      "    'maximum_learning_rate':0.7,\r\n",
      "    'number_of_learning_rates_to_try':7,\r\n",
      "    'minimum_number_of_blocks':1,\r\n",
      "    'maximum_number_of_blocks':5,\r\n",
      "    'minimum_number_of_layers_per_block':2,\r\n",
      "    'maximum_number_of_layers_per_block':4,\r\n",
      "    'minimum_neurons_per_block_layer':60,\r\n",
      "    'maximum_neurons_per_block_layer':110,\r\n",
      "    'n_options_of_neurons_per_layer_to_try':7,\r\n",
      "    'minimum_neurons_per_block_layer_decay':10,\r\n",
      "    'maximum_neurons_per_block_layer_decay':40,\r\n",
      "    'minimum_dropout_rate_for_bypass_layers':0.2,\r\n",
      "    'maximim_dropout_rate_for_bypass_layers':.6,\r\n",
      "    'n_options_dropout_rate_for_bypass_layers':3,\r\n",
      "    'minimum_inter_block_layers_per_block':0,\r\n",
      "    'maximum_inter_block_layers_per_block':85,\r\n",
      "    'n_options_inter_block_layers_per_block':7,\r\n",
      "    'minimum_dropout_rate':0.2,\r\n",
      "    'maximum_dropout_rate':.4,\r\n",
      "    'n_options_dropout_rate':5,\r\n",
      "    'minimum_final_dense_layers':0,\r\n",
      "    'maximum_final_dense_layers':200,\r\n",
      "    'n_options_final_dense_layers':7\r\n",
      "}\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    \r\n",
      "    BASE_FILE_NAME = f\"{TIME_STAMP}_CIFAR10_EfficientNetB7-ResidualMLP_NAS_SECOND_PASS_SEARCH\"\r\n",
      "    SHELL_LOGS_FILE_NAME = f\"{BASE_FILE_NAME}_python3_shell_log.txt\"\r\n",
      "    SHELL_SCRIPT_NAME = f\"{BASE_FILE_NAME}_task_trigger.sh\"\r\n",
      "    \r\n",
      "\r\n",
      "    back_slash = \"\\\\\"\r\n",
      "    shell_script_content = f\"python3 task.py {back_slash}\" + \"\\n\"\r\n",
      "    \r\n",
      "    for key, value in HPARAMS.items():\r\n",
      "        shell_script_content += f\"    --{key} '{value}'{back_slash}\" + \"\\n\"\r\n",
      "    shell_script_content = shell_script_content[:-2] + \"\\n\"\r\n",
      "    print(shell_script_content)\r\n",
      "\r\n",
      "    \r\n",
      "    with open(SHELL_SCRIPT_NAME,'w',encoding=\"utf8\") as f:\r\n",
      "        f.write(shell_script_content)\r\n",
      "    \r\n",
      "    command = f\"sh {SHELL_SCRIPT_NAME} >> {SHELL_LOGS_FILE_NAME} &\"\r\n",
      "    subprocess.run(command,\r\n",
      "                   shell=True,\r\n",
      "                   check=True)"
     ]
    }
   ],
   "source": [
    "!cat task_trigger.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "025cf36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3 task.py \\\r\n",
      "    --project_name 'CIFAR10_EfficientNetB7-ResidualMLP_NAS_SECOND_PASS_SEARCH'\\\r\n",
      "    --training_set_size '50000'\\\r\n",
      "    --patience '25'\\\r\n",
      "    --patience_min_delta '1e-05'\\\r\n",
      "    --batch_size '50'\\\r\n",
      "    --max_epochs '150'\\\r\n",
      "    --minimum_learning_rate '7e-05'\\\r\n",
      "    --maximum_learning_rate '0.7'\\\r\n",
      "    --number_of_learning_rates_to_try '7'\\\r\n",
      "    --minimum_number_of_blocks '1'\\\r\n",
      "    --maximum_number_of_blocks '5'\\\r\n",
      "    --minimum_number_of_layers_per_block '2'\\\r\n",
      "    --maximum_number_of_layers_per_block '4'\\\r\n",
      "    --minimum_neurons_per_block_layer '60'\\\r\n",
      "    --maximum_neurons_per_block_layer '110'\\\r\n",
      "    --n_options_of_neurons_per_layer_to_try '7'\\\r\n",
      "    --minimum_neurons_per_block_layer_decay '10'\\\r\n",
      "    --maximum_neurons_per_block_layer_decay '40'\\\r\n",
      "    --minimum_dropout_rate_for_bypass_layers '0.2'\\\r\n",
      "    --maximim_dropout_rate_for_bypass_layers '0.6'\\\r\n",
      "    --n_options_dropout_rate_for_bypass_layers '3'\\\r\n",
      "    --minimum_inter_block_layers_per_block '0'\\\r\n",
      "    --maximum_inter_block_layers_per_block '85'\\\r\n",
      "    --n_options_inter_block_layers_per_block '7'\\\r\n",
      "    --minimum_dropout_rate '0.2'\\\r\n",
      "    --maximum_dropout_rate '0.4'\\\r\n",
      "    --n_options_dropout_rate '5'\\\r\n",
      "    --minimum_final_dense_layers '0'\\\r\n",
      "    --maximum_final_dense_layers '200'\\\r\n",
      "    --n_options_final_dense_layers '7'\r\n"
     ]
    }
   ],
   "source": [
    "!cat 2022-02-09_04-29_CIFAR10_EfficientNetB7-ResidualMLP_NAS_SECOND_PASS_SEARCH_task_trigger.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14c5c5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\r\n",
      "import subprocess\r\n",
      "import argparse\r\n",
      "# Add parser for common params\r\n",
      "\r\n",
      "try:\r\n",
      "    import pendulum\r\n",
      "except ModuleNotFoundError as not_found:\r\n",
      "    print(\"We're missing a module:\")\r\n",
      "    print(not_found)\r\n",
      "    print(\"No problem, we install it...\")\r\n",
      "    subprocess.run(\"pip3 install pendulum\", \r\n",
      "    \t           shell=True, \r\n",
      "    \t           check=True)\r\n",
      "    import pendulum\r\n",
      "\r\n",
      "try:\r\n",
      "    import pandas as pd\r\n",
      "except ModuleNotFoundError as not_found:\r\n",
      "    print(\"We're missing a module:\")\r\n",
      "    print(not_found)\r\n",
      "    print(\"No problem, we install it...\")\r\n",
      "    subprocess.run(\"pip3 install pandas\", \r\n",
      "    \t           shell=True, \r\n",
      "    \t           check=True)\r\n",
      "    import pandas as pd\r\n",
      "\r\n",
      "try:\r\n",
      "    import keras_tuner as kt\r\n",
      "except ModuleNotFoundError as not_found:\r\n",
      "    print(\"We're missing a module:\")\r\n",
      "    print(not_found)\r\n",
      "    print(\"No problem, we install it...\")\r\n",
      "    subprocess.run(\"pip3 install keras_tuner\", \r\n",
      "    \t           shell=True, \r\n",
      "    \t           check=True)\r\n",
      "    import keras_tuner as kt\r\n",
      "\r\n",
      "\r\n",
      "try:\r\n",
      "    import tensorflow as tf\r\n",
      "except ModuleNotFoundError as not_found:\r\n",
      "    print(\"We're missing a module:\")\r\n",
      "    print(not_found)\r\n",
      "    print(\"No problem, we install it...\")\r\n",
      "    subprocess.run(\"pip3 install tensorflow\", \r\n",
      "    \t           shell=True, \r\n",
      "    \t           check=True)\r\n",
      "    import tensorflow as tf\r\n",
      "\r\n",
      "from residualmlp.residual_mlp import ResidualMLP\r\n",
      "\r\n",
      "parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "    \"--project_name\", type=str, \r\n",
      "    help=\"Name for this project.\",\r\n",
      "    default=\"CIFAR10_EfficientNetB7-ResidualMLP_NAS_AUGMENTED_SPACE\")\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "    \"--training_set_size\", \r\n",
      "    type=int,\r\n",
      "    help=\"Training set size (how many observations).\",\r\n",
      "    default=50000)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "    \"--patience\",\r\n",
      "    type=int,\r\n",
      "    help=\"How many epochs with no improved performance before the \"\r\n",
      "        \"early stopping callback stops further training?\",\r\n",
      "    default=25)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "    \"--patience_min_delta\",\r\n",
      "    type=float,\r\n",
      "    help=\"How sensitive should the early stopping callback be\"\r\n",
      "        \"  to change?\",\r\n",
      "    default=0.00001)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "    \"--batch_size\",\r\n",
      "    type=int,\r\n",
      "    help=\"How many observations to train with...\",\r\n",
      "    default=50)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "    \"--max_epochs\",\r\n",
      "    type=int,\r\n",
      "    help=\"max_epochs: Integer, the maximum number of epochs to train one \"\r\n",
      "         \"model. It is recommended to set this to a value slightly higher \"\r\n",
      "         \"than the expected epochs to convergence for your largest Model, \"\r\n",
      "         \"and to use early stopping during training (for example, via .\",\r\n",
      "         default=50)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--minimum_learning_rate\",\r\n",
      "        type=float,\r\n",
      "        help=\"Lowest learning rate to try?\",\r\n",
      "        default = 0.00007)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--maximum_learning_rate\",\r\n",
      "        type=float,\r\n",
      "        help=\"Highest learning rate to try?\",\r\n",
      "        default = 0.7)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--number_of_learning_rates_to_try\",\r\n",
      "        type=int,\r\n",
      "        help=\"How many learning rate to try (maximum)?\",\r\n",
      "        default = 7)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--minimum_number_of_blocks\",\r\n",
      "        type=int,\r\n",
      "        help=\"Minimum number of ResidualMLP blocks in neural architectures \"\r\n",
      "             \"to try?\",\r\n",
      "        default = 1)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--maximum_number_of_blocks\",\r\n",
      "        type=int,\r\n",
      "        help=\"Maximum number of ResidualMLP blocks in neural architectures \"\r\n",
      "              \"to try?\",\r\n",
      "        default = 8)\r\n",
      "\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--minimum_number_of_layers_per_block\",\r\n",
      "        type=int,\r\n",
      "        help=\"Minimum number of layers to try in each ResidualMLP block in \"\r\n",
      "             \"the neural architectures to try?\",\r\n",
      "        default = 1)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--maximum_number_of_layers_per_block\",\r\n",
      "        type=int,\r\n",
      "        help=\"Maximum number of layers to try in each ResidualMLP block in \"\r\n",
      "             \"the neural architectures to try?\",\r\n",
      "        default = 8)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--minimum_neurons_per_block_layer\",\r\n",
      "        type=int,\r\n",
      "        help=\"Minimum number of neurons to try in the Dense layers in \"\r\n",
      "             \"each ResidualMLP block in the neural architectures to tried?\",\r\n",
      "        default = 30)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--maximum_neurons_per_block_layer\",\r\n",
      "        type=int,\r\n",
      "        help=\"Maximum number of neurons to try in the Dense layers in \"\r\n",
      "             \"each ResidualMLP block in the neural architectures to tried?\",\r\n",
      "        default = 130)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--n_options_of_neurons_per_layer_to_try\",\r\n",
      "        type=int,\r\n",
      "        help=\"How many different numbers of neurons (at most) to try in \"\r\n",
      "             \"the Dense layers try in each ResidualMLP block in the neural \"\r\n",
      "             \"architectures to tried?\",\r\n",
      "        default = 7)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--minimum_neurons_per_block_layer_decay\",\r\n",
      "        type=int,\r\n",
      "        help=\"Lowest decay in number of neurons (n less neurons than the \"\r\n",
      "             \"last layer) to try in the Dense layers try in each \"\r\n",
      "             \"ResidualMLP block in the neural \"\r\n",
      "             \"architectures to tried?\",\r\n",
      "        default = 0)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--maximum_neurons_per_block_layer_decay\",\r\n",
      "        type=int,\r\n",
      "        help=\"Highest decay in number of neurons (n less neurons than the \"\r\n",
      "             \"last layer) to try in the Dense layers try in each \"\r\n",
      "             \"ResidualMLP block in the neural \"\r\n",
      "             \"architectures to tried?\",\r\n",
      "        default = 50)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--minimum_dropout_rate_for_bypass_layers\",\r\n",
      "        type=float,\r\n",
      "        help=\"Lowest dropout rate to try for dropout layers located in \"\r\n",
      "             \"residual byass layers?\",\r\n",
      "        default = 0.01)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--maximim_dropout_rate_for_bypass_layers\",\r\n",
      "        type=float,\r\n",
      "        help=\"Highest dropout rate to try for dropout layers located in \"\r\n",
      "             \"residual byass layers?\",\r\n",
      "        default = 0.7)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--n_options_dropout_rate_for_bypass_layers\",\r\n",
      "        type=int,\r\n",
      "        help=\"How many dropout rates to try for dropout layers located in \"\r\n",
      "             \"residual byass layers?\",\r\n",
      "        default = 7)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--minimum_inter_block_layers_per_block\",\r\n",
      "        type=int,\r\n",
      "        help=\"Minimum number of neurons per Dense layer for Dense layers \"\r\n",
      "             \"inserted between ResidualMLP blocks? 0 Will not insert a \"\r\n",
      "             \"layer if selected.\",\r\n",
      "        default = 0)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--maximum_inter_block_layers_per_block\",\r\n",
      "        type=int,\r\n",
      "        help=\"Maximum number of neurons per Dense layer for Dense layers \"\r\n",
      "             \"inserted between ResidualMLP blocks? 0 Will not insert a \"\r\n",
      "             \"layer if selected.\",\r\n",
      "        default = 150)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--n_options_inter_block_layers_per_block\",\r\n",
      "        type=int,\r\n",
      "        help=\"How many different numbers of neurons per Dense layer for \"\r\n",
      "             \"Dense layers inserted between ResidualMLP blocks will we try \"\r\n",
      "             \"(at most)?\",\r\n",
      "        default = 7)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--minimum_dropout_rate\",\r\n",
      "        type=float,\r\n",
      "        help=\"Minimum dropout rate for final Dense layers?\",\r\n",
      "        default = 0.01)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--maximum_dropout_rate\",\r\n",
      "        type=float,\r\n",
      "        help=\"Maximum dropout rate for final Dense layers?\",\r\n",
      "        default = 0.7)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--n_options_dropout_rate\",\r\n",
      "        type=int,\r\n",
      "        help=\"How many dropout rates (at max) to try for final Dense layers?\",\r\n",
      "        default = 7)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--minimum_final_dense_layers\",\r\n",
      "        type=int,\r\n",
      "        help=\"Lowest number of neurons to try for final Dense layers, after \"\r\n",
      "             \"the last ResidualMLP block, before the very last Dense layer \"\r\n",
      "             \"returning an output? \"\r\n",
      "             \"(0 doesn't create a layer')\",\r\n",
      "        default = 0)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--maximum_final_dense_layers\",\r\n",
      "        type=int,\r\n",
      "        help=\"Highest number of neurons to try for final Dense layers, after \"\r\n",
      "             \"the last ResidualMLP block, before the very last Dense layer \"\r\n",
      "             \"returning an output? \"\r\n",
      "             \"(0 doesn't create a layer')\",\r\n",
      "        default = 150)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--n_options_final_dense_layers\",\r\n",
      "        type=int,\r\n",
      "        help=\"How many options for neurons to try for final Dense layers, \"\r\n",
      "             \"after the last ResidualMLP block, before the very last \"\r\n",
      "             \"Dense layer returning an output?\",\r\n",
      "        default = 7)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "args, _ = parser.parse_known_args()\r\n",
      "hparams = args.__dict__\r\n",
      "\r\n",
      "# Boilerplate args\r\n",
      "\r\n",
      "DATE = pendulum.now().__str__()[:16].replace(\"T\",\"_\").replace(\":\",\"_\")\r\n",
      "PROJECT_NAME = hparams[\"project_name\"]\r\n",
      "TRAINING_SET_SIZE = hparams[\"training_set_size\"]\r\n",
      "\r\n",
      "# Keras tuner search & fit args\r\n",
      "\r\n",
      "PATIENCE = hparams[\"patience\"]\r\n",
      "PATIENCE_MIN_DELTA = hparams[\"patience_min_delta\"]\r\n",
      "BATCH_SIZE = hparams[\"batch_size\"]\r\n",
      "MAX_EPOCHS = hparams[\"max_epochs\"]\r\n",
      "RESULTS_DIR_FOR_SEARCH =\\\r\n",
      "    f'{DATE}_{PROJECT_NAME}_SEARCH_RUN'\r\n",
      "\r\n",
      "\r\n",
      "# Base model args\r\n",
      "\r\n",
      "BASE_MODEL_INPUT_SHAPE = (600,600,3)\r\n",
      "\r\n",
      "# ResidualMLP model args\r\n",
      "\r\n",
      "MINIMUM_LEARNING_RATE = hparams[\"minimum_learning_rate\"]\r\n",
      "MAXIMUM_LEARNING_RATE = hparams[\"maximum_learning_rate\"]\r\n",
      "NUMBER_OF_LEARNING_RATES_TO_TRY = hparams[\"number_of_learning_rates_to_try\"]\r\n",
      "MINIMUM_NUMBER_OF_BLOCKS = hparams[\"minimum_number_of_blocks\"]\r\n",
      "MAXIMUM_NUMBER_OF_BLOCKS = hparams[\"maximum_number_of_blocks\"]\r\n",
      "MINIMUM_NUMBER_OF_LAYERS_PER_BLOCK =\\\r\n",
      "    hparams[\"minimum_number_of_layers_per_block\"]\r\n",
      "MAXIMUM_NUMBER_OF_LAYERS_PER_BLOCK =\\\r\n",
      "    hparams[\"maximum_number_of_layers_per_block\"]\r\n",
      "MINIMUM_NEURONS_PER_BLOCK_LAYER = hparams[\"minimum_neurons_per_block_layer\"]\r\n",
      "MAXIMUM_NEURONS_PER_BLOCK_LAYER = hparams[\"maximum_neurons_per_block_layer\"]\r\n",
      "N_OPTIONS_OF_NEURONS_PER_LAYER_TO_TRY =\\\r\n",
      "    hparams[\"n_options_of_neurons_per_layer_to_try\"]\r\n",
      "MINIMUM_NEURONS_PER_BLOCK_LAYER_DECAY =\\\r\n",
      "    hparams[\"minimum_neurons_per_block_layer_decay\"]\r\n",
      "MAXIMUM_NEURONS_PER_BLOCK_LAYER_DECAY =\\\r\n",
      "    hparams[\"maximum_neurons_per_block_layer_decay\"]\r\n",
      "MINIMUM_DROPOUT_RATE_FOR_BYPASS_LAYERS =\\\r\n",
      "    hparams[\"minimum_dropout_rate_for_bypass_layers\"]\r\n",
      "MAXIMIM_DROPOUT_RATE_FOR_BYPASS_LAYERS =\\\r\n",
      "    hparams[\"maximim_dropout_rate_for_bypass_layers\"]\r\n",
      "N_OPTIONS_DROPOUT_RATE_FOR_BYPASS_LAYERS =\\\r\n",
      "    hparams[\"n_options_dropout_rate_for_bypass_layers\"]\r\n",
      "MINIMUM_INTER_BLOCK_LAYERS_PER_BLOCK =\\\r\n",
      "    hparams[\"minimum_inter_block_layers_per_block\"]\r\n",
      "MAXIMUM_INTER_BLOCK_LAYERS_PER_BLOCK =\\\r\n",
      "    hparams[\"maximum_inter_block_layers_per_block\"]\r\n",
      "N_OPTIONS_INTER_BLOCK_LAYERS_PER_BLOCK =\\\r\n",
      "    hparams[\"n_options_inter_block_layers_per_block\"]\r\n",
      "MINIMUM_DROPOUT_RATE = hparams[\"minimum_dropout_rate\"]\r\n",
      "MAXIMUM_DROPOUT_RATE = hparams[\"maximum_dropout_rate\"]\r\n",
      "N_OPTIONS_DROPOUT_RATE = hparams[\"n_options_dropout_rate\"]\r\n",
      "MINIMUM_FINAL_DENSE_LAYERS = hparams[\"minimum_final_dense_layers\"]\r\n",
      "MAXIMUM_FINAL_DENSE_LAYERS = hparams[\"maximum_final_dense_layers\"]\r\n",
      "N_OPTIONS_FINAL_DENSE_LAYERS = hparams[\"n_options_final_dense_layers\"]\r\n",
      "\r\n",
      "# CIFAR10_EfficientNetB7-ResidualMLP_NAS\r\n",
      "\r\n",
      "RESULTS_DIR_FOR_FINAL_MODEL =\\\r\n",
      "    f'{DATE}_{PROJECT_NAME}_FINAL_MODEL'\r\n",
      "\r\n",
      "FINAL_MODEL_PATH =\\\r\n",
      "    f\"{DATE}_{PROJECT_NAME}_FINAL_EXPORTED_MODEL\"\r\n",
      "\r\n",
      "# Print header info to shell script logs:\r\n",
      "\r\n",
      "print(f\"Date: {DATE}\")\r\n",
      "print(f\"project_name: {PROJECT_NAME}\")\r\n",
      "print(f\"training_set_size: {TRAINING_SET_SIZE}\")\r\n",
      "print(f\"patience: {PATIENCE}\")\r\n",
      "print(f\"PATIENCE_MIN_DELTA: {PATIENCE_MIN_DELTA}\")\r\n",
      "print(f\"BATCH_SIZE: {BATCH_SIZE}\")\r\n",
      "print(f\"MAX_EPOCHS: {MAX_EPOCHS}\")\r\n",
      "print(f\"RESULTS_DIR_FOR_SEARCH: {RESULTS_DIR_FOR_SEARCH}\")\r\n",
      "print(f\"MINIMUM_LEARNING_RATE {MINIMUM_LEARNING_RATE}\")\r\n",
      "print(f\"MAXIMUM_LEARNING_RATE: {MAXIMUM_LEARNING_RATE}\")\r\n",
      "print(f\"NUMBER_OF_LEARNING_RATES_TO_TRY: {NUMBER_OF_LEARNING_RATES_TO_TRY}\")\r\n",
      "print(f\"MINIMUM_NUMBER_OF_BLOCKS: {MINIMUM_NUMBER_OF_BLOCKS}\")\r\n",
      "print(f\"MAXIMUM_NUMBER_OF_BLOCKS: {MAXIMUM_NUMBER_OF_BLOCKS}\")\r\n",
      "print(\"MINIMUM_NUMBER_OF_LAYERS_PER_BLOCK: \"\r\n",
      "      f\"{MINIMUM_NUMBER_OF_LAYERS_PER_BLOCK}\")\r\n",
      "print(\"MAXIMUM_NUMBER_OF_LAYERS_PER_BLOCK: \"\r\n",
      "      f\"{MAXIMUM_NUMBER_OF_LAYERS_PER_BLOCK}\")\r\n",
      "print(f\"MINIMUM_NEURONS_PER_BLOCK_LAYER: {MINIMUM_NEURONS_PER_BLOCK_LAYER}\")\r\n",
      "print(f\"MAXIMUM_NEURONS_PER_BLOCK_LAYER: {MAXIMUM_NEURONS_PER_BLOCK_LAYER}\")\r\n",
      "print(\"N_OPTIONS_OF_NEURONS_PER_LAYER_TO_TRY: \"\r\n",
      "      f\"{N_OPTIONS_OF_NEURONS_PER_LAYER_TO_TRY}\")\r\n",
      "print(\"MINIMUM_NEURONS_PER_BLOCK_LAYER_DECAY: \"\r\n",
      "      f\"{MINIMUM_NEURONS_PER_BLOCK_LAYER_DECAY}\")\r\n",
      "print(\"MAXIMUM_NEURONS_PER_BLOCK_LAYER_DECAY: \"\r\n",
      "      f\"{MAXIMUM_NEURONS_PER_BLOCK_LAYER_DECAY}\")\r\n",
      "print(\"MINIMUM_DROPOUT_RATE_FOR_BYPASS_LAYERS: \"\r\n",
      "      f\"{MINIMUM_DROPOUT_RATE_FOR_BYPASS_LAYERS}\")\r\n",
      "print(\"MAXIMIM_DROPOUT_RATE_FOR_BYPASS_LAYERS: \"\r\n",
      "      f\"{MAXIMIM_DROPOUT_RATE_FOR_BYPASS_LAYERS}\")\r\n",
      "print(\"N_OPTIONS_DROPOUT_RATE_FOR_BYPASS_LAYERS: \"\r\n",
      "      f\"{N_OPTIONS_DROPOUT_RATE_FOR_BYPASS_LAYERS}\")\r\n",
      "print(\"MINIMUM_INTER_BLOCK_LAYERS_PER_BLOCK: \"\r\n",
      "      f\"{MINIMUM_INTER_BLOCK_LAYERS_PER_BLOCK}\")\r\n",
      "print(\"MAXIMUM_INTER_BLOCK_LAYERS_PER_BLOCK: \"\r\n",
      "      f\"{MAXIMUM_INTER_BLOCK_LAYERS_PER_BLOCK}\")\r\n",
      "print(\"N_OPTIONS_INTER_BLOCK_LAYERS_PER_BLOCK: \"\r\n",
      "      f\"{N_OPTIONS_INTER_BLOCK_LAYERS_PER_BLOCK}\")\r\n",
      "print(f\"MINIMUM_DROPOUT_RATE: {MINIMUM_DROPOUT_RATE}\")\r\n",
      "print(f\"MAXIMUM_DROPOUT_RATE: {MAXIMUM_DROPOUT_RATE}\")\r\n",
      "print(f\"N_OPTIONS_DROPOUT_RATE: {N_OPTIONS_DROPOUT_RATE}\")\r\n",
      "print(f\"MINIMUM_FINAL_DENSE_LAYERS: {MINIMUM_FINAL_DENSE_LAYERS}\")\r\n",
      "print(f\"MAXIMUM_FINAL_DENSE_LAYERS: {MAXIMUM_FINAL_DENSE_LAYERS}\")\r\n",
      "print(f\"N_OPTIONS_FINAL_DENSE_LAYERS: {N_OPTIONS_FINAL_DENSE_LAYERS}\")\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "    cifar = tf.keras.datasets.cifar10.load_data()\r\n",
      "    (x_train, y_train), (x_test, y_test) = cifar\r\n",
      "    \r\n",
      "    y_train_ohe = tf.one_hot([i[0] for i in  y_train],10)\r\n",
      "    indexes_for_rows = tf.range(0,y_train.shape[0])\r\n",
      "    shuffled_indexes = tf.random.shuffle(indexes_for_rows)\r\n",
      "    selected_indexes = shuffled_indexes[:TRAINING_SET_SIZE]\r\n",
      "    selected_x_train = x_train[selected_indexes,:,:,:]\r\n",
      "    selected_y_train_ohe = y_train_ohe.numpy()[selected_indexes,:]\r\n",
      "    \r\n",
      "    \r\n",
      "    \r\n",
      "    \r\n",
      "    mod_with_fc_raw = tf.keras.applications.efficientnet.EfficientNetB7(\r\n",
      "        include_top=True, weights='imagenet', input_tensor=None,\r\n",
      "        input_shape = BASE_MODEL_INPUT_SHAPE, pooling='max', classes=1000\r\n",
      "    )\r\n",
      "    \r\n",
      "    # Make the deepest conv2d layer trainable, leave everything else\r\n",
      "    # as not trainable\r\n",
      "    for layer in mod_with_fc_raw.layers:\r\n",
      "        layer.trainable = False\r\n",
      "    # Last conv2d layer. This we want to train .\r\n",
      "    mod_with_fc_raw.layers[-6].trainable = True\r\n",
      "    \r\n",
      "    # Create the final base model\r\n",
      "    # (remove the final Dense and BatchNormalization layers ...) \r\n",
      "    efficient_net_b_7_transferable_base_model =\\\r\n",
      "        tf.keras.Model(inputs=mod_with_fc_raw.layers[0].input, \r\n",
      "                        outputs=mod_with_fc_raw.layers[-3].output)\r\n",
      "    \r\n",
      "    model_builder = ResidualMLP(\r\n",
      "                        problem_type= \"classification\",  \r\n",
      "                        minimum_learning_rate = MINIMUM_LEARNING_RATE, \r\n",
      "                        maximum_learning_rate = MAXIMUM_LEARNING_RATE, \r\n",
      "                        number_of_learning_rates_to_try =\r\n",
      "                            NUMBER_OF_LEARNING_RATES_TO_TRY, \r\n",
      "                        input_shape = (32,32,3), \r\n",
      "                        bw_images = False, \r\n",
      "                        base_model = \r\n",
      "                            efficient_net_b_7_transferable_base_model, \r\n",
      "                        base_model_input_shape = BASE_MODEL_INPUT_SHAPE, \r\n",
      "                        flatten_after_base_model = False, \r\n",
      "                        minimum_number_of_blocks = MINIMUM_NUMBER_OF_BLOCKS, \r\n",
      "                        maximum_number_of_blocks = MAXIMUM_NUMBER_OF_BLOCKS, \r\n",
      "                        minimum_number_of_layers_per_block =\r\n",
      "                            MINIMUM_NUMBER_OF_LAYERS_PER_BLOCK, \r\n",
      "                        maximum_number_of_layers_per_block =\r\n",
      "                            MAXIMUM_NUMBER_OF_LAYERS_PER_BLOCK,\r\n",
      "                        minimum_neurons_per_block_layer =\r\n",
      "                            MINIMUM_NEURONS_PER_BLOCK_LAYER, \r\n",
      "                        maximum_neurons_per_block_layer =\r\n",
      "                            MAXIMUM_NEURONS_PER_BLOCK_LAYER, \r\n",
      "                        n_options_of_neurons_per_layer_to_try =\r\n",
      "                            N_OPTIONS_OF_NEURONS_PER_LAYER_TO_TRY, \r\n",
      "                        minimum_neurons_per_block_layer_decay =\r\n",
      "                            MINIMUM_NEURONS_PER_BLOCK_LAYER_DECAY, \r\n",
      "                        maximum_neurons_per_block_layer_decay = \r\n",
      "                            MAXIMUM_NEURONS_PER_BLOCK_LAYER_DECAY, \r\n",
      "                        minimum_dropout_rate_for_bypass_layers =\r\n",
      "                            MINIMUM_DROPOUT_RATE_FOR_BYPASS_LAYERS, \r\n",
      "                        maximim_dropout_rate_for_bypass_layers =\r\n",
      "                            MAXIMIM_DROPOUT_RATE_FOR_BYPASS_LAYERS, \r\n",
      "                        n_options_dropout_rate_for_bypass_layers =\r\n",
      "                            N_OPTIONS_DROPOUT_RATE_FOR_BYPASS_LAYERS,\r\n",
      "                        minimum_inter_block_layers_per_block =\r\n",
      "                            MINIMUM_INTER_BLOCK_LAYERS_PER_BLOCK, \r\n",
      "                        maximum_inter_block_layers_per_block =\r\n",
      "                            MAXIMUM_INTER_BLOCK_LAYERS_PER_BLOCK,\r\n",
      "                        n_options_inter_block_layers_per_block =\\\r\n",
      "                            N_OPTIONS_INTER_BLOCK_LAYERS_PER_BLOCK,\r\n",
      "                        minimum_dropout_rate = MINIMUM_DROPOUT_RATE, \r\n",
      "                        maximum_dropout_rate = MAXIMUM_DROPOUT_RATE,\r\n",
      "                        n_options_dropout_rate = N_OPTIONS_DROPOUT_RATE, \r\n",
      "                        minimum_final_dense_layers =\r\n",
      "                            MINIMUM_FINAL_DENSE_LAYERS,\r\n",
      "                        maximum_final_dense_layers =\r\n",
      "                            MAXIMUM_FINAL_DENSE_LAYERS, \r\n",
      "                        n_options_final_dense_layers =\r\n",
      "                            N_OPTIONS_FINAL_DENSE_LAYERS, \r\n",
      "                        number_of_classes = 10,\r\n",
      "                        final_activation = tf.keras.activations.softmax)\r\n",
      "    \r\n",
      "    \r\n",
      "    \r\n",
      "    logdir_for_search = os.path.join(\"logs\", RESULTS_DIR_FOR_SEARCH + \"_TB\")\r\n",
      "    tensorboard_callback_search =\\\r\n",
      "        tf.keras.callbacks.TensorBoard(logdir_for_search, histogram_freq=1)\r\n",
      "    \r\n",
      "    tuner = kt.Hyperband(\r\n",
      "        model_builder.build_auto_residual_mlp,\r\n",
      "        objective='val_loss',\r\n",
      "        max_epochs = MAX_EPOCHS,\r\n",
      "        hyperband_iterations = 2)\r\n",
      "    \r\n",
      "    \r\n",
      "    tuner.search(x=selected_x_train,  \r\n",
      "                 y=selected_y_train_ohe,\r\n",
      "                 epochs=MAX_EPOCHS,\r\n",
      "                 batch_size=BATCH_SIZE, \r\n",
      "                 callbacks=[\r\n",
      "                        tf.keras.callbacks.EarlyStopping(\r\n",
      "                            monitor=\"val_loss\",\r\n",
      "                            patience=PATIENCE,\r\n",
      "                            min_delta=PATIENCE_MIN_DELTA,\r\n",
      "                            restore_best_weights=True,\r\n",
      "                        ),\r\n",
      "                        tensorboard_callback_search,\r\n",
      "                    ],\r\n",
      "                 validation_split=0.3)\r\n",
      "    \r\n",
      "    print(\"These are the best params and results:\")\r\n",
      "    tuner.results_summary(num_trials=10)\r\n",
      "    \r\n",
      "    # final_model = tuner.get_best_models(num_models=1)[0]\r\n",
      "    \r\n",
      "    best_hp = tuner.get_best_hyperparameters()[0]\r\n",
      "    final_model = tuner.hypermodel.build(best_hp)\r\n",
      "\r\n",
      "    \r\n",
      "    \r\n",
      "    logdir_final_model = os.path.join(\"logs\",\r\n",
      "                                      RESULTS_DIR_FOR_FINAL_MODEL + \"_TB\")\r\n",
      "    tensorboard_callback_final =\\\r\n",
      "        tf.keras.callbacks.TensorBoard(logdir_final_model, histogram_freq=1)\r\n",
      "    \r\n",
      "    history = final_model.fit(x=selected_x_train,  \r\n",
      "                        y=selected_y_train_ohe, \r\n",
      "                        batch_size=BATCH_SIZE, \r\n",
      "                        epochs=150,      \r\n",
      "                        verbose='auto', \r\n",
      "                        callbacks=[tf.keras.callbacks.\\\r\n",
      "                                   EarlyStopping(monitor='val_loss',\r\n",
      "                                                 patience=PATIENCE,\r\n",
      "                                                 min_delta=PATIENCE_MIN_DELTA,\r\n",
      "                                                 restore_best_weights=True),\r\n",
      "                                tensorboard_callback_final], \r\n",
      "                        validation_split=0.3,\r\n",
      "                        validation_data=None,\r\n",
      "                        shuffle=True,\r\n",
      "                        class_weight=None, \r\n",
      "                        sample_weight=None, \r\n",
      "                        initial_epoch=0, \r\n",
      "                        steps_per_epoch=None, \r\n",
      "                        validation_steps=None, \r\n",
      "                        validation_batch_size=10, \r\n",
      "                        validation_freq=1, \r\n",
      "                        max_queue_size=10, \r\n",
      "                        workers=5, \r\n",
      "                        use_multiprocessing=True)\r\n",
      "    \r\n",
      "    \r\n",
      "    \r\n",
      "    hy = pd.DataFrame(history.history)\r\n",
      "    hy.to_csv(f'{DATE}_test_history.csv')\r\n",
      "    hy.to_json(f'{DATE}_test_history.json')\r\n",
      "    \r\n",
      "    final_model.save(FINAL_MODEL_PATH)\r\n",
      "    print(\"Successful Run!\")\r\n"
     ]
    }
   ],
   "source": [
    "!cat task.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aead295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try:\r\n",
      "    import keras_tuner as kt\r\n",
      "except Exception as exc:\r\n",
      "    print(\"Importing Keras tuner appears to be unsuccesful. \"\r\n",
      "          \"keras tuner may need to be installed $ pip install -q -U \"\r\n",
      "          \"keras-tuner. The auto-ml features are disabled until this is \"\r\n",
      "           \"fixed, but ResidualMLP will work. A more detailed error is: \"\r\n",
      "           f\"{exc}\")\r\n",
      "import tensorflow as tf\r\n",
      "import numpy as np\r\n",
      "import pandas as pd\r\n",
      "import pendulum\r\n",
      "\r\n",
      "# Becomes a layer to convert BW images to RGB\r\n",
      "\r\n",
      "class ResidualMLP:\r\n",
      "    \r\n",
      "    def __init__(self, problem_type = 'classification',\r\n",
      "                      learning_rate = .0007,\r\n",
      "                      minimum_learning_rate = 0.00007,\r\n",
      "                      maximum_learning_rate = 0.7,\r\n",
      "                      number_of_learning_rates_to_try = 5,\r\n",
      "                      input_shape = (32,32,3),\r\n",
      "                      bw_images = False,\r\n",
      "                      base_model = '',\r\n",
      "                      base_model_input_shape = (600,600,3),\r\n",
      "                      base_model_hyperparameters = {},\r\n",
      "                      flatten_after_base_model = True,\r\n",
      "                          # 2D ARRAY: Each row: \r\n",
      "                          # [number_of_layers,\r\n",
      "                          #  first_layer_neurons, \r\n",
      "                          #  decay_of_n_Dense_units_per_layer]\r\n",
      "                      blocks = [[5,400,50]],\r\n",
      "                      minimum_number_of_blocks = 1,\r\n",
      "                      maximum_number_of_blocks = 7,\r\n",
      "                      minimum_number_of_layers_per_block = 1,\r\n",
      "                      maximum_number_of_layers_per_block = 7,\r\n",
      "                      minimum_neurons_per_block_layer = 3,\r\n",
      "                      maximum_neurons_per_block_layer = 30,\r\n",
      "                      n_options_of_neurons_per_layer_to_try = 7,\r\n",
      "                      minimum_neurons_per_block_layer_decay = 1,\r\n",
      "                      maximum_neurons_per_block_layer_decay = 7,\r\n",
      "                      residual_bypass_dense_layers = list(),\r\n",
      "                      b_norm_or_dropout_residual_bypass_layers = 'dropout',\r\n",
      "                      dropout_rate_for_bypass_layers = .35,\r\n",
      "                      minimum_dropout_rate_for_bypass_layers = 0.01,\r\n",
      "                      maximim_dropout_rate_for_bypass_layers = 0.7,\r\n",
      "                      n_options_dropout_rate_for_bypass_layers = 7,\r\n",
      "                      inter_block_layers_per_block = list(),\r\n",
      "                      minimum_inter_block_layers_per_block = 3,\r\n",
      "                      maximum_inter_block_layers_per_block = 30,\r\n",
      "                      n_options_inter_block_layers_per_block = 7,\r\n",
      "                      b_norm_or_dropout_last_layers = 'dropout', # | 'bnorm'\r\n",
      "                      dropout_rate = .2, #\r\n",
      "                      minimum_dropout_rate = 0.01,\r\n",
      "                      maximum_dropout_rate = 0.7,\r\n",
      "                      n_options_dropout_rate = 7,\r\n",
      "                      activation = tf.keras.activations.relu,\r\n",
      "                      final_dense_layers = [75,35],\r\n",
      "                      minimum_final_dense_layers = 0,\r\n",
      "                      maximum_final_dense_layers = 30,\r\n",
      "                      n_options_final_dense_layers = 2,\r\n",
      "                      number_of_classes = 10, # 1 if a regression problem\r\n",
      "                      final_activation = tf.keras.activations.softmax,\r\n",
      "                      loss = tf.keras.losses.CategoricalCrossentropy(\r\n",
      "                          from_logits=False)):\r\n",
      "        self.problem_type = problem_type\r\n",
      "        self.learning_rate = learning_rate\r\n",
      "        self.minimum_learning_rate = minimum_learning_rate\r\n",
      "        self.maximum_learning_rate = maximum_learning_rate\r\n",
      "        self.number_of_learning_rates_to_try = number_of_learning_rates_to_try\r\n",
      "        self.input_shape = input_shape\r\n",
      "        self.bw_images = bw_images\r\n",
      "        self.base_model = base_model\r\n",
      "        self.base_model_input_shape = base_model_input_shape\r\n",
      "        self.flatten_after_base_model = flatten_after_base_model\r\n",
      "        self.blocks = blocks\r\n",
      "        self.minimum_number_of_blocks = minimum_number_of_blocks\r\n",
      "        self.maximum_number_of_blocks = maximum_number_of_blocks\r\n",
      "        self.minimum_number_of_layers_per_block =\\\r\n",
      "            minimum_number_of_layers_per_block\r\n",
      "        self.maximum_number_of_layers_per_block =\\\r\n",
      "            maximum_number_of_layers_per_block\r\n",
      "        self.minimum_neurons_per_block_layer = minimum_neurons_per_block_layer\r\n",
      "        self.maximum_neurons_per_block_layer = maximum_neurons_per_block_layer\r\n",
      "        self.n_options_of_neurons_per_layer_to_try =\\\r\n",
      "            n_options_of_neurons_per_layer_to_try\r\n",
      "        self.minimum_neurons_per_block_layer_decay =\\\r\n",
      "            minimum_neurons_per_block_layer_decay\r\n",
      "        self.maximum_neurons_per_block_layer_decay =\\\r\n",
      "            maximum_neurons_per_block_layer_decay\r\n",
      "        # residual_bypass_dense_layers\r\n",
      "        # Screen for nonsense combinations of the parameters 'blocks' and \r\n",
      "        # 'residual_bypass_dense_layers'\r\n",
      "        if not isinstance(residual_bypass_dense_layers,list):\r\n",
      "            raise ValueError(\"The parameter residual_bypass_dense_layers \"\r\n",
      "                             \"should be one of these 2: 1. a 2d list, one 1d \"\r\n",
      "                             \"list of positive    integers for each list in \"\r\n",
      "                             \"blocks, or 2. an empty list.\")\r\n",
      "        if len(residual_bypass_dense_layers) != 0:\r\n",
      "            if len(blocks) != len(residual_bypass_dense_layers):\r\n",
      "                raise ValueError(\"The parameter 'blocks' and \"\r\n",
      "                                 \"'residual_bypass_dense_layers are 2d \"\r\n",
      "                                 \"arrays' must have the same number of 1d \"\r\n",
      "                                 \"arrays nested within them OR be aan empty \"\r\n",
      "                                 \"list or left default. To fix this error \"\r\n",
      "                                 \"do one of the following: Fix 1: Make them \" \r\n",
      "                                 \"the same number of 1d arrays, for example \"\r\n",
      "                                 \"blocks = [[5,20,2],[5,20,2],[5,20,2]] ...\"\r\n",
      "                                 \" (3 nested 1d arrays) \"\r\n",
      "                                 \"residual_bypass_dense_layers = \"\r\n",
      "                                 \"[[10,7],[],[10]] also 3. \"\r\n",
      "                                 \"the ith array nested in \"\r\n",
      "                                 \"residual_bypass_dense_layers will be \"\r\n",
      "                                 \"associated with the i_th block in blocks. \"\r\n",
      "                                 \" each j_th item in the i_th nested 1d \"\r\n",
      "                                 \"array will make one dense layer in the \"\r\n",
      "                                 \"tensor that bypasses the main block of \"\r\n",
      "                                 \"dense layers in the ith block in the \"\r\n",
      "                                 \"residual MLP. This is probably a bit \"\r\n",
      "                                 \"confusing to read. Please refer to the \"\r\n",
      "                                 \"tutorials and documentation. Note \"\r\n",
      "                                 \"the order I referred to the tutorials and \"\r\n",
      "                                 \"documentation in. Fix 2: leave \"\r\n",
      "                                 \"residual_bypass_dense_layers default / \"\r\n",
      "                                 \"set it to an empty list.\")\r\n",
      "            else:\r\n",
      "                self.residual_bypass_dense_layers =\\\r\n",
      "                    residual_bypass_dense_layers\r\n",
      "        else:\r\n",
      "            self.residual_bypass_dense_layers = [[] for block in blocks]\r\n",
      "        self.b_norm_or_dropout_residual_bypass_layers =\\\r\n",
      "            b_norm_or_dropout_residual_bypass_layers\r\n",
      "        self.dropout_rate_for_bypass_layers = dropout_rate_for_bypass_layers\r\n",
      "        self.minimum_dropout_rate_for_bypass_layers =\\\r\n",
      "            minimum_dropout_rate_for_bypass_layers\r\n",
      "        self.maximim_dropout_rate_for_bypass_layers =\\\r\n",
      "            maximim_dropout_rate_for_bypass_layers\r\n",
      "        self.n_options_dropout_rate_for_bypass_layers =\\\r\n",
      "            n_options_dropout_rate_for_bypass_layers\r\n",
      "        self.inter_block_layers_per_block = inter_block_layers_per_block\r\n",
      "        self.minimum_inter_block_layers_per_block =\\\r\n",
      "            minimum_inter_block_layers_per_block\r\n",
      "        self.maximum_inter_block_layers_per_block =\\\r\n",
      "            maximum_inter_block_layers_per_block\r\n",
      "        self.n_options_inter_block_layers_per_block =\\\r\n",
      "            n_options_inter_block_layers_per_block\r\n",
      "        self.b_norm_or_dropout_last_layers = b_norm_or_dropout_last_layers\r\n",
      "        self.dropout_rate  = dropout_rate\r\n",
      "        self.minimum_dropout_rate = minimum_dropout_rate\r\n",
      "        self.maximum_dropout_rate = maximum_dropout_rate\r\n",
      "        self.n_options_dropout_rate = n_options_dropout_rate\r\n",
      "        self.activation = activation\r\n",
      "        self.final_dense_layers = final_dense_layers\r\n",
      "        self.minimum_final_dense_layers = minimum_final_dense_layers\r\n",
      "        self.maximum_final_dense_layers = maximum_final_dense_layers\r\n",
      "        self.n_options_final_dense_layers = n_options_final_dense_layers\r\n",
      "        self.number_of_classes = number_of_classes\r\n",
      "        self.final_activation = final_activation\r\n",
      "        self.loss = loss\r\n",
      "        \r\n",
      "    \r\n",
      "    def grayscale_to_rgb(images, channel_axis=-1):\r\n",
      "        images= tf.expand_dims(images, axis=channel_axis)\r\n",
      "        tiling = [1] * 4    # 4 dimensions: B, H, W, C\r\n",
      "        tiling[channel_axis] *= 3\r\n",
      "        images= tf.tile(images, tiling)\r\n",
      "        im = tf.keras.preprocessing.image.smart_resize(images,(224,224))\r\n",
      "        return im\r\n",
      "\r\n",
      "\r\n",
      "    # builds and compiles a tandem model given these params and\r\n",
      "    # selected base model:\r\n",
      "    def make_tandem_model(self):\r\n",
      "        if self.problem_type == 'classification':\r\n",
      "            precision = tf.keras.metrics.Precision(),\r\n",
      "            recall = tf.keras.metrics.Recall()\r\n",
      "            accuracy = tf.keras.metrics.Accuracy()\r\n",
      "        if self.problem_type == 'classification' and\\\r\n",
      "                self.number_of_classes > 1:\r\n",
      "            metrics = [tf.keras.metrics.TopKCategoricalAccuracy(\r\n",
      "                k = k,\r\n",
      "                name=f'top_{k}_'\r\n",
      "                'categorical_'\r\n",
      "                'accuracy',\r\n",
      "                dtype=None)\r\n",
      "                           for k in np.arange(1,self.number_of_classes)\\\r\n",
      "                               if k < 10]\r\n",
      "            metrics.append(precision)\r\n",
      "            metrics.append(recall)\r\n",
      "            metrics.append(accuracy)\r\n",
      "        elif self.problem_type == 'classification' and\\\r\n",
      "                self.number_of_classes == 1:    \r\n",
      "            metrics = [precision, recall, accuracy]\r\n",
      "        else:\r\n",
      "            rmse = tf.keras.metrics.RootMeanSquaredError()\r\n",
      "            mae = tf.keras.metrics.MeanAbsoluteError()\r\n",
      "            metrics = [rmse, mae]\r\n",
      "    \r\n",
      "        inp = tf.keras.layers.Input(shape = self.input_shape) \r\n",
      "        # Start with input layer that fits. \r\n",
      "        # The keras fucntional API requires an explicit input layer\r\n",
      "        if self.bw_images:\r\n",
      "            x = self.grayscale_to_rgb(inp)\r\n",
      "        else:\r\n",
      "            x = inp\r\n",
      "        if self.base_model != '':\r\n",
      "            x = tf.keras.layers.Resizing(self.base_model_input_shape[0],\r\n",
      "                                         self.base_model_input_shape[1])(x)\r\n",
      "            x = self.base_model(x)\r\n",
      "        if self.flatten_after_base_model:\r\n",
      "            x = tf.keras.layers.Flatten()(x)\r\n",
      "        initializer = tf.keras.initializers.GlorotNormal()\r\n",
      "        for bl in np.arange(len(self.blocks)):\r\n",
      "            block = self.blocks[bl]\r\n",
      "            bypass_block = self.residual_bypass_dense_layers[bl]\r\n",
      "            \r\n",
      "            \r\n",
      "            x = tf.keras.layers.Dense(block[1],\r\n",
      "                                      self.activation,\r\n",
      "                                      kernel_initializer=initializer)(x)\r\n",
      "            y = x\r\n",
      "            x = tf.keras.layers.BatchNormalization()(x)\r\n",
      "            # x proceeds sequentially to the \r\n",
      "            # next Dense layer.\r\n",
      "            \r\n",
      "            if self.b_norm_or_dropout_residual_bypass_layers == 'dropout':\r\n",
      "                y = tf.keras.layers\\\r\n",
      "                    .Dropout(self.dropout_rate_for_bypass_layers)(y)\r\n",
      "            elif self.b_norm_or_dropout_residual_bypass_layers == 'bnorm':\r\n",
      "                y = tf.keras.layers.BatchNormalization()(y)\r\n",
      "            else:\r\n",
      "                raise ValueError(\"The parameter: \"\r\n",
      "                                 \"'b_norm_or_dropout_residual_bypass_\"\r\n",
      "                                 \"layers'\"\r\n",
      "                                 \" must be left default '', or be \"\r\n",
      "                                 \"'dropout' or may be 'bnorm'.\")\r\n",
      "            for bypass_layer in bypass_block:\r\n",
      "                y = tf.keras.layers.Dense(bypass_layer,\r\n",
      "                                          self.activation,\r\n",
      "                                          kernel_initializer=initializer)(y)\r\n",
      "                if self.b_norm_or_dropout_residual_bypass_layers == 'dropout':\r\n",
      "                    y = tf.keras.layers\\\r\n",
      "                        .Dropout(self.dropout_rate_for_bypass_layers)(y)\r\n",
      "                elif self.b_norm_or_dropout_residual_bypass_layers == 'bnorm':\r\n",
      "                    y = tf.keras.layers.BatchNormalization()(y)\r\n",
      "                else:\r\n",
      "                    raise ValueError(\"The parameter: \"\r\n",
      "                                     \"'b_norm_or_dropout_residual_bypass_\"\r\n",
      "                                     \"layers' must be left default '', or be \"\r\n",
      "                                     \"'dropout' or may be 'bnorm'.\")\r\n",
      "            # y does NOT proceed sequentially\r\n",
      "            # to the next layer. This bypasses \r\n",
      "            # several layers and give a memory \r\n",
      "            # that attenuates some of the \r\n",
      "            # deleterious effects of a deeper \r\n",
      "            # network and lets us capture more \r\n",
      "            # complex interactions before \r\n",
      "            # overfitting becomes an issue than \r\n",
      "            # the textbook sequential multi - \r\n",
      "            # layer perceptron ...\r\n",
      "            for j in np.arange(block[0]): \r\n",
      "                x = tf.keras.layers.Dense(block[1] - block[2] * j,\r\n",
      "                                          self.activation,\r\n",
      "                                          kernel_initializer=initializer)(x) \r\n",
      "                x = tf.keras.layers.BatchNormalization()(x)\r\n",
      "    \r\n",
      "            x = tf.keras.layers.Concatenate(axis=1)([x, y])\r\n",
      "            \r\n",
      "            if bl != np.arange(len(self.blocks)).max():\r\n",
      "                for inter_block_layer in self.inter_block_layers_per_block:\r\n",
      "                    x = tf.keras.layers.Dense(inter_block_layer,\r\n",
      "                                          self.activation,\r\n",
      "                                          kernel_initializer=initializer)(x)\r\n",
      "                    x = tf.keras.layers.BatchNormalization()(x)\r\n",
      "    \r\n",
      "        for i in self.final_dense_layers:\r\n",
      "            x = tf.keras.layers.Dense(i,\r\n",
      "                                      self.activation,\r\n",
      "                                      kernel_initializer=initializer)(x) \r\n",
      "            if self.b_norm_or_dropout_last_layers == 'bnorm':\r\n",
      "                x = tf.keras.layers.BatchNormalization()(x)\r\n",
      "            elif self.b_norm_or_dropout_last_layers == 'dropout':\r\n",
      "                x = tf.keras.layers.Dropout(self.dropout_rate)(x)\r\n",
      "            else:\r\n",
      "                raise ValueError(\"For b_norm_or_dropout_last_layers, \" \r\n",
      "                                 \"you must pick either 'dropout' or 'bnorm'\")\r\n",
      "        out = tf.keras.layers.Dense(self.number_of_classes,\r\n",
      "                                    self.final_activation,\r\n",
      "                                    kernel_initializer=initializer)(x)\r\n",
      "    \r\n",
      "        # Declare the graph for our model ...\r\n",
      "        modelo_final = tf.keras.Model(inputs=inp,outputs = out)\r\n",
      "        \r\n",
      "        modelo_final\\\r\n",
      "            .compile(optimizer=\\\r\n",
      "                     tf.keras.optimizers.Adam(\r\n",
      "                         learning_rate=self.learning_rate, \r\n",
      "                         clipnorm=1.0),\r\n",
      "                         loss=self.loss, \r\n",
      "                         metrics=metrics)\r\n",
      "        return modelo_final\r\n",
      "    \r\n",
      "    \r\n",
      "    def parse_block(self,number_of_blocks,\r\n",
      "                        layers_per_block,\r\n",
      "                        neurons_per_block_layer,\r\n",
      "                        neurons_per_block_layer_decay):\r\n",
      "        blocks_0 = []\r\n",
      "        for i in np.arange(number_of_blocks):\r\n",
      "            block_0 = [layers_per_block,\r\n",
      "                       neurons_per_block_layer,\r\n",
      "                       neurons_per_block_layer_decay]\r\n",
      "            blocks_0.append(block_0)\r\n",
      "        return blocks_0\r\n",
      "    \r\n",
      "    def build_auto_residual_mlp(self,hp):\r\n",
      "        \r\n",
      "        self.learning_rate = hp.Float(name='learning_rate',\r\n",
      "                                      min_value=self.minimum_learning_rate, \r\n",
      "                                      max_value=self.maximum_learning_rate,\r\n",
      "                                      sampling='log')\r\n",
      "\r\n",
      "        permutations_of_blocks_array = np.array([[i,j,k,l]\r\n",
      "         for i in np.arange(self.minimum_number_of_blocks,\r\n",
      "                            self.maximum_number_of_blocks + 1)\r\n",
      "         for j in np.arange(self.minimum_number_of_layers_per_block,\r\n",
      "                            self.maximum_number_of_layers_per_block + 1)\r\n",
      "         for k in np.linspace(self.minimum_neurons_per_block_layer,\r\n",
      "                              self.maximum_neurons_per_block_layer, \r\n",
      "                              self.n_options_of_neurons_per_layer_to_try,\r\n",
      "                              dtype=int)\r\n",
      "         for l in np.arange(self.minimum_neurons_per_block_layer_decay,\r\n",
      "                            self.maximum_neurons_per_block_layer_decay + 1)])\r\n",
      "        \r\n",
      "        permutations_of_blocks_df = pd.DataFrame(permutations_of_blocks_array)\r\n",
      "        permutations_of_blocks_df.columns = ['number_of_blocks',\r\n",
      "                                             'layers_per_block',\r\n",
      "                                             'neurons_per_block_layer',\r\n",
      "                                             'neurons_per_block_layer_decay']\r\n",
      "        print(\"All permutations:\")\r\n",
      "        print(permutations_of_blocks_df)\r\n",
      "\r\n",
      "        # Filter out any invalid permutations that would try creating Dense\r\n",
      "        # layers with 0 units or a negative number of units.\r\n",
      "        permutations_of_blocks_df['valid_block'] =\\\r\n",
      "            permutations_of_blocks_df['neurons_per_block_layer'] >\\\r\n",
      "            permutations_of_blocks_df[\"layers_per_block\"] *\\\r\n",
      "            permutations_of_blocks_df[\"neurons_per_block_layer_decay\"]\r\n",
      "        valid_permutations_df =\\\r\n",
      "            permutations_of_blocks_df.query(\"valid_block == True\")\\\r\n",
      "            .reset_index(drop=True)\r\n",
      "        print(\"Valid permutations\")\r\n",
      "        print(valid_permutations_df)\r\n",
      "        \r\n",
      "        list_of_blocks_args = list()\r\n",
      "        for i in np.arange(valid_permutations_df.shape[0]):\r\n",
      "            blocks_arg = self.parse_block(\r\n",
      "                valid_permutations_df.loc[i]['number_of_blocks'],\r\n",
      "                valid_permutations_df.loc[i]['layers_per_block'],\r\n",
      "                valid_permutations_df.loc[i]['neurons_per_block_layer'],\r\n",
      "                valid_permutations_df.loc[i]['neurons_per_block_layer_decay'])\r\n",
      "            list_of_blocks_args.append(blocks_arg)\r\n",
      "        \r\n",
      "        valid_permutations_df['blocks'] = list_of_blocks_args\r\n",
      "        \r\n",
      "        print(\"Valid permutations with blocks column\")\r\n",
      "        print(valid_permutations_df)\r\n",
      " \r\n",
      "        valid_permutations_df.sort_values(['layers_per_block',\r\n",
      "                                           'neurons_per_block_layer'],\r\n",
      "                                            ascending=True)\\\r\n",
      "            .reset_index(drop=True)\r\n",
      "        \r\n",
      "        # for reeference, the list of block options is saved as a csv\r\n",
      "        date = pendulum.now().__str__()[:16].replace(\"T\",\"_\").replace(\":\",\"_\")\r\n",
      "        valid_permutations_df.to_csv(f'{date}_blocks_permutations.csv')\r\n",
      "        \r\n",
      "        list_to_choose_blocks_option_from =\\\r\n",
      "            [int(i) for i in np.arange(valid_permutations_df.shape[0])]\r\n",
      "        blocks_index_chosen = hp.Choice(\r\n",
      "                    name='blocks',\r\n",
      "                    values=list_to_choose_blocks_option_from,\r\n",
      "                    ordered=True)\r\n",
      "        \r\n",
      "        \r\n",
      "        self.blocks = valid_permutations_df.loc[blocks_index_chosen]['blocks']\r\n",
      "        print(self.blocks)\r\n",
      "        \r\n",
      "        bypass_layers_units = hp.Choice(name='bypass_layers_units',\r\n",
      "                        values=[int(i) \r\n",
      "                                for i in np.linspace(\r\n",
      "                                self.minimum_neurons_per_block_layer ,\r\n",
      "                                self.maximum_neurons_per_block_layer,\r\n",
      "                                self.n_options_of_neurons_per_layer_to_try,\r\n",
      "                                dtype=int)],\r\n",
      "                        ordered=True)\r\n",
      "        \r\n",
      "        if bypass_layers_units == 0:\r\n",
      "            self.residual_bypass_dense_layers =\\\r\n",
      "                [list() for _ in np.arange(len(self.blocks))]\r\n",
      "        else:\r\n",
      "            self.residual_bypass_dense_layers =\\\r\n",
      "                [[bypass_layers_units] for _ in np.arange(len(self.blocks))]\r\n",
      "        \r\n",
      "        inter_block_layers_per_block_options =\\\r\n",
      "            [int(i) for i in\r\n",
      "                np.linspace(self.minimum_inter_block_layers_per_block,\r\n",
      "                            self.maximum_inter_block_layers_per_block,\r\n",
      "                            self.n_options_inter_block_layers_per_block,\r\n",
      "                            dtype=int)]\r\n",
      "        inter_block_layers_per_block_choice =\\\r\n",
      "            hp.Choice(name='inter_block_layers',\r\n",
      "                      values=inter_block_layers_per_block_options,\r\n",
      "                      ordered=True)\r\n",
      "\r\n",
      "        self.inter_block_layers_per_block = list()\r\n",
      "        if inter_block_layers_per_block_choice > 1:\r\n",
      "            for i in range(len(self.blocks) -1):\r\n",
      "                self.inter_block_layers_per_block\\\r\n",
      "                    .append(inter_block_layers_per_block_choice)\r\n",
      "\r\n",
      "        final_dense_layers_options = \\\r\n",
      "            [int(i) for i in np.linspace(self.minimum_final_dense_layers,\r\n",
      "                                         self.maximum_final_dense_layers,\r\n",
      "                                         self.n_options_final_dense_layers,\r\n",
      "                                         dtype=int)]\r\n",
      "        final_dense_layers_choice = hp.Choice(\r\n",
      "                                            name='final_dense_layers',\r\n",
      "                                            values=final_dense_layers_options,\r\n",
      "                                            ordered=True)\r\n",
      "        if final_dense_layers_choice == 0:\r\n",
      "            self.final_dense_layers = []\r\n",
      "        else:\r\n",
      "            self.final_dense_layers = [final_dense_layers_choice]\r\n",
      "\r\n",
      "        self.b_norm_or_dropout_residual_bypass_layers =\\\r\n",
      "            hp.Choice(name=\"b_norm_or_dropout_residual_bypass_layers\",\r\n",
      "                      values=['dropout','bnorm'],\r\n",
      "                      ordered=False)\r\n",
      "        dropout_rate_for_bypass_layers_choices =\\\r\n",
      "            [float(i) for i in\r\n",
      "                np.linspace(self.minimum_dropout_rate_for_bypass_layers,\r\n",
      "                        self.maximim_dropout_rate_for_bypass_layers,\r\n",
      "                        self.n_options_dropout_rate_for_bypass_layers,\r\n",
      "                        dtype=float)]\r\n",
      "        self.dropout_rate_for_bypass_layers =\\\r\n",
      "            hp.Choice(name='dropout_rate_for_bypass_layers',\r\n",
      "                      values=dropout_rate_for_bypass_layers_choices,\r\n",
      "                      ordered=True)\r\n",
      "\r\n",
      "        self.b_norm_or_dropout_last_layers =\\\r\n",
      "                        hp.Choice(name='b_norm_or_dropout_last_layers',\r\n",
      "                                  values=['dropout','bnorm'],\r\n",
      "                                  ordered=False)\r\n",
      "        \r\n",
      "        dropout_rate_options = [float(i) for i in \r\n",
      "                                np.linspace(self.minimum_dropout_rate, \r\n",
      "                                            self.maximum_dropout_rate,\r\n",
      "                                            self.n_options_dropout_rate,\r\n",
      "                                            dtype=float)]\r\n",
      "        self.dropout_rate = hp.Choice(name='dropout_rate',\r\n",
      "                                      values=dropout_rate_options,\r\n",
      "                                      ordered=True)\r\n",
      "\r\n",
      "        if self.problem_type == 'classification':\r\n",
      "            precision = tf.keras.metrics.Precision(),\r\n",
      "            recall = tf.keras.metrics.Recall()\r\n",
      "            accuracy = tf.keras.metrics.Accuracy()\r\n",
      "        if self.problem_type == 'classification' and\\\r\n",
      "                self.number_of_classes > 1:\r\n",
      "            metrics = [tf.keras.metrics.TopKCategoricalAccuracy(\r\n",
      "                k = k,\r\n",
      "                name=f'top_{k}_'\r\n",
      "                'categorical_'\r\n",
      "                'accuracy',\r\n",
      "                dtype=None)\r\n",
      "                           for k in np.arange(1,self.number_of_classes)\\\r\n",
      "                               if k < 10]\r\n",
      "            metrics.append(precision)\r\n",
      "            metrics.append(recall)\r\n",
      "            metrics.append(accuracy)\r\n",
      "        elif self.problem_type == 'classification' and\\\r\n",
      "                self.number_of_classes == 1:    \r\n",
      "            metrics = [precision, recall, accuracy]\r\n",
      "        else:\r\n",
      "            rmse = tf.keras.metrics.RootMeanSquaredError()\r\n",
      "            mae = tf.keras.metrics.MeanAbsoluteError()\r\n",
      "            metrics = [rmse, mae]\r\n",
      "    \r\n",
      "        inp = tf.keras.layers.Input(shape = self.input_shape) \r\n",
      "        # Start with input layer that fits. \r\n",
      "        # The keras fucntional API requires an explicit input layer\r\n",
      "        if self.bw_images:\r\n",
      "            x = self.grayscale_to_rgb(inp)\r\n",
      "        else:\r\n",
      "            x = inp\r\n",
      "        if self.base_model != '':\r\n",
      "            x = tf.keras.layers.Resizing(self.base_model_input_shape[0],\r\n",
      "                                         self.base_model_input_shape[1])(x)\r\n",
      "            x = self.base_model(x)\r\n",
      "        if self.flatten_after_base_model:\r\n",
      "            x = tf.keras.layers.Flatten()(x)\r\n",
      "        initializer = tf.keras.initializers.GlorotNormal()\r\n",
      "        for bl in np.arange(len(self.blocks)):\r\n",
      "            block = self.blocks[bl]\r\n",
      "            bypass_block = self.residual_bypass_dense_layers[bl]\r\n",
      "            \r\n",
      "            \r\n",
      "            x = tf.keras.layers.Dense(block[1],\r\n",
      "                                      self.activation,\r\n",
      "                                      kernel_initializer=initializer)(x)\r\n",
      "            y = x\r\n",
      "            x = tf.keras.layers.BatchNormalization()(x)\r\n",
      "            # x proceeds sequentially to the \r\n",
      "            # next Dense layer.\r\n",
      "            \r\n",
      "            if self.b_norm_or_dropout_residual_bypass_layers == 'dropout':\r\n",
      "                y = tf.keras.layers\\\r\n",
      "                    .Dropout(self.dropout_rate_for_bypass_layers)(y)\r\n",
      "            elif self.b_norm_or_dropout_residual_bypass_layers == 'bnorm':\r\n",
      "                y = tf.keras.layers.BatchNormalization()(y)\r\n",
      "            else:\r\n",
      "                raise ValueError(\"The parameter: \"\r\n",
      "                                 \"'b_norm_or_dropout_residual_bypass_\"\r\n",
      "                                 \"layers'\"\r\n",
      "                                 \" must be left default '', or be \"\r\n",
      "                                 \"'dropout' or may be 'bnorm'.\")\r\n",
      "            for bypass_layer in bypass_block:\r\n",
      "                y = tf.keras.layers.Dense(bypass_layer,\r\n",
      "                                          self.activation,\r\n",
      "                                          kernel_initializer=initializer)(y)\r\n",
      "                if self.b_norm_or_dropout_residual_bypass_layers == 'dropout':\r\n",
      "                    y = tf.keras.layers\\\r\n",
      "                        .Dropout(self.dropout_rate_for_bypass_layers)(y)\r\n",
      "                elif self.b_norm_or_dropout_residual_bypass_layers == 'bnorm':\r\n",
      "                    y = tf.keras.layers.BatchNormalization()(y)\r\n",
      "                else:\r\n",
      "                    raise ValueError(\"The parameter: \"\r\n",
      "                                     \"'b_norm_or_dropout_residual_bypass_\"\r\n",
      "                                     \"layers' must be left default '', or be \"\r\n",
      "                                     \"'dropout' or may be 'bnorm'.\")\r\n",
      "            # y does NOT proceed sequentially\r\n",
      "            # to the next layer. This bypasses \r\n",
      "            # several layers and give a memory \r\n",
      "            # that attenuates some of the \r\n",
      "            # deleterious effects of a deeper \r\n",
      "            # network and lets us capture more \r\n",
      "            # complex interactions before \r\n",
      "            # overfitting becomes an issue than \r\n",
      "            # the textbook sequential multi - \r\n",
      "            # layer perceptron ...\r\n",
      "            for j in np.arange(block[0]): \r\n",
      "                x = tf.keras.layers.Dense(block[1] - block[2] * j,\r\n",
      "                                          self.activation,\r\n",
      "                                          kernel_initializer=initializer)(x) \r\n",
      "                x = tf.keras.layers.BatchNormalization()(x)\r\n",
      "    \r\n",
      "            x = tf.keras.layers.Concatenate(axis=1)([x, y])\r\n",
      "            \r\n",
      "            if bl != np.arange(len(self.blocks)).max():\r\n",
      "                for inter_block_layer in self.inter_block_layers_per_block:\r\n",
      "                    x = tf.keras.layers.Dense(inter_block_layer,\r\n",
      "                                          self.activation,\r\n",
      "                                          kernel_initializer=initializer)(x)\r\n",
      "                    x = tf.keras.layers.BatchNormalization()(x)\r\n",
      "    \r\n",
      "        for i in self.final_dense_layers:\r\n",
      "            x = tf.keras.layers.Dense(i,\r\n",
      "                                      self.activation,\r\n",
      "                                      kernel_initializer=initializer)(x) \r\n",
      "            if self.b_norm_or_dropout_last_layers == 'bnorm':\r\n",
      "                x = tf.keras.layers.BatchNormalization()(x)\r\n",
      "            elif self.b_norm_or_dropout_last_layers == 'dropout':\r\n",
      "                x = tf.keras.layers.Dropout(self.dropout_rate)(x)\r\n",
      "            else:\r\n",
      "                raise ValueError(\"For b_norm_or_dropout_last_layers, \" \r\n",
      "                                 \"you must pick either 'dropout' or 'bnorm'\")\r\n",
      "        out = tf.keras.layers.Dense(self.number_of_classes,\r\n",
      "                                    self.final_activation,\r\n",
      "                                    kernel_initializer=initializer)(x)\r\n",
      "    \r\n",
      "        # Declare the graph for our model ...\r\n",
      "        modelo_final = tf.keras.Model(inputs=inp,outputs = out)\r\n",
      "        \r\n",
      "        modelo_final\\\r\n",
      "            .compile(optimizer=\\\r\n",
      "                     tf.keras.optimizers.Adam(\r\n",
      "                         learning_rate=self.learning_rate, \r\n",
      "                         clipnorm=1.0),\r\n",
      "                         loss=self.loss, \r\n",
      "                         metrics=metrics)\r\n",
      "        return modelo_final\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat residualmlp/residual_mlp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "908957f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Head of the console log from the NAS run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dfa9a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\r\n",
      "  Downloading pandas-1.4.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\r\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.7/11.7 MB 71.4 MB/s eta 0:00:00\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2021.1)\r\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.19.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\r\n",
      "Installing collected packages: pandas\r\n",
      "Successfully installed pandas-1.4.0\r\n",
      "Collecting keras_tuner\r\n",
      "  Downloading keras_tuner-1.1.0-py3-none-any.whl (98 kB)\r\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.0/98.0 KB 16.9 MB/s eta 0:00:00\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from keras_tuner) (2.26.0)\r\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.8/dist-packages (from keras_tuner) (2.6.0)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from keras_tuner) (1.4.1)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from keras_tuner) (1.19.4)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from keras_tuner) (21.0)\r\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (from keras_tuner) (7.27.0)\r\n",
      "Collecting kt-legacy\r\n",
      "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (4.7.0)\r\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (5.1.0)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (0.18.0)\r\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (0.7.5)\r\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (0.1.3)\r\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (2.10.0)\r\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (58.1.0)\r\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (5.1.0)\r\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (3.0.20)\r\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (0.2.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->keras_tuner) (2.4.7)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->keras_tuner) (2021.5.30)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests->keras_tuner) (2.0.6)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->keras_tuner) (3.2)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->keras_tuner) (1.26.7)\r\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (1.35.0)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (0.6.1)\r\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (0.37.0)\r\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (0.12.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (3.3.4)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (0.4.6)\r\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (3.17.3)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (1.8.0)\r\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (1.39.0)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (2.0.1)\r\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from absl-py>=0.4->tensorboard->keras_tuner) (1.15.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras_tuner) (0.2.8)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras_tuner) (4.7.2)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras_tuner) (4.2.2)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras_tuner) (1.3.0)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.16->ipython->keras_tuner) (0.8.2)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect>4.3->ipython->keras_tuner) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras_tuner) (0.2.5)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->keras_tuner) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras_tuner) (3.1.1)\r\n",
      "Installing collected packages: kt-legacy, keras_tuner\r\n",
      "Successfully installed keras_tuner-1.1.0 kt-legacy-1.0.4\r\n",
      "We're missing a module:\r\n",
      "No module named 'pandas'\r\n",
      "No problem, we install it...\r\n",
      "We're missing a module:\r\n",
      "No module named 'keras_tuner'\r\n",
      "No problem, we install it...\r\n",
      "Date: 2022-02-09_04_29\r\n",
      "project_name: CIFAR10_EfficientNetB7-ResidualMLP_NAS_SECOND_PASS_SEARCH\r\n",
      "training_set_size: 50000\r\n",
      "patience: 25\r\n",
      "PATIENCE_MIN_DELTA: 1e-05\r\n",
      "BATCH_SIZE: 50\r\n",
      "MAX_EPOCHS: 150\r\n",
      "RESULTS_DIR_FOR_SEARCH: 2022-02-09_04_29_CIFAR10_EfficientNetB7-ResidualMLP_NAS_SECOND_PASS_SEARCH_SEARCH_RUN\r\n",
      "MINIMUM_LEARNING_RATE 7e-05\r\n",
      "MAXIMUM_LEARNING_RATE: 0.7\r\n",
      "NUMBER_OF_LEARNING_RATES_TO_TRY: 7\r\n",
      "MINIMUM_NUMBER_OF_BLOCKS: 1\r\n",
      "MAXIMUM_NUMBER_OF_BLOCKS: 5\r\n",
      "MINIMUM_NUMBER_OF_LAYERS_PER_BLOCK: 2\r\n",
      "MAXIMUM_NUMBER_OF_LAYERS_PER_BLOCK: 4\r\n",
      "MINIMUM_NEURONS_PER_BLOCK_LAYER: 60\r\n",
      "MAXIMUM_NEURONS_PER_BLOCK_LAYER: 110\r\n",
      "N_OPTIONS_OF_NEURONS_PER_LAYER_TO_TRY: 7\r\n",
      "MINIMUM_NEURONS_PER_BLOCK_LAYER_DECAY: 10\r\n",
      "MAXIMUM_NEURONS_PER_BLOCK_LAYER_DECAY: 40\r\n",
      "MINIMUM_DROPOUT_RATE_FOR_BYPASS_LAYERS: 0.2\r\n",
      "MAXIMIM_DROPOUT_RATE_FOR_BYPASS_LAYERS: 0.6\r\n",
      "N_OPTIONS_DROPOUT_RATE_FOR_BYPASS_LAYERS: 3\r\n",
      "MINIMUM_INTER_BLOCK_LAYERS_PER_BLOCK: 0\r\n",
      "MAXIMUM_INTER_BLOCK_LAYERS_PER_BLOCK: 85\r\n",
      "N_OPTIONS_INTER_BLOCK_LAYERS_PER_BLOCK: 7\r\n",
      "MINIMUM_DROPOUT_RATE: 0.2\r\n",
      "MAXIMUM_DROPOUT_RATE: 0.4\r\n",
      "N_OPTIONS_DROPOUT_RATE: 5\r\n",
      "MINIMUM_FINAL_DENSE_LAYERS: 0\r\n",
      "MAXIMUM_FINAL_DENSE_LAYERS: 200\r\n",
      "N_OPTIONS_FINAL_DENSE_LAYERS: 7\r\n",
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\r\n",
      "\r",
      "    16384/170498071 [..............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "   712704/170498071 [..............................] - ETA: 11s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  3678208/170498071 [..............................] - ETA: 4s \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  7069696/170498071 [>.............................] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 11165696/170498071 [>.............................] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 15753216/170498071 [=>............................] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 20848640/170498071 [==>...........................] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 25124864/170498071 [===>..........................] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 30056448/170498071 [====>.........................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 34922496/170498071 [=====>........................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 40165376/170498071 [======>.......................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 44048384/170498071 [======>.......................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 49373184/170498071 [=======>......................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 55107584/170498071 [========>.....................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 59416576/170498071 [=========>....................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 63791104/170498071 [==========>...................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 69386240/170498071 [===========>..................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 74743808/170498071 [============>.................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 79159296/170498071 [============>.................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 83632128/170498071 [=============>................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 89006080/170498071 [==============>...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 94265344/170498071 [===============>..............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 98623488/170498071 [================>.............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "103424000/170498071 [=================>............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "109076480/170498071 [==================>...........] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "113565696/170498071 [==================>...........] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "118349824/170498071 [===================>..........] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "123248640/170498071 [====================>.........] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "128802816/170498071 [=====================>........] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "133054464/170498071 [======================>.......] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "137740288/170498071 [=======================>......] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "143106048/170498071 [========================>.....] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "148168704/170498071 [=========================>....] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "152494080/170498071 [=========================>....] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "157229056/170498071 [==========================>...] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "162881536/170498071 [===========================>..] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "167829504/170498071 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "170500096/170498071 [==============================] - 2s 0us/step\r\n",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "170508288/170498071 [==============================] - 2s 0us/step\r\n",
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb7.h5\r\n",
      "\r",
      "    16384/268326632 [..............................] - ETA: 34s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  1425408/268326632 [..............................] - ETA: 9s \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  4202496/268326632 [..............................] - ETA: 9s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  6889472/268326632 [..............................] - ETA: 7s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  8396800/268326632 [..............................] - ETA: 9s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  9707520/268326632 [>.............................] - ETA: 9s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 15982592/268326632 [>.............................] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 16785408/268326632 [>.............................] - ETA: 9s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 19456000/268326632 [=>............................] - ETA: 8s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 24240128/268326632 [=>............................] - ETA: 7s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 25174016/268326632 [=>............................] - ETA: 8s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 27500544/268326632 [==>...........................] - ETA: 8s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 29564928/268326632 [==>...........................] - ETA: 8s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 33562624/268326632 [==>...........................] - ETA: 7s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 37167104/268326632 [===>..........................] - ETA: 7s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 40624128/268326632 [===>..........................] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 41951232/268326632 [===>..........................] - ETA: 7s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 45113344/268326632 [====>.........................] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 48848896/268326632 [====>.........................] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 50339840/268326632 [====>.........................] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 54190080/268326632 [=====>........................] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 57991168/268326632 [=====>........................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 58728448/268326632 [=====>........................] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 61251584/268326632 [=====>........................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 64626688/268326632 [======>.......................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 67117056/268326632 [======>.......................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 70516736/268326632 [======>.......................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 73949184/268326632 [=======>......................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 75505664/268326632 [=======>......................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 79175680/268326632 [=======>......................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 82698240/268326632 [========>.....................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 83894272/268326632 [========>.....................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 87007232/268326632 [========>.....................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 88907776/268326632 [========>.....................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 92282880/268326632 [=========>....................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 95952896/268326632 [=========>....................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 99803136/268326632 [==========>...................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "100671488/268326632 [==========>...................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "102555648/268326632 [==========>...................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "107528192/268326632 [===========>..................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "109060096/268326632 [===========>..................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "110321664/268326632 [===========>..................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "114778112/268326632 [===========>..................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "119463936/268326632 [============>.................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "123084800/268326632 [============>.................] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "125837312/268326632 [=============>................] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "128671744/268326632 [=============>................] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "132579328/268326632 [=============>................] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "135421952/268326632 [==============>...............] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "138747904/268326632 [==============>...............] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "142614528/268326632 [==============>...............] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "144629760/268326632 [===============>..............] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "149970944/268326632 [===============>..............] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "151199744/268326632 [===============>..............] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "154206208/268326632 [================>.............] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "158244864/268326632 [================>.............] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "159391744/268326632 [================>.............] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "163241984/268326632 [=================>............] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "167010304/268326632 [=================>............] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "167780352/268326632 [=================>............] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "171155456/268326632 [==================>...........] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "174923776/268326632 [==================>...........] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "176168960/268326632 [==================>...........] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "177954816/268326632 [==================>...........] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "181231616/268326632 [===================>..........] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "184557568/268326632 [===================>..........] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "187703296/268326632 [===================>..........] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "188850176/268326632 [====================>.........] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "193257472/268326632 [====================>.........] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "195059712/268326632 [====================>.........] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "198844416/268326632 [=====================>........] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "204283904/268326632 [=====================>........] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "206831616/268326632 [======================>.......] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "209330176/268326632 [======================>.......] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "211836928/268326632 [======================>.......] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "213671936/268326632 [======================>.......] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "216113152/268326632 [=======================>......] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "218112000/268326632 [=======================>......] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "221306880/268326632 [=======================>......] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "222437376/268326632 [=======================>......] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "226500608/268326632 [========================>.....] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "228868096/268326632 [========================>.....] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "231448576/268326632 [========================>.....] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "234889216/268326632 [=========================>....] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "238575616/268326632 [=========================>....] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "241721344/268326632 [==========================>...] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "243277824/268326632 [==========================>...] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "245751808/268326632 [==========================>...] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "247881728/268326632 [==========================>...] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "251666432/268326632 [===========================>..] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "254959616/268326632 [===========================>..] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "258916352/268326632 [===========================>..] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "260808704/268326632 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "264380416/268326632 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "268083200/268326632 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "268328960/268326632 [==============================] - 7s 0us/step\r\n",
      "cat: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!cat 2022-02-09_04-29_CIFAR10_EfficientNetB7-ResidualMLP_NAS_SECOND_PASS_SEARCH_python3_shell_log.txt | head -n 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba03a5a7",
   "metadata": {},
   "source": [
    "## Recreate the environment in the task needed to run the evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b7db12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.5.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.3 MB 12.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (21.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.19.4)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 29.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-9.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.3 MB 38.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.29.1-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 34.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
      "Installing collected packages: pillow, kiwisolver, fonttools, cycler, matplotlib\n",
      "Successfully installed cycler-0.11.0 fonttools-4.29.1 kiwisolver-1.3.2 matplotlib-3.5.1 pillow-9.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.19.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "We're missing a module:\n",
      "No module named 'pendulum'\n",
      "No problem, we install it...\n",
      "Collecting pendulum\n",
      "  Downloading pendulum-2.1.2-cp38-cp38-manylinux1_x86_64.whl (155 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0,>=2.6 in /usr/local/lib/python3.8/dist-packages (from pendulum) (2.8.2)\n",
      "Collecting pytzdata>=2020.1\n",
      "  Downloading pytzdata-2020.1-py2.py3-none-any.whl (489 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil<3.0,>=2.6->pendulum) (1.15.0)\n",
      "Installing collected packages: pytzdata, pendulum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "WARNING: You are using pip version 21.2.4; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed pendulum-2.1.2 pytzdata-2020.1\n",
      "We're missing a module:\n",
      "No module named 'pandas'\n",
      "No problem, we install it...\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.4.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.19.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n",
      "Installing collected packages: pandas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "WARNING: You are using pip version 21.2.4; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed pandas-1.4.0\n",
      "We're missing a module:\n",
      "No module named 'keras_tuner'\n",
      "No problem, we install it...\n",
      "Collecting keras_tuner\n",
      "  Downloading keras_tuner-1.1.0-py3-none-any.whl (98 kB)\n",
      "Collecting kt-legacy\n",
      "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from keras_tuner) (21.0)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.8/dist-packages (from keras_tuner) (2.6.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from keras_tuner) (1.19.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from keras_tuner) (1.4.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from keras_tuner) (2.26.0)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (from keras_tuner) (7.27.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (0.18.0)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (0.1.3)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (5.1.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (3.0.20)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (4.7.0)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (2.10.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (0.7.5)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (5.1.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (58.1.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.16->ipython->keras_tuner) (0.8.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect>4.3->ipython->keras_tuner) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras_tuner) (0.2.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->keras_tuner) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->keras_tuner) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->keras_tuner) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests->keras_tuner) (2.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->keras_tuner) (3.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (2.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (0.4.6)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (3.17.3)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (1.39.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (1.35.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (0.37.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (1.8.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (0.12.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from absl-py>=0.4->tensorboard->keras_tuner) (1.15.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras_tuner) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras_tuner) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras_tuner) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras_tuner) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->keras_tuner) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras_tuner) (3.1.1)\n",
      "Installing collected packages: kt-legacy, keras-tuner\n",
      "Successfully installed keras-tuner-1.1.0 kt-legacy-1.0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "WARNING: You are using pip version 21.2.4; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.5.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (21.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.19.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (4.29.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install matplotlib\n",
    "! pip3 install numpy\n",
    "import os\n",
    "import subprocess\n",
    "import argparse\n",
    "# Add parser for common params\n",
    "\n",
    "try:\n",
    "    import pendulum\n",
    "except ModuleNotFoundError as not_found:\n",
    "    print(\"We're missing a module:\")\n",
    "    print(not_found)\n",
    "    print(\"No problem, we install it...\")\n",
    "    subprocess.run(\"pip3 install pendulum\", \n",
    "    \t           shell=True, \n",
    "    \t           check=True)\n",
    "    import pendulum\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ModuleNotFoundError as not_found:\n",
    "    print(\"We're missing a module:\")\n",
    "    print(not_found)\n",
    "    print(\"No problem, we install it...\")\n",
    "    subprocess.run(\"pip3 install pandas\", \n",
    "    \t           shell=True, \n",
    "    \t           check=True)\n",
    "    import pandas as pd\n",
    "\n",
    "try:\n",
    "    import keras_tuner as kt\n",
    "except ModuleNotFoundError as not_found:\n",
    "    print(\"We're missing a module:\")\n",
    "    print(not_found)\n",
    "    print(\"No problem, we install it...\")\n",
    "    subprocess.run(\"pip3 install keras_tuner\", \n",
    "    \t           shell=True, \n",
    "    \t           check=True)\n",
    "    import keras_tuner as kt\n",
    "\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except ModuleNotFoundError as not_found:\n",
    "    print(\"We're missing a module:\")\n",
    "    print(not_found)\n",
    "    print(\"No problem, we install it...\")\n",
    "    subprocess.run(\"pip3 install tensorflow\", \n",
    "    \t           shell=True, \n",
    "    \t           check=True)\n",
    "    import tensorflow as tf\n",
    "\n",
    "!pip3 install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "from residualmlp.residual_mlp import ResidualMLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d657d6ed",
   "metadata": {},
   "source": [
    "## Load the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b4f980d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 8s 0us/step\n",
      "170508288/170498071 [==============================] - 8s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-10 05:40:09.507196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-10 05:40:09.684874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-10 05:40:09.685948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-10 05:40:09.692044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-10 05:40:09.693161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-10 05:40:09.694057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-10 05:40:12.418159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-10 05:40:12.419183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-10 05:40:12.420036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-10 05:40:12.421350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14817 MB memory:  -> device: 0, name: Quadro RTX 5000, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cifar = tf.keras.datasets.cifar10.load_data()\n",
    "(x_train, y_train), (x_test, y_test) = cifar\n",
    "\n",
    "y_train_ohe = tf.one_hot([i[0] for i in  y_train],10)\n",
    "y_test_ohe = tf.one_hot([i[0] for i in y_test],10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fc6d07",
   "metadata": {},
   "source": [
    "## Set up a few global parameters for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f19da6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE = pendulum.now().__str__()[:16].replace(\"T\",\"_\").replace(\":\",\"_\")\n",
    "\n",
    "# Keras tuner search & fit args\n",
    "\n",
    "PATIENCE = 25\n",
    "PATIENCE_MIN_DELTA = .00001\n",
    "BATCH_SIZE = 50\n",
    "MAX_EPOCHS = 150\n",
    "\n",
    "\n",
    "\n",
    "# Base model args\n",
    "INPUT_SHAPE = (32,32,3)\n",
    "BASE_MODEL_INPUT_SHAPE = (600,600,3)\n",
    "\n",
    "PROJECT_NAME = \"CIFAR10_EfficientNetB7-ResidualMLP_NAS_SECOND_PASS_SEARCH\"\n",
    "TRAINING_SET_SIZE = 50000\n",
    "PATIENCE = 25\n",
    "PATIENCE_MIN_DELTA: 1e-05\n",
    "BATCH_SIZE = 50\n",
    "MAX_EPOCHS = 150\n",
    "RESULTS_DIR_FOR_SEARCH =\\\n",
    "    \"2022-02-09_04_29_CIFAR10_EfficientNetB7-ResidualMLP_NAS_SECOND_PASS_SEARCH_SEARCH_RUN\"\n",
    "MINIMUM_LEARNING_RATE = 7e-05\n",
    "MAXIMUM_LEARNING_RATE = 0.7\n",
    "NUMBER_OF_LEARNING_RATES_TO_TRY = 7\n",
    "MINIMUM_NUMBER_OF_BLOCKS = 1\n",
    "MAXIMUM_NUMBER_OF_BLOCKS = 5\n",
    "MINIMUM_NUMBER_OF_LAYERS_PER_BLOCK = 2\n",
    "MAXIMUM_NUMBER_OF_LAYERS_PER_BLOCK = 4\n",
    "MINIMUM_NEURONS_PER_BLOCK_LAYER = 60\n",
    "MAXIMUM_NEURONS_PER_BLOCK_LAYER = 110\n",
    "N_OPTIONS_OF_NEURONS_PER_LAYER_TO_TRY = 7\n",
    "MINIMUM_NEURONS_PER_BLOCK_LAYER_DECAY = 10\n",
    "MAXIMUM_NEURONS_PER_BLOCK_LAYER_DECAY = 40\n",
    "MINIMUM_DROPOUT_RATE_FOR_BYPASS_LAYERS = 0.2\n",
    "MAXIMIM_DROPOUT_RATE_FOR_BYPASS_LAYERS = 0.6\n",
    "N_OPTIONS_DROPOUT_RATE_FOR_BYPASS_LAYERS = 3\n",
    "MINIMUM_INTER_BLOCK_LAYERS_PER_BLOCK = 0\n",
    "MAXIMUM_INTER_BLOCK_LAYERS_PER_BLOCK = 85\n",
    "N_OPTIONS_INTER_BLOCK_LAYERS_PER_BLOCK = 7\n",
    "MINIMUM_DROPOUT_RATE = 0.2\n",
    "MAXIMUM_DROPOUT_RATE = 0.4\n",
    "N_OPTIONS_DROPOUT_RATE = 5\n",
    "MINIMUM_FINAL_DENSE_LAYERS = 0\n",
    "MAXIMUM_FINAL_DENSE_LAYERS = 200\n",
    "N_OPTIONS_FINAL_DENSE_LAYERS = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77242e22",
   "metadata": {},
   "source": [
    "## Set up our base model: \n",
    "1. EfficientNetB7\n",
    "2. Pre-trained on ImageNet.\n",
    "3. Set only the last Conv2D layer to trainable.\n",
    "4. Drop the trailing Dense layer and BatchNormalization layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cd56174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb7.h5\n",
      "268328960/268326632 [==============================] - 12s 0us/step\n",
      "268337152/268326632 [==============================] - 12s 0us/step\n"
     ]
    }
   ],
   "source": [
    "mod_with_fc_raw = tf.keras.applications.efficientnet.EfficientNetB7(\n",
    "    include_top=True, weights='imagenet', input_tensor=None,\n",
    "    input_shape = BASE_MODEL_INPUT_SHAPE, pooling='max', classes=1000\n",
    ")\n",
    "\n",
    "# Make the deepest conv2d layer trainable, leave everything else\n",
    "# as not trainable\n",
    "for layer in mod_with_fc_raw.layers:\n",
    "    layer.trainable = False\n",
    "# Last conv2d layer. This we want to train .\n",
    "mod_with_fc_raw.layers[-6].trainable = True\n",
    "\n",
    "# Create the final base model\n",
    "# (remove the final Dense and BatchNormalization layers ...) \n",
    "efficient_net_b_7_transferable_base_model =\\\n",
    "    tf.keras.Model(inputs=mod_with_fc_raw.layers[0].input, \n",
    "                    outputs=mod_with_fc_raw.layers[-3].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71dcf58",
   "metadata": {},
   "source": [
    "## Instantiate  the ResidualMLP class object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe0bd91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_builder = ResidualMLP(\n",
    "                    problem_type= \"classification\",  \n",
    "                    minimum_learning_rate = MINIMUM_LEARNING_RATE, \n",
    "                    maximum_learning_rate = MAXIMUM_LEARNING_RATE, \n",
    "                    number_of_learning_rates_to_try =\n",
    "                        NUMBER_OF_LEARNING_RATES_TO_TRY, \n",
    "                    input_shape = (32,32,3), \n",
    "                    bw_images = False, \n",
    "                    base_model = \n",
    "                        efficient_net_b_7_transferable_base_model, \n",
    "                    base_model_input_shape = BASE_MODEL_INPUT_SHAPE, \n",
    "                    flatten_after_base_model = False, \n",
    "                    minimum_number_of_blocks = MINIMUM_NUMBER_OF_BLOCKS, \n",
    "                    maximum_number_of_blocks = MAXIMUM_NUMBER_OF_BLOCKS, \n",
    "                    minimum_number_of_layers_per_block =\n",
    "                        MINIMUM_NUMBER_OF_LAYERS_PER_BLOCK, \n",
    "                    maximum_number_of_layers_per_block =\n",
    "                        MAXIMUM_NUMBER_OF_LAYERS_PER_BLOCK,\n",
    "                    minimum_neurons_per_block_layer =\n",
    "                        MINIMUM_NEURONS_PER_BLOCK_LAYER, \n",
    "                    maximum_neurons_per_block_layer =\n",
    "                        MAXIMUM_NEURONS_PER_BLOCK_LAYER, \n",
    "                    n_options_of_neurons_per_layer_to_try =\n",
    "                        N_OPTIONS_OF_NEURONS_PER_LAYER_TO_TRY, \n",
    "                    minimum_neurons_per_block_layer_decay =\n",
    "                        MINIMUM_NEURONS_PER_BLOCK_LAYER_DECAY, \n",
    "                    maximum_neurons_per_block_layer_decay = \n",
    "                        MAXIMUM_NEURONS_PER_BLOCK_LAYER_DECAY, \n",
    "                    minimum_dropout_rate_for_bypass_layers =\n",
    "                        MINIMUM_DROPOUT_RATE_FOR_BYPASS_LAYERS, \n",
    "                    maximim_dropout_rate_for_bypass_layers =\n",
    "                        MAXIMIM_DROPOUT_RATE_FOR_BYPASS_LAYERS, \n",
    "                    n_options_dropout_rate_for_bypass_layers =\n",
    "                        N_OPTIONS_DROPOUT_RATE_FOR_BYPASS_LAYERS,\n",
    "                    minimum_inter_block_layers_per_block =\n",
    "                        MINIMUM_INTER_BLOCK_LAYERS_PER_BLOCK, \n",
    "                    maximum_inter_block_layers_per_block =\n",
    "                        MAXIMUM_INTER_BLOCK_LAYERS_PER_BLOCK,\n",
    "                    n_options_inter_block_layers_per_block =\\\n",
    "                        N_OPTIONS_INTER_BLOCK_LAYERS_PER_BLOCK,\n",
    "                    minimum_dropout_rate = MINIMUM_DROPOUT_RATE, \n",
    "                    maximum_dropout_rate = MAXIMUM_DROPOUT_RATE,\n",
    "                    n_options_dropout_rate = N_OPTIONS_DROPOUT_RATE, \n",
    "                    minimum_final_dense_layers =\n",
    "                        MINIMUM_FINAL_DENSE_LAYERS,\n",
    "                    maximum_final_dense_layers =\n",
    "                        MAXIMUM_FINAL_DENSE_LAYERS, \n",
    "                    n_options_final_dense_layers =\n",
    "                        N_OPTIONS_FINAL_DENSE_LAYERS, \n",
    "                    number_of_classes = 10,\n",
    "                    final_activation = tf.keras.activations.softmax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24232cb4",
   "metadata": {},
   "source": [
    "## Instantiate our Hyperband object. This will pull the \"Oracles\" from the default location and restore the object to its state at the end of the training run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f650583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-10 05:40:30.408545: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2022-02-10 05:40:30.408625: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2022-02-10 05:40:30.410607: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project ./untitled_project/oracle.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-10 05:40:30.600298: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2022-02-10 05:40:30.600545: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1749] CUPTI activity buffer flushed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All permutations:\n",
      "      number_of_blocks  layers_per_block  neurons_per_block_layer  \\\n",
      "0                    1                 2                       60   \n",
      "1                    1                 2                       60   \n",
      "2                    1                 2                       60   \n",
      "3                    1                 2                       60   \n",
      "4                    1                 2                       60   \n",
      "...                ...               ...                      ...   \n",
      "3250                 5                 4                      110   \n",
      "3251                 5                 4                      110   \n",
      "3252                 5                 4                      110   \n",
      "3253                 5                 4                      110   \n",
      "3254                 5                 4                      110   \n",
      "\n",
      "      neurons_per_block_layer_decay  \n",
      "0                                10  \n",
      "1                                11  \n",
      "2                                12  \n",
      "3                                13  \n",
      "4                                14  \n",
      "...                             ...  \n",
      "3250                             36  \n",
      "3251                             37  \n",
      "3252                             38  \n",
      "3253                             39  \n",
      "3254                             40  \n",
      "\n",
      "[3255 rows x 4 columns]\n",
      "Valid permutations\n",
      "      number_of_blocks  layers_per_block  neurons_per_block_layer  \\\n",
      "0                    1                 2                       60   \n",
      "1                    1                 2                       60   \n",
      "2                    1                 2                       60   \n",
      "3                    1                 2                       60   \n",
      "4                    1                 2                       60   \n",
      "...                ...               ...                      ...   \n",
      "2030                 5                 4                      110   \n",
      "2031                 5                 4                      110   \n",
      "2032                 5                 4                      110   \n",
      "2033                 5                 4                      110   \n",
      "2034                 5                 4                      110   \n",
      "\n",
      "      neurons_per_block_layer_decay  valid_block  \n",
      "0                                10         True  \n",
      "1                                11         True  \n",
      "2                                12         True  \n",
      "3                                13         True  \n",
      "4                                14         True  \n",
      "...                             ...          ...  \n",
      "2030                             23         True  \n",
      "2031                             24         True  \n",
      "2032                             25         True  \n",
      "2033                             26         True  \n",
      "2034                             27         True  \n",
      "\n",
      "[2035 rows x 5 columns]\n",
      "Valid permutations with blocks column\n",
      "      number_of_blocks  layers_per_block  neurons_per_block_layer  \\\n",
      "0                    1                 2                       60   \n",
      "1                    1                 2                       60   \n",
      "2                    1                 2                       60   \n",
      "3                    1                 2                       60   \n",
      "4                    1                 2                       60   \n",
      "...                ...               ...                      ...   \n",
      "2030                 5                 4                      110   \n",
      "2031                 5                 4                      110   \n",
      "2032                 5                 4                      110   \n",
      "2033                 5                 4                      110   \n",
      "2034                 5                 4                      110   \n",
      "\n",
      "      neurons_per_block_layer_decay  valid_block  \\\n",
      "0                                10         True   \n",
      "1                                11         True   \n",
      "2                                12         True   \n",
      "3                                13         True   \n",
      "4                                14         True   \n",
      "...                             ...          ...   \n",
      "2030                             23         True   \n",
      "2031                             24         True   \n",
      "2032                             25         True   \n",
      "2033                             26         True   \n",
      "2034                             27         True   \n",
      "\n",
      "                                                 blocks  \n",
      "0                                         [[2, 60, 10]]  \n",
      "1                                         [[2, 60, 11]]  \n",
      "2                                         [[2, 60, 12]]  \n",
      "3                                         [[2, 60, 13]]  \n",
      "4                                         [[2, 60, 14]]  \n",
      "...                                                 ...  \n",
      "2030  [[4, 110, 23], [4, 110, 23], [4, 110, 23], [4,...  \n",
      "2031  [[4, 110, 24], [4, 110, 24], [4, 110, 24], [4,...  \n",
      "2032  [[4, 110, 25], [4, 110, 25], [4, 110, 25], [4,...  \n",
      "2033  [[4, 110, 26], [4, 110, 26], [4, 110, 26], [4,...  \n",
      "2034  [[4, 110, 27], [4, 110, 27], [4, 110, 27], [4,...  \n",
      "\n",
      "[2035 rows x 6 columns]\n",
      "[[2, 60, 10]]\n",
      "INFO:tensorflow:Reloading Tuner from ./untitled_project/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "RESULTS_DIR_FOR_SEARCH =\\\n",
    "    f'{DATE}_{PROJECT_NAME}_SEARCH_RUN'\n",
    "\n",
    "logdir_for_search = os.path.join(\"logs\", RESULTS_DIR_FOR_SEARCH + \"_TB\")\n",
    "tensorboard_callback_search =\\\n",
    "    tf.keras.callbacks.TensorBoard(logdir_for_search, histogram_freq=1)\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    model_builder.build_auto_residual_mlp,\n",
    "    objective='val_loss',\n",
    "    max_epochs = MAX_EPOCHS,\n",
    "    hyperband_iterations = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b4de5e",
   "metadata": {},
   "source": [
    "\n",
    "## Let's look at a summary of the training run's 10 best trials:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1217daf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in ./untitled_project\n",
      "Showing 10 best trials\n",
      "Objective(name='val_loss', direction='min')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0037513129240040457\n",
      "blocks: 493\n",
      "bypass_layers_units: 110\n",
      "inter_block_layers: 0\n",
      "final_dense_layers: 133\n",
      "b_norm_or_dropout_residual_bypass_layers: bnorm\n",
      "dropout_rate_for_bypass_layers: 0.4\n",
      "b_norm_or_dropout_last_layers: bnorm\n",
      "dropout_rate: 0.30000000000000004\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.2851129472255707\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0006246016228043756\n",
      "blocks: 1498\n",
      "bypass_layers_units: 85\n",
      "inter_block_layers: 0\n",
      "final_dense_layers: 200\n",
      "b_norm_or_dropout_residual_bypass_layers: dropout\n",
      "dropout_rate_for_bypass_layers: 0.4\n",
      "b_norm_or_dropout_last_layers: dropout\n",
      "dropout_rate: 0.35000000000000003\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.3170013427734375\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0008470649536294128\n",
      "blocks: 1398\n",
      "bypass_layers_units: 76\n",
      "inter_block_layers: 56\n",
      "final_dense_layers: 66\n",
      "b_norm_or_dropout_residual_bypass_layers: bnorm\n",
      "dropout_rate_for_bypass_layers: 0.2\n",
      "b_norm_or_dropout_last_layers: dropout\n",
      "dropout_rate: 0.2\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.42188236117362976\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0002917111442780575\n",
      "blocks: 143\n",
      "bypass_layers_units: 85\n",
      "inter_block_layers: 56\n",
      "final_dense_layers: 33\n",
      "b_norm_or_dropout_residual_bypass_layers: bnorm\n",
      "dropout_rate_for_bypass_layers: 0.2\n",
      "b_norm_or_dropout_last_layers: bnorm\n",
      "dropout_rate: 0.35000000000000003\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.4766380786895752\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0003789906570254459\n",
      "blocks: 1614\n",
      "bypass_layers_units: 68\n",
      "inter_block_layers: 14\n",
      "final_dense_layers: 33\n",
      "b_norm_or_dropout_residual_bypass_layers: bnorm\n",
      "dropout_rate_for_bypass_layers: 0.4\n",
      "b_norm_or_dropout_last_layers: bnorm\n",
      "dropout_rate: 0.35000000000000003\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 0.6864215135574341\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.00013363813653334276\n",
      "blocks: 620\n",
      "bypass_layers_units: 76\n",
      "inter_block_layers: 0\n",
      "final_dense_layers: 33\n",
      "b_norm_or_dropout_residual_bypass_layers: dropout\n",
      "dropout_rate_for_bypass_layers: 0.2\n",
      "b_norm_or_dropout_last_layers: dropout\n",
      "dropout_rate: 0.4\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 1.4589403867721558\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0012050701779102787\n",
      "blocks: 1586\n",
      "bypass_layers_units: 60\n",
      "inter_block_layers: 14\n",
      "final_dense_layers: 33\n",
      "b_norm_or_dropout_residual_bypass_layers: bnorm\n",
      "dropout_rate_for_bypass_layers: 0.2\n",
      "b_norm_or_dropout_last_layers: dropout\n",
      "dropout_rate: 0.25\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 1.4733072519302368\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0016066949548811903\n",
      "blocks: 1317\n",
      "bypass_layers_units: 93\n",
      "inter_block_layers: 42\n",
      "final_dense_layers: 100\n",
      "b_norm_or_dropout_residual_bypass_layers: dropout\n",
      "dropout_rate_for_bypass_layers: 0.4\n",
      "b_norm_or_dropout_last_layers: dropout\n",
      "dropout_rate: 0.30000000000000004\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 1.7312233448028564\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0025446792783502432\n",
      "blocks: 1624\n",
      "bypass_layers_units: 101\n",
      "inter_block_layers: 70\n",
      "final_dense_layers: 0\n",
      "b_norm_or_dropout_residual_bypass_layers: dropout\n",
      "dropout_rate_for_bypass_layers: 0.6\n",
      "b_norm_or_dropout_last_layers: bnorm\n",
      "dropout_rate: 0.25\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 1.9862803220748901\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.003860073187707063\n",
      "blocks: 1628\n",
      "bypass_layers_units: 93\n",
      "inter_block_layers: 56\n",
      "final_dense_layers: 33\n",
      "b_norm_or_dropout_residual_bypass_layers: dropout\n",
      "dropout_rate_for_bypass_layers: 0.2\n",
      "b_norm_or_dropout_last_layers: dropout\n",
      "dropout_rate: 0.30000000000000004\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 4\n",
      "tuner/round: 0\n",
      "Score: 1.9964237213134766\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3da849",
   "metadata": {},
   "source": [
    "## Let's restore the best model: (This has only been trained for 2 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b39b1340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All permutations:\n",
      "      number_of_blocks  layers_per_block  neurons_per_block_layer  \\\n",
      "0                    1                 2                       60   \n",
      "1                    1                 2                       60   \n",
      "2                    1                 2                       60   \n",
      "3                    1                 2                       60   \n",
      "4                    1                 2                       60   \n",
      "...                ...               ...                      ...   \n",
      "3250                 5                 4                      110   \n",
      "3251                 5                 4                      110   \n",
      "3252                 5                 4                      110   \n",
      "3253                 5                 4                      110   \n",
      "3254                 5                 4                      110   \n",
      "\n",
      "      neurons_per_block_layer_decay  \n",
      "0                                10  \n",
      "1                                11  \n",
      "2                                12  \n",
      "3                                13  \n",
      "4                                14  \n",
      "...                             ...  \n",
      "3250                             36  \n",
      "3251                             37  \n",
      "3252                             38  \n",
      "3253                             39  \n",
      "3254                             40  \n",
      "\n",
      "[3255 rows x 4 columns]\n",
      "Valid permutations\n",
      "      number_of_blocks  layers_per_block  neurons_per_block_layer  \\\n",
      "0                    1                 2                       60   \n",
      "1                    1                 2                       60   \n",
      "2                    1                 2                       60   \n",
      "3                    1                 2                       60   \n",
      "4                    1                 2                       60   \n",
      "...                ...               ...                      ...   \n",
      "2030                 5                 4                      110   \n",
      "2031                 5                 4                      110   \n",
      "2032                 5                 4                      110   \n",
      "2033                 5                 4                      110   \n",
      "2034                 5                 4                      110   \n",
      "\n",
      "      neurons_per_block_layer_decay  valid_block  \n",
      "0                                10         True  \n",
      "1                                11         True  \n",
      "2                                12         True  \n",
      "3                                13         True  \n",
      "4                                14         True  \n",
      "...                             ...          ...  \n",
      "2030                             23         True  \n",
      "2031                             24         True  \n",
      "2032                             25         True  \n",
      "2033                             26         True  \n",
      "2034                             27         True  \n",
      "\n",
      "[2035 rows x 5 columns]\n",
      "Valid permutations with blocks column\n",
      "      number_of_blocks  layers_per_block  neurons_per_block_layer  \\\n",
      "0                    1                 2                       60   \n",
      "1                    1                 2                       60   \n",
      "2                    1                 2                       60   \n",
      "3                    1                 2                       60   \n",
      "4                    1                 2                       60   \n",
      "...                ...               ...                      ...   \n",
      "2030                 5                 4                      110   \n",
      "2031                 5                 4                      110   \n",
      "2032                 5                 4                      110   \n",
      "2033                 5                 4                      110   \n",
      "2034                 5                 4                      110   \n",
      "\n",
      "      neurons_per_block_layer_decay  valid_block  \\\n",
      "0                                10         True   \n",
      "1                                11         True   \n",
      "2                                12         True   \n",
      "3                                13         True   \n",
      "4                                14         True   \n",
      "...                             ...          ...   \n",
      "2030                             23         True   \n",
      "2031                             24         True   \n",
      "2032                             25         True   \n",
      "2033                             26         True   \n",
      "2034                             27         True   \n",
      "\n",
      "                                                 blocks  \n",
      "0                                         [[2, 60, 10]]  \n",
      "1                                         [[2, 60, 11]]  \n",
      "2                                         [[2, 60, 12]]  \n",
      "3                                         [[2, 60, 13]]  \n",
      "4                                         [[2, 60, 14]]  \n",
      "...                                                 ...  \n",
      "2030  [[4, 110, 23], [4, 110, 23], [4, 110, 23], [4,...  \n",
      "2031  [[4, 110, 24], [4, 110, 24], [4, 110, 24], [4,...  \n",
      "2032  [[4, 110, 25], [4, 110, 25], [4, 110, 25], [4,...  \n",
      "2033  [[4, 110, 26], [4, 110, 26], [4, 110, 26], [4,...  \n",
      "2034  [[4, 110, 27], [4, 110, 27], [4, 110, 27], [4,...  \n",
      "\n",
      "[2035 rows x 6 columns]\n",
      "[[2, 85, 24], [2, 85, 24]]\n"
     ]
    }
   ],
   "source": [
    "best_model_2022_02_09 = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11d6b11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "resizing (Resizing)             (None, 600, 600, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model (Functional)              (None, 2560)         64097687    resizing[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 85)           217685      model[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 85)           340         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 85)           7310        batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 85)           340         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 85)           340         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 61)           5246        batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 110)          9460        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 61)           244         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 110)          440         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 171)          0           batch_normalization_4[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 85)           14620       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 85)           340         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 85)           7310        batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 85)           340         dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 85)           340         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 61)           5246        batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 110)          9460        batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 61)           244         dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 110)          440         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 171)          0           batch_normalization_9[0][0]      \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 133)          22876       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 133)          532         dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 10)           1340        batch_normalization_10[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 64,402,180\n",
      "Trainable params: 1,940,923\n",
      "Non-trainable params: 62,461,257\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model_2022_02_09.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11dd958",
   "metadata": {},
   "source": [
    "## Out of the box model from the NAS tested:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a655094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-10 05:40:54.088724: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 30720000 exceeds 10% of free system memory.\n",
      "2022-02-10 05:40:54.129476: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-02-10 05:41:02.748672: I tensorflow/stream_executor/cuda/cuda_dnn.cc:381] Loaded cuDNN version 8204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 450s 1s/step - loss: 0.2970 - top_1_categorical_accuracy: 0.9051 - top_2_categorical_accuracy: 0.9680 - top_3_categorical_accuracy: 0.9838 - top_4_categorical_accuracy: 0.9915 - top_5_categorical_accuracy: 0.9959 - top_6_categorical_accuracy: 0.9980 - top_7_categorical_accuracy: 0.9989 - top_8_categorical_accuracy: 0.9994 - top_9_categorical_accuracy: 0.9998 - precision: 0.9228 - recall: 0.8930 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.29702749848365784,\n",
       " 0.9050999879837036,\n",
       " 0.9679999947547913,\n",
       " 0.9837999939918518,\n",
       " 0.9915000200271606,\n",
       " 0.9958999752998352,\n",
       " 0.9980000257492065,\n",
       " 0.9988999962806702,\n",
       " 0.9994000196456909,\n",
       " 0.9998000264167786,\n",
       " 0.9228066802024841,\n",
       " 0.8930000066757202,\n",
       " 0.0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "best_model_2022_02_09.evaluate(x_test,y_test_ohe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d7ac87",
   "metadata": {},
   "source": [
    "## The NAS generated and trained model's performance: top_1_categorical_accuracy: 0.9051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f580345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All permutations:\n",
      "      number_of_blocks  layers_per_block  neurons_per_block_layer  \\\n",
      "0                    1                 2                       60   \n",
      "1                    1                 2                       60   \n",
      "2                    1                 2                       60   \n",
      "3                    1                 2                       60   \n",
      "4                    1                 2                       60   \n",
      "...                ...               ...                      ...   \n",
      "3250                 5                 4                      110   \n",
      "3251                 5                 4                      110   \n",
      "3252                 5                 4                      110   \n",
      "3253                 5                 4                      110   \n",
      "3254                 5                 4                      110   \n",
      "\n",
      "      neurons_per_block_layer_decay  \n",
      "0                                10  \n",
      "1                                11  \n",
      "2                                12  \n",
      "3                                13  \n",
      "4                                14  \n",
      "...                             ...  \n",
      "3250                             36  \n",
      "3251                             37  \n",
      "3252                             38  \n",
      "3253                             39  \n",
      "3254                             40  \n",
      "\n",
      "[3255 rows x 4 columns]\n",
      "Valid permutations\n",
      "      number_of_blocks  layers_per_block  neurons_per_block_layer  \\\n",
      "0                    1                 2                       60   \n",
      "1                    1                 2                       60   \n",
      "2                    1                 2                       60   \n",
      "3                    1                 2                       60   \n",
      "4                    1                 2                       60   \n",
      "...                ...               ...                      ...   \n",
      "2030                 5                 4                      110   \n",
      "2031                 5                 4                      110   \n",
      "2032                 5                 4                      110   \n",
      "2033                 5                 4                      110   \n",
      "2034                 5                 4                      110   \n",
      "\n",
      "      neurons_per_block_layer_decay  valid_block  \n",
      "0                                10         True  \n",
      "1                                11         True  \n",
      "2                                12         True  \n",
      "3                                13         True  \n",
      "4                                14         True  \n",
      "...                             ...          ...  \n",
      "2030                             23         True  \n",
      "2031                             24         True  \n",
      "2032                             25         True  \n",
      "2033                             26         True  \n",
      "2034                             27         True  \n",
      "\n",
      "[2035 rows x 5 columns]\n",
      "Valid permutations with blocks column\n",
      "      number_of_blocks  layers_per_block  neurons_per_block_layer  \\\n",
      "0                    1                 2                       60   \n",
      "1                    1                 2                       60   \n",
      "2                    1                 2                       60   \n",
      "3                    1                 2                       60   \n",
      "4                    1                 2                       60   \n",
      "...                ...               ...                      ...   \n",
      "2030                 5                 4                      110   \n",
      "2031                 5                 4                      110   \n",
      "2032                 5                 4                      110   \n",
      "2033                 5                 4                      110   \n",
      "2034                 5                 4                      110   \n",
      "\n",
      "      neurons_per_block_layer_decay  valid_block  \\\n",
      "0                                10         True   \n",
      "1                                11         True   \n",
      "2                                12         True   \n",
      "3                                13         True   \n",
      "4                                14         True   \n",
      "...                             ...          ...   \n",
      "2030                             23         True   \n",
      "2031                             24         True   \n",
      "2032                             25         True   \n",
      "2033                             26         True   \n",
      "2034                             27         True   \n",
      "\n",
      "                                                 blocks  \n",
      "0                                         [[2, 60, 10]]  \n",
      "1                                         [[2, 60, 11]]  \n",
      "2                                         [[2, 60, 12]]  \n",
      "3                                         [[2, 60, 13]]  \n",
      "4                                         [[2, 60, 14]]  \n",
      "...                                                 ...  \n",
      "2030  [[4, 110, 23], [4, 110, 23], [4, 110, 23], [4,...  \n",
      "2031  [[4, 110, 24], [4, 110, 24], [4, 110, 24], [4,...  \n",
      "2032  [[4, 110, 25], [4, 110, 25], [4, 110, 25], [4,...  \n",
      "2033  [[4, 110, 26], [4, 110, 26], [4, 110, 26], [4,...  \n",
      "2034  [[4, 110, 27], [4, 110, 27], [4, 110, 27], [4,...  \n",
      "\n",
      "[2035 rows x 6 columns]\n",
      "[[2, 85, 24], [2, 85, 24]]\n"
     ]
    }
   ],
   "source": [
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "final_model = model_builder.build_auto_residual_mlp(best_hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2339d520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "resizing_1 (Resizing)           (None, 600, 600, 3)  0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model (Functional)              (None, 2560)         64097687    resizing_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 85)           217685      model[2][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 85)           340         dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 85)           7310        batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 85)           340         dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 85)           340         dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 61)           5246        batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 110)          9460        batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 61)           244         dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 110)          440         dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 171)          0           batch_normalization_15[0][0]     \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 85)           14620       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 85)           340         dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 85)           7310        batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 85)           340         dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 85)           340         dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 61)           5246        batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 110)          9460        batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 61)           244         dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 110)          440         dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 171)          0           batch_normalization_20[0][0]     \n",
      "                                                                 batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 133)          22876       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 133)          532         dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 10)           1340        batch_normalization_21[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 64,402,180\n",
      "Trainable params: 1,940,923\n",
      "Non-trainable params: 62,461,257\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d77ef3f",
   "metadata": {},
   "source": [
    "## Train the model manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f174772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-10 05:53:07.141442: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2022-02-10 05:53:07.141516: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2022-02-10 05:53:08.072859: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2022-02-10 05:53:08.073124: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1749] CUPTI activity buffer flushed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "RESULTS_DIR_FOR_FINAL_MODEL = \"2022-02-09_manually_trained_fianl_model\"\n",
    "\n",
    "logdir_final_model = os.path.join(\"logs\",\n",
    "                                  RESULTS_DIR_FOR_FINAL_MODEL + \"_TB\")\n",
    "tensorboard_callback_final =\\\n",
    "    tf.keras.callbacks.TensorBoard(logdir_final_model, histogram_freq=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc322f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-10 05:54:13.776717: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 138240000 exceeds 10% of free system memory.\n",
      "2022-02-10 05:54:13.856200: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 138240000 exceeds 10% of free system memory.\n",
      "/usr/local/lib/python3.8/dist-packages/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "  1/900 [..............................] - ETA: 6:01:48 - loss: 3.2705 - top_1_categorical_accuracy: 0.1200 - top_2_categorical_accuracy: 0.2000 - top_3_categorical_accuracy: 0.3200 - top_4_categorical_accuracy: 0.3800 - top_5_categorical_accuracy: 0.4400 - top_6_categorical_accuracy: 0.4800 - top_7_categorical_accuracy: 0.5800 - top_8_categorical_accuracy: 0.6800 - top_9_categorical_accuracy: 0.8600 - precision_1: 0.1000 - recall_1: 0.0200 - accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-10 05:54:39.188578: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2022-02-10 05:54:39.188675: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  2/900 [..............................] - ETA: 1:02:40 - loss: 2.7756 - top_1_categorical_accuracy: 0.1700 - top_2_categorical_accuracy: 0.3300 - top_3_categorical_accuracy: 0.4200 - top_4_categorical_accuracy: 0.5300 - top_5_categorical_accuracy: 0.6000 - top_6_categorical_accuracy: 0.6800 - top_7_categorical_accuracy: 0.7600 - top_8_categorical_accuracy: 0.8100 - top_9_categorical_accuracy: 0.9100 - precision_1: 0.2083 - recall_1: 0.0500 - accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-10 05:54:42.550361: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2022-02-10 05:54:42.551014: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1749] CUPTI activity buffer flushed\n",
      "2022-02-10 05:54:42.655734: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 2871 callback api events and 2868 activity events. \n",
      "2022-02-10 05:54:42.777403: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2022-02-10 05:54:42.932690: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/2022-02-09_manually_trained_fianl_model_TB/train/plugins/profile/2022_02_10_05_54_42\n",
      "\n",
      "2022-02-10 05:54:42.981485: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to logs/2022-02-09_manually_trained_fianl_model_TB/train/plugins/profile/2022_02_10_05_54_42/nqnzthuxgu.trace.json.gz\n",
      "2022-02-10 05:54:43.170175: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/2022-02-09_manually_trained_fianl_model_TB/train/plugins/profile/2022_02_10_05_54_42\n",
      "\n",
      "2022-02-10 05:54:43.181377: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to logs/2022-02-09_manually_trained_fianl_model_TB/train/plugins/profile/2022_02_10_05_54_42/nqnzthuxgu.memory_profile.json.gz\n",
      "2022-02-10 05:54:43.192915: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: logs/2022-02-09_manually_trained_fianl_model_TB/train/plugins/profile/2022_02_10_05_54_42\n",
      "Dumped tool data for xplane.pb to logs/2022-02-09_manually_trained_fianl_model_TB/train/plugins/profile/2022_02_10_05_54_42/nqnzthuxgu.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs/2022-02-09_manually_trained_fianl_model_TB/train/plugins/profile/2022_02_10_05_54_42/nqnzthuxgu.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/2022-02-09_manually_trained_fianl_model_TB/train/plugins/profile/2022_02_10_05_54_42/nqnzthuxgu.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/2022-02-09_manually_trained_fianl_model_TB/train/plugins/profile/2022_02_10_05_54_42/nqnzthuxgu.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/2022-02-09_manually_trained_fianl_model_TB/train/plugins/profile/2022_02_10_05_54_42/nqnzthuxgu.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900/900 [==============================] - 2293s 3s/step - loss: 0.3827 - top_1_categorical_accuracy: 0.8817 - top_2_categorical_accuracy: 0.9554 - top_3_categorical_accuracy: 0.9763 - top_4_categorical_accuracy: 0.9865 - top_5_categorical_accuracy: 0.9921 - top_6_categorical_accuracy: 0.9954 - top_7_categorical_accuracy: 0.9969 - top_8_categorical_accuracy: 0.9981 - top_9_categorical_accuracy: 0.9992 - precision_1: 0.9062 - recall_1: 0.8606 - accuracy: 0.0000e+00 - val_loss: 0.2796 - val_top_1_categorical_accuracy: 0.9108 - val_top_2_categorical_accuracy: 0.9702 - val_top_3_categorical_accuracy: 0.9858 - val_top_4_categorical_accuracy: 0.9936 - val_top_5_categorical_accuracy: 0.9972 - val_top_6_categorical_accuracy: 0.9990 - val_top_7_categorical_accuracy: 0.9998 - val_top_8_categorical_accuracy: 1.0000 - val_top_9_categorical_accuracy: 1.0000 - val_precision_1: 0.9245 - val_recall_1: 0.8990 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-10 06:32:30.167044: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 36126720 exceeds 10% of free system memory.\n",
      "2022-02-10 06:32:30.250268: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 36126720 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/150\n",
      "900/900 [==============================] - 2216s 2s/step - loss: 0.2881 - top_1_categorical_accuracy: 0.9079 - top_2_categorical_accuracy: 0.9698 - top_3_categorical_accuracy: 0.9855 - top_4_categorical_accuracy: 0.9926 - top_5_categorical_accuracy: 0.9963 - top_6_categorical_accuracy: 0.9982 - top_7_categorical_accuracy: 0.9990 - top_8_categorical_accuracy: 0.9996 - top_9_categorical_accuracy: 0.9998 - precision_1: 0.9243 - recall_1: 0.8935 - accuracy: 0.0000e+00 - val_loss: 0.2676 - val_top_1_categorical_accuracy: 0.9178 - val_top_2_categorical_accuracy: 0.9728 - val_top_3_categorical_accuracy: 0.9874 - val_top_4_categorical_accuracy: 0.9952 - val_top_5_categorical_accuracy: 0.9980 - val_top_6_categorical_accuracy: 0.9990 - val_top_7_categorical_accuracy: 0.9994 - val_top_8_categorical_accuracy: 0.9996 - val_top_9_categorical_accuracy: 1.0000 - val_precision_1: 0.9286 - val_recall_1: 0.9084 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/150\n",
      " 26/900 [..............................] - ETA: 32:20 - loss: 0.2470 - top_1_categorical_accuracy: 0.9262 - top_2_categorical_accuracy: 0.9800 - top_3_categorical_accuracy: 0.9931 - top_4_categorical_accuracy: 0.9969 - top_5_categorical_accuracy: 0.9977 - top_6_categorical_accuracy: 0.9985 - top_7_categorical_accuracy: 0.9985 - top_8_categorical_accuracy: 1.0000 - top_9_categorical_accuracy: 1.0000 - precision_1: 0.9401 - recall_1: 0.9054 - accuracy: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_103/2842298412.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = final_model.fit(x=x_train,  \n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train_ohe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "history = final_model.fit(x=x_train,  \n",
    "                    y=y_train_ohe, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=150,      \n",
    "                    verbose='auto', \n",
    "                    callbacks=[tf.keras.callbacks.\\\n",
    "                               EarlyStopping(monitor='val_loss',\n",
    "                                             patience=PATIENCE,\n",
    "                                             min_delta=PATIENCE_MIN_DELTA,\n",
    "                                             restore_best_weights=True),\n",
    "                            tensorboard_callback_final], \n",
    "                    validation_split=0.1,\n",
    "                    validation_data=None,\n",
    "                    shuffle=True,\n",
    "                    class_weight=None, \n",
    "                    sample_weight=None, \n",
    "                    initial_epoch=0, \n",
    "                    steps_per_epoch=None, \n",
    "                    validation_steps=None, \n",
    "                    validation_batch_size=50, \n",
    "                    validation_freq=1, \n",
    "                    max_queue_size=10, \n",
    "                    workers=5, \n",
    "                    use_multiprocessing=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910feae8",
   "metadata": {},
   "source": [
    "## Stop training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc4bd508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 439s 1s/step - loss: 0.2714 - top_1_categorical_accuracy: 0.9167 - top_2_categorical_accuracy: 0.9710 - top_3_categorical_accuracy: 0.9878 - top_4_categorical_accuracy: 0.9949 - top_5_categorical_accuracy: 0.9980 - top_6_categorical_accuracy: 0.9992 - top_7_categorical_accuracy: 0.9995 - top_8_categorical_accuracy: 0.9996 - top_9_categorical_accuracy: 1.0000 - precision_1: 0.9278 - recall_1: 0.9079 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2713839113712311,\n",
       " 0.916700005531311,\n",
       " 0.9710000157356262,\n",
       " 0.9878000020980835,\n",
       " 0.9948999881744385,\n",
       " 0.9980000257492065,\n",
       " 0.9991999864578247,\n",
       " 0.9994999766349792,\n",
       " 0.9995999932289124,\n",
       " 1.0,\n",
       " 0.927848756313324,\n",
       " 0.9078999757766724,\n",
       " 0.0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.evaluate(x_test,y_test_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc52d6c1",
   "metadata": {},
   "source": [
    "## top-1 test set categorical accuracy # 0.9167"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6082580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-10 07:23:28.949034: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: 2022-02-09_manually_trained_exported_final_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    }
   ],
   "source": [
    "final_model.save('2022-02-09_manually_trained_exported_final_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe3a630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
