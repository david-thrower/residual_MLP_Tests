{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a70b12f",
   "metadata": {},
   "source": [
    "# Best Model Evaluation: Cold start EfficientNetB7-ResidualMLP Neural Architecture Search using hyperband (on CIFAR10 dataset)\n",
    "## Ran task.py with default settings on 2022-02-04\n",
    "## Here, we evaluate the best model from 12 hours of training / search\n",
    "## The first several cells are re-creating the environment from the run. The first 2 are displaying the NAS task (task.py) which was run and the content of my Python package residualmlp that was used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "947f7591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\r\n",
      "import subprocess\r\n",
      "import argparse\r\n",
      "# Add parser for common params\r\n",
      "\r\n",
      "try:\r\n",
      "    import pendulum\r\n",
      "except ModuleNotFoundError as not_found:\r\n",
      "    print(\"We're missing a module:\")\r\n",
      "    print(not_found)\r\n",
      "    print(\"No problem, we install it...\")\r\n",
      "    subprocess.run(\"pip3 install pendulum\", \r\n",
      "    \t           shell=True, \r\n",
      "    \t           check=True)\r\n",
      "    import pendulum\r\n",
      "\r\n",
      "try:\r\n",
      "    import pandas as pd\r\n",
      "except ModuleNotFoundError as not_found:\r\n",
      "    print(\"We're missing a module:\")\r\n",
      "    print(not_found)\r\n",
      "    print(\"No problem, we install it...\")\r\n",
      "    subprocess.run(\"pip3 install pandas\", \r\n",
      "    \t           shell=True, \r\n",
      "    \t           check=True)\r\n",
      "    import pandas as pd\r\n",
      "\r\n",
      "try:\r\n",
      "    import keras_tuner as kt\r\n",
      "except ModuleNotFoundError as not_found:\r\n",
      "    print(\"We're missing a module:\")\r\n",
      "    print(not_found)\r\n",
      "    print(\"No problem, we install it...\")\r\n",
      "    subprocess.run(\"pip3 install keras_tuner\", \r\n",
      "    \t           shell=True, \r\n",
      "    \t           check=True)\r\n",
      "    import keras_tuner as kt\r\n",
      "\r\n",
      "\r\n",
      "try:\r\n",
      "    import tensorflow as tf\r\n",
      "except ModuleNotFoundError as not_found:\r\n",
      "    print(\"We're missing a module:\")\r\n",
      "    print(not_found)\r\n",
      "    print(\"No problem, we install it...\")\r\n",
      "    subprocess.run(\"pip3 install tensorflow\", \r\n",
      "    \t           shell=True, \r\n",
      "    \t           check=True)\r\n",
      "    import tensorflow as tf\r\n",
      "\r\n",
      "from residualmlp.residual_mlp import ResidualMLP\r\n",
      "\r\n",
      "parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "    \"--project_name\", type=str, \r\n",
      "    help=\"Name for this project.\",\r\n",
      "    default=\"CIFAR10_EfficientNetB7-ResidualMLP_NAS_AUGMENTED_SPACE\")\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "    \"--training_set_size\", \r\n",
      "    type=int,\r\n",
      "    help=\"Training set size (how many observations).\",\r\n",
      "    default=50000)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "    \"--patience\",\r\n",
      "    type=int,\r\n",
      "    help=\"How many epochs with no improved performance before the \"\r\n",
      "        \"early stopping callback stops further training?\",\r\n",
      "    default=25)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "    \"--patience_min_delta\",\r\n",
      "    type=float,\r\n",
      "    help=\"How sensitive should the early stopping callback be\"\r\n",
      "        \"  to change?\",\r\n",
      "    default=0.00001)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "    \"--batch_size\",\r\n",
      "    type=int,\r\n",
      "    help=\"How many observations to train with...\",\r\n",
      "    default=50)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "    \"--max_epochs\",\r\n",
      "    type=int,\r\n",
      "    help=\"max_epochs: Integer, the maximum number of epochs to train one \"\r\n",
      "         \"model. It is recommended to set this to a value slightly higher \"\r\n",
      "         \"than the expected epochs to convergence for your largest Model, \"\r\n",
      "         \"and to use early stopping during training (for example, via .\",\r\n",
      "         default=50)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--minimum_learning_rate\",\r\n",
      "        type=float,\r\n",
      "        help=\"Lowest learning rate to try?\",\r\n",
      "        default = 0.00007)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--maximum_learning_rate\",\r\n",
      "        type=float,\r\n",
      "        help=\"Highest learning rate to try?\",\r\n",
      "        default = 0.7)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--number_of_learning_rates_to_try\",\r\n",
      "        type=int,\r\n",
      "        help=\"How many learning rate to try (maximum)?\",\r\n",
      "        default = 7)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--minimum_number_of_blocks\",\r\n",
      "        type=int,\r\n",
      "        help=\"Minimum number of ResidualMLP blocks in neural architectures \"\r\n",
      "             \"to try?\",\r\n",
      "        default = 1)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--maximum_number_of_blocks\",\r\n",
      "        type=int,\r\n",
      "        help=\"Maximum number of ResidualMLP blocks in neural architectures \"\r\n",
      "              \"to try?\",\r\n",
      "        default = 8)\r\n",
      "\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--minimum_number_of_layers_per_block\",\r\n",
      "        type=int,\r\n",
      "        help=\"Minimum number of layers to try in each ResidualMLP block in \"\r\n",
      "             \"the neural architectures to try?\",\r\n",
      "        default = 1)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--maximum_number_of_layers_per_block\",\r\n",
      "        type=int,\r\n",
      "        help=\"Maximum number of layers to try in each ResidualMLP block in \"\r\n",
      "             \"the neural architectures to try?\",\r\n",
      "        default = 8)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--minimum_neurons_per_block_layer\",\r\n",
      "        type=int,\r\n",
      "        help=\"Minimum number of neurons to try in the Dense layers in \"\r\n",
      "             \"each ResidualMLP block in the neural architectures to tried?\",\r\n",
      "        default = 30)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--maximum_neurons_per_block_layer\",\r\n",
      "        type=int,\r\n",
      "        help=\"Maximum number of neurons to try in the Dense layers in \"\r\n",
      "             \"each ResidualMLP block in the neural architectures to tried?\",\r\n",
      "        default = 130)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--n_options_of_neurons_per_layer_to_try\",\r\n",
      "        type=int,\r\n",
      "        help=\"How many different numbers of neurons (at most) to try in \"\r\n",
      "             \"the Dense layers try in each ResidualMLP block in the neural \"\r\n",
      "             \"architectures to tried?\",\r\n",
      "        default = 7)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--minimum_neurons_per_block_layer_decay\",\r\n",
      "        type=int,\r\n",
      "        help=\"Lowest decay in number of neurons (n less neurons than the \"\r\n",
      "             \"last layer) to try in the Dense layers try in each \"\r\n",
      "             \"ResidualMLP block in the neural \"\r\n",
      "             \"architectures to tried?\",\r\n",
      "        default = 0)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--maximum_neurons_per_block_layer_decay\",\r\n",
      "        type=int,\r\n",
      "        help=\"Highest decay in number of neurons (n less neurons than the \"\r\n",
      "             \"last layer) to try in the Dense layers try in each \"\r\n",
      "             \"ResidualMLP block in the neural \"\r\n",
      "             \"architectures to tried?\",\r\n",
      "        default = 50)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--minimum_dropout_rate_for_bypass_layers\",\r\n",
      "        type=float,\r\n",
      "        help=\"Lowest dropout rate to try for dropout layers located in \"\r\n",
      "             \"residual byass layers?\",\r\n",
      "        default = 0.01)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--maximim_dropout_rate_for_bypass_layers\",\r\n",
      "        type=float,\r\n",
      "        help=\"Highest dropout rate to try for dropout layers located in \"\r\n",
      "             \"residual byass layers?\",\r\n",
      "        default = 0.7)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--n_options_dropout_rate_for_bypass_layers\",\r\n",
      "        type=int,\r\n",
      "        help=\"How many dropout rates to try for dropout layers located in \"\r\n",
      "             \"residual byass layers?\",\r\n",
      "        default = 7)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--minimum_inter_block_layers_per_block\",\r\n",
      "        type=int,\r\n",
      "        help=\"Minimum number of neurons per Dense layer for Dense layers \"\r\n",
      "             \"inserted between ResidualMLP blocks? 0 Will not insert a \"\r\n",
      "             \"layer if selected.\",\r\n",
      "        default = 0)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--maximum_inter_block_layers_per_block\",\r\n",
      "        type=int,\r\n",
      "        help=\"Maximum number of neurons per Dense layer for Dense layers \"\r\n",
      "             \"inserted between ResidualMLP blocks? 0 Will not insert a \"\r\n",
      "             \"layer if selected.\",\r\n",
      "        default = 150)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--n_options_inter_block_layers_per_block\",\r\n",
      "        type=int,\r\n",
      "        help=\"How many different numbers of neurons per Dense layer for \"\r\n",
      "             \"Dense layers inserted between ResidualMLP blocks will we try \"\r\n",
      "             \"(at most)?\",\r\n",
      "        default = 7)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--minimum_dropout_rate\",\r\n",
      "        type=float,\r\n",
      "        help=\"Minimum dropout rate for final Dense layers?\",\r\n",
      "        default = 0.01)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--maximum_dropout_rate\",\r\n",
      "        type=float,\r\n",
      "        help=\"Maximum dropout rate for final Dense layers?\",\r\n",
      "        default = 0.7)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--n_options_dropout_rate\",\r\n",
      "        type=int,\r\n",
      "        help=\"How many dropout rates (at max) to try for final Dense layers?\",\r\n",
      "        default = 7)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--minimum_final_dense_layers\",\r\n",
      "        type=int,\r\n",
      "        help=\"Lowest number of neurons to try for final Dense layers, after \"\r\n",
      "             \"the last ResidualMLP block, before the very last Dense layer \"\r\n",
      "             \"returning an output? \"\r\n",
      "             \"(0 doesn't create a layer')\",\r\n",
      "        default = 0)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--maximum_final_dense_layers\",\r\n",
      "        type=int,\r\n",
      "        help=\"Highest number of neurons to try for final Dense layers, after \"\r\n",
      "             \"the last ResidualMLP block, before the very last Dense layer \"\r\n",
      "             \"returning an output? \"\r\n",
      "             \"(0 doesn't create a layer')\",\r\n",
      "        default = 150)\r\n",
      "\r\n",
      "parser.add_argument(\r\n",
      "        \"--n_options_final_dense_layers\",\r\n",
      "        type=int,\r\n",
      "        help=\"How many options for neurons to try for final Dense layers, \"\r\n",
      "             \"after the last ResidualMLP block, before the very last \"\r\n",
      "             \"Dense layer returning an output?\",\r\n",
      "        default = 7)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "args, _ = parser.parse_known_args()\r\n",
      "hparams = args.__dict__\r\n",
      "\r\n",
      "# Boilerplate args\r\n",
      "\r\n",
      "DATE = pendulum.now().__str__()[:16].replace(\"T\",\"_\").replace(\":\",\"_\")\r\n",
      "PROJECT_NAME = hparams[\"project_name\"]\r\n",
      "TRAINING_SET_SIZE = hparams[\"training_set_size\"]\r\n",
      "\r\n",
      "# Keras tuner search & fit args\r\n",
      "\r\n",
      "PATIENCE = hparams[\"patience\"]\r\n",
      "PATIENCE_MIN_DELTA = hparams[\"patience_min_delta\"]\r\n",
      "BATCH_SIZE = hparams[\"batch_size\"]\r\n",
      "MAX_EPOCHS = hparams[\"max_epochs\"]\r\n",
      "RESULTS_DIR_FOR_SEARCH =\\\r\n",
      "    f'{DATE}_{PROJECT_NAME}_SEARCH_RUN'\r\n",
      "\r\n",
      "\r\n",
      "# Base model args\r\n",
      "\r\n",
      "BASE_MODEL_INPUT_SHAPE = (600,600,3)\r\n",
      "\r\n",
      "# ResidualMLP model args\r\n",
      "\r\n",
      "MINIMUM_LEARNING_RATE = hparams[\"minimum_learning_rate\"]\r\n",
      "MAXIMUM_LEARNING_RATE = hparams[\"maximum_learning_rate\"]\r\n",
      "NUMBER_OF_LEARNING_RATES_TO_TRY = hparams[\"number_of_learning_rates_to_try\"]\r\n",
      "MINIMUM_NUMBER_OF_BLOCKS = hparams[\"minimum_number_of_blocks\"]\r\n",
      "MAXIMUM_NUMBER_OF_BLOCKS = hparams[\"maximum_number_of_blocks\"]\r\n",
      "MINIMUM_NUMBER_OF_LAYERS_PER_BLOCK =\\\r\n",
      "    hparams[\"minimum_number_of_layers_per_block\"]\r\n",
      "MAXIMUM_NUMBER_OF_LAYERS_PER_BLOCK =\\\r\n",
      "    hparams[\"maximum_number_of_layers_per_block\"]\r\n",
      "MINIMUM_NEURONS_PER_BLOCK_LAYER = hparams[\"minimum_neurons_per_block_layer\"]\r\n",
      "MAXIMUM_NEURONS_PER_BLOCK_LAYER = hparams[\"maximum_neurons_per_block_layer\"]\r\n",
      "N_OPTIONS_OF_NEURONS_PER_LAYER_TO_TRY =\\\r\n",
      "    hparams[\"n_options_of_neurons_per_layer_to_try\"]\r\n",
      "MINIMUM_NEURONS_PER_BLOCK_LAYER_DECAY =\\\r\n",
      "    hparams[\"minimum_neurons_per_block_layer_decay\"]\r\n",
      "MAXIMUM_NEURONS_PER_BLOCK_LAYER_DECAY =\\\r\n",
      "    hparams[\"maximum_neurons_per_block_layer_decay\"]\r\n",
      "MINIMUM_DROPOUT_RATE_FOR_BYPASS_LAYERS =\\\r\n",
      "    hparams[\"minimum_dropout_rate_for_bypass_layers\"]\r\n",
      "MAXIMIM_DROPOUT_RATE_FOR_BYPASS_LAYERS =\\\r\n",
      "    hparams[\"maximim_dropout_rate_for_bypass_layers\"]\r\n",
      "N_OPTIONS_DROPOUT_RATE_FOR_BYPASS_LAYERS =\\\r\n",
      "    hparams[\"n_options_dropout_rate_for_bypass_layers\"]\r\n",
      "MINIMUM_INTER_BLOCK_LAYERS_PER_BLOCK =\\\r\n",
      "    hparams[\"minimum_inter_block_layers_per_block\"]\r\n",
      "MAXIMUM_INTER_BLOCK_LAYERS_PER_BLOCK =\\\r\n",
      "    hparams[\"maximum_inter_block_layers_per_block\"]\r\n",
      "N_OPTIONS_INTER_BLOCK_LAYERS_PER_BLOCK =\\\r\n",
      "    hparams[\"n_options_inter_block_layers_per_block\"]\r\n",
      "MINIMUM_DROPOUT_RATE = hparams[\"minimum_dropout_rate\"]\r\n",
      "MAXIMUM_DROPOUT_RATE = hparams[\"maximum_dropout_rate\"]\r\n",
      "N_OPTIONS_DROPOUT_RATE = hparams[\"n_options_dropout_rate\"]\r\n",
      "MINIMUM_FINAL_DENSE_LAYERS = hparams[\"minimum_final_dense_layers\"]\r\n",
      "MAXIMUM_FINAL_DENSE_LAYERS = hparams[\"maximum_final_dense_layers\"]\r\n",
      "N_OPTIONS_FINAL_DENSE_LAYERS = hparams[\"n_options_final_dense_layers\"]\r\n",
      "\r\n",
      "# CIFAR10_EfficientNetB7-ResidualMLP_NAS\r\n",
      "\r\n",
      "RESULTS_DIR_FOR_FINAL_MODEL =\\\r\n",
      "    f'{DATE}_{PROJECT_NAME}_FINAL_MODEL'\r\n",
      "\r\n",
      "FINAL_MODEL_PATH =\\\r\n",
      "    f\"{DATE}_{PROJECT_NAME}_FINAL_EXPORTED_MODEL\"\r\n",
      "\r\n",
      "# Print header info to shell script logs:\r\n",
      "\r\n",
      "print(f\"Date: {DATE}\")\r\n",
      "print(f\"project_name: {PROJECT_NAME}\")\r\n",
      "print(f\"training_set_size: {TRAINING_SET_SIZE}\")\r\n",
      "print(f\"patience: {PATIENCE}\")\r\n",
      "print(f\"PATIENCE_MIN_DELTA: {PATIENCE_MIN_DELTA}\")\r\n",
      "print(f\"BATCH_SIZE: {BATCH_SIZE}\")\r\n",
      "print(f\"MAX_EPOCHS: {MAX_EPOCHS}\")\r\n",
      "print(f\"RESULTS_DIR_FOR_SEARCH: {RESULTS_DIR_FOR_SEARCH}\")\r\n",
      "print(f\"MINIMUM_LEARNING_RATE {MINIMUM_LEARNING_RATE}\")\r\n",
      "print(f\"MAXIMUM_LEARNING_RATE: {MAXIMUM_LEARNING_RATE}\")\r\n",
      "print(f\"NUMBER_OF_LEARNING_RATES_TO_TRY: {NUMBER_OF_LEARNING_RATES_TO_TRY}\")\r\n",
      "print(f\"MINIMUM_NUMBER_OF_BLOCKS: {MINIMUM_NUMBER_OF_BLOCKS}\")\r\n",
      "print(f\"MAXIMUM_NUMBER_OF_BLOCKS: {MAXIMUM_NUMBER_OF_BLOCKS}\")\r\n",
      "print(\"MINIMUM_NUMBER_OF_LAYERS_PER_BLOCK: \"\r\n",
      "      f\"{MINIMUM_NUMBER_OF_LAYERS_PER_BLOCK}\")\r\n",
      "print(\"MAXIMUM_NUMBER_OF_LAYERS_PER_BLOCK: \"\r\n",
      "      f\"{MAXIMUM_NUMBER_OF_LAYERS_PER_BLOCK}\")\r\n",
      "print(f\"MINIMUM_NEURONS_PER_BLOCK_LAYER: {MINIMUM_NEURONS_PER_BLOCK_LAYER}\")\r\n",
      "print(f\"MAXIMUM_NEURONS_PER_BLOCK_LAYER: {MAXIMUM_NEURONS_PER_BLOCK_LAYER}\")\r\n",
      "print(\"N_OPTIONS_OF_NEURONS_PER_LAYER_TO_TRY: \"\r\n",
      "      f\"{N_OPTIONS_OF_NEURONS_PER_LAYER_TO_TRY}\")\r\n",
      "print(\"MINIMUM_NEURONS_PER_BLOCK_LAYER_DECAY: \"\r\n",
      "      f\"{MINIMUM_NEURONS_PER_BLOCK_LAYER_DECAY}\")\r\n",
      "print(\"MAXIMUM_NEURONS_PER_BLOCK_LAYER_DECAY: \"\r\n",
      "      f\"{MAXIMUM_NEURONS_PER_BLOCK_LAYER_DECAY}\")\r\n",
      "print(\"MINIMUM_DROPOUT_RATE_FOR_BYPASS_LAYERS: \"\r\n",
      "      f\"{MINIMUM_DROPOUT_RATE_FOR_BYPASS_LAYERS}\")\r\n",
      "print(\"MAXIMIM_DROPOUT_RATE_FOR_BYPASS_LAYERS: \"\r\n",
      "      f\"{MAXIMIM_DROPOUT_RATE_FOR_BYPASS_LAYERS}\")\r\n",
      "print(\"N_OPTIONS_DROPOUT_RATE_FOR_BYPASS_LAYERS: \"\r\n",
      "      f\"{N_OPTIONS_DROPOUT_RATE_FOR_BYPASS_LAYERS}\")\r\n",
      "print(\"MINIMUM_INTER_BLOCK_LAYERS_PER_BLOCK: \"\r\n",
      "      f\"{MINIMUM_INTER_BLOCK_LAYERS_PER_BLOCK}\")\r\n",
      "print(\"MAXIMUM_INTER_BLOCK_LAYERS_PER_BLOCK: \"\r\n",
      "      f\"{MAXIMUM_INTER_BLOCK_LAYERS_PER_BLOCK}\")\r\n",
      "print(\"N_OPTIONS_INTER_BLOCK_LAYERS_PER_BLOCK: \"\r\n",
      "      f\"{N_OPTIONS_INTER_BLOCK_LAYERS_PER_BLOCK}\")\r\n",
      "print(f\"MINIMUM_DROPOUT_RATE: {MINIMUM_DROPOUT_RATE}\")\r\n",
      "print(f\"MAXIMUM_DROPOUT_RATE: {MAXIMUM_DROPOUT_RATE}\")\r\n",
      "print(f\"N_OPTIONS_DROPOUT_RATE: {N_OPTIONS_DROPOUT_RATE}\")\r\n",
      "print(f\"MINIMUM_FINAL_DENSE_LAYERS: {MINIMUM_FINAL_DENSE_LAYERS}\")\r\n",
      "print(f\"MAXIMUM_FINAL_DENSE_LAYERS: {MAXIMUM_FINAL_DENSE_LAYERS}\")\r\n",
      "print(f\"N_OPTIONS_FINAL_DENSE_LAYERS: {N_OPTIONS_FINAL_DENSE_LAYERS}\")\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "    cifar = tf.keras.datasets.cifar10.load_data()\r\n",
      "    (x_train, y_train), (x_test, y_test) = cifar\r\n",
      "    \r\n",
      "    y_train_ohe = tf.one_hot([i[0] for i in  y_train],10)\r\n",
      "    indexes_for_rows = tf.range(0,y_train.shape[0])\r\n",
      "    shuffled_indexes = tf.random.shuffle(indexes_for_rows)\r\n",
      "    selected_indexes = shuffled_indexes[:TRAINING_SET_SIZE]\r\n",
      "    selected_x_train = x_train[selected_indexes,:,:,:]\r\n",
      "    selected_y_train_ohe = y_train_ohe.numpy()[selected_indexes,:]\r\n",
      "    \r\n",
      "    \r\n",
      "    \r\n",
      "    \r\n",
      "    mod_with_fc_raw = tf.keras.applications.efficientnet.EfficientNetB7(\r\n",
      "        include_top=True, weights='imagenet', input_tensor=None,\r\n",
      "        input_shape = BASE_MODEL_INPUT_SHAPE, pooling='max', classes=1000\r\n",
      "    )\r\n",
      "    \r\n",
      "    # Make the deepest conv2d layer trainable, leave everything else\r\n",
      "    # as not trainable\r\n",
      "    for layer in mod_with_fc_raw.layers:\r\n",
      "        layer.trainable = False\r\n",
      "    # Last conv2d layer. This we want to train .\r\n",
      "    mod_with_fc_raw.layers[-6].trainable = True\r\n",
      "    \r\n",
      "    # Create the final base model\r\n",
      "    # (remove the final Dense and BatchNormalization layers ...) \r\n",
      "    efficient_net_b_7_transferable_base_model =\\\r\n",
      "        tf.keras.Model(inputs=mod_with_fc_raw.layers[0].input, \r\n",
      "                        outputs=mod_with_fc_raw.layers[-3].output)\r\n",
      "    \r\n",
      "    model_builder = ResidualMLP(\r\n",
      "                        problem_type= \"classification\",  \r\n",
      "                        minimum_learning_rate = MINIMUM_LEARNING_RATE, \r\n",
      "                        maximum_learning_rate = MAXIMUM_LEARNING_RATE, \r\n",
      "                        number_of_learning_rates_to_try =\r\n",
      "                            NUMBER_OF_LEARNING_RATES_TO_TRY, \r\n",
      "                        input_shape = (32,32,3), \r\n",
      "                        bw_images = False, \r\n",
      "                        base_model = \r\n",
      "                            efficient_net_b_7_transferable_base_model, \r\n",
      "                        base_model_input_shape = BASE_MODEL_INPUT_SHAPE, \r\n",
      "                        flatten_after_base_model = False, \r\n",
      "                        minimum_number_of_blocks = MINIMUM_NUMBER_OF_BLOCKS, \r\n",
      "                        maximum_number_of_blocks = MAXIMUM_NUMBER_OF_BLOCKS, \r\n",
      "                        minimum_number_of_layers_per_block =\r\n",
      "                            MINIMUM_NUMBER_OF_LAYERS_PER_BLOCK, \r\n",
      "                        maximum_number_of_layers_per_block =\r\n",
      "                            MAXIMUM_NUMBER_OF_LAYERS_PER_BLOCK,\r\n",
      "                        minimum_neurons_per_block_layer =\r\n",
      "                            MINIMUM_NEURONS_PER_BLOCK_LAYER, \r\n",
      "                        maximum_neurons_per_block_layer =\r\n",
      "                            MAXIMUM_NEURONS_PER_BLOCK_LAYER, \r\n",
      "                        n_options_of_neurons_per_layer_to_try =\r\n",
      "                            N_OPTIONS_OF_NEURONS_PER_LAYER_TO_TRY, \r\n",
      "                        minimum_neurons_per_block_layer_decay =\r\n",
      "                            MINIMUM_NEURONS_PER_BLOCK_LAYER_DECAY, \r\n",
      "                        maximum_neurons_per_block_layer_decay = \r\n",
      "                            MAXIMUM_NEURONS_PER_BLOCK_LAYER_DECAY, \r\n",
      "                        minimum_dropout_rate_for_bypass_layers =\r\n",
      "                            MINIMUM_DROPOUT_RATE_FOR_BYPASS_LAYERS, \r\n",
      "                        maximim_dropout_rate_for_bypass_layers =\r\n",
      "                            MAXIMIM_DROPOUT_RATE_FOR_BYPASS_LAYERS, \r\n",
      "                        n_options_dropout_rate_for_bypass_layers =\r\n",
      "                            N_OPTIONS_DROPOUT_RATE_FOR_BYPASS_LAYERS,\r\n",
      "                        minimum_inter_block_layers_per_block =\r\n",
      "                            MINIMUM_INTER_BLOCK_LAYERS_PER_BLOCK, \r\n",
      "                        maximum_inter_block_layers_per_block =\r\n",
      "                            MAXIMUM_INTER_BLOCK_LAYERS_PER_BLOCK,\r\n",
      "                        n_options_inter_block_layers_per_block =\\\r\n",
      "                            N_OPTIONS_INTER_BLOCK_LAYERS_PER_BLOCK,\r\n",
      "                        minimum_dropout_rate = MINIMUM_DROPOUT_RATE, \r\n",
      "                        maximum_dropout_rate = MAXIMUM_DROPOUT_RATE,\r\n",
      "                        n_options_dropout_rate = N_OPTIONS_DROPOUT_RATE, \r\n",
      "                        minimum_final_dense_layers =\r\n",
      "                            MINIMUM_FINAL_DENSE_LAYERS,\r\n",
      "                        maximum_final_dense_layers =\r\n",
      "                            MAXIMUM_FINAL_DENSE_LAYERS, \r\n",
      "                        n_options_final_dense_layers =\r\n",
      "                            N_OPTIONS_FINAL_DENSE_LAYERS, \r\n",
      "                        number_of_classes = 10,\r\n",
      "                        final_activation = tf.keras.activations.softmax)\r\n",
      "    \r\n",
      "    \r\n",
      "    \r\n",
      "    logdir_for_search = os.path.join(\"logs\", RESULTS_DIR_FOR_SEARCH + \"_TB\")\r\n",
      "    tensorboard_callback_search =\\\r\n",
      "        tf.keras.callbacks.TensorBoard(logdir_for_search, histogram_freq=1)\r\n",
      "    \r\n",
      "    tuner = kt.Hyperband(\r\n",
      "        model_builder.build_auto_residual_mlp,\r\n",
      "        objective='val_loss',\r\n",
      "        max_epochs = MAX_EPOCHS,\r\n",
      "        hyperband_iterations = 2)\r\n",
      "    \r\n",
      "    \r\n",
      "    tuner.search(x=selected_x_train,  \r\n",
      "                 y=selected_y_train_ohe,\r\n",
      "                 epochs=MAX_EPOCHS,\r\n",
      "                 batch_size=BATCH_SIZE, \r\n",
      "                 callbacks=[\r\n",
      "                        tf.keras.callbacks.EarlyStopping(\r\n",
      "                            monitor=\"val_loss\",\r\n",
      "                            patience=PATIENCE,\r\n",
      "                            min_delta=PATIENCE_MIN_DELTA,\r\n",
      "                            restore_best_weights=True,\r\n",
      "                        ),\r\n",
      "                        tensorboard_callback_search,\r\n",
      "                    ],\r\n",
      "                 validation_split=0.3)\r\n",
      "    \r\n",
      "    print(\"These are the best params and results:\")\r\n",
      "    tuner.results_summary(num_trials=10)\r\n",
      "    \r\n",
      "    # final_model = tuner.get_best_models(num_models=1)[0]\r\n",
      "    \r\n",
      "    best_hp = tuner.get_best_hyperparameters()[0]\r\n",
      "    final_model = tuner.hypermodel.build(best_hp)\r\n",
      "\r\n",
      "    \r\n",
      "    \r\n",
      "    logdir_final_model = os.path.join(\"logs\",\r\n",
      "                                      RESULTS_DIR_FOR_FINAL_MODEL + \"_TB\")\r\n",
      "    tensorboard_callback_final =\\\r\n",
      "        tf.keras.callbacks.TensorBoard(logdir_final_model, histogram_freq=1)\r\n",
      "    \r\n",
      "    history = final_model.fit(x=selected_x_train,  \r\n",
      "                        y=selected_y_train_ohe, \r\n",
      "                        batch_size=BATCH_SIZE, \r\n",
      "                        epochs=150,      \r\n",
      "                        verbose='auto', \r\n",
      "                        callbacks=[tf.keras.callbacks.\\\r\n",
      "                                   EarlyStopping(monitor='val_loss',\r\n",
      "                                                 patience=PATIENCE,\r\n",
      "                                                 min_delta=PATIENCE_MIN_DELTA,\r\n",
      "                                                 restore_best_weights=True),\r\n",
      "                                tensorboard_callback_final], \r\n",
      "                        validation_split=0.3,\r\n",
      "                        validation_data=None,\r\n",
      "                        shuffle=True,\r\n",
      "                        class_weight=None, \r\n",
      "                        sample_weight=None, \r\n",
      "                        initial_epoch=0, \r\n",
      "                        steps_per_epoch=None, \r\n",
      "                        validation_steps=None, \r\n",
      "                        validation_batch_size=10, \r\n",
      "                        validation_freq=1, \r\n",
      "                        max_queue_size=10, \r\n",
      "                        workers=5, \r\n",
      "                        use_multiprocessing=True)\r\n",
      "    \r\n",
      "    \r\n",
      "    \r\n",
      "    hy = pd.DataFrame(history.history)\r\n",
      "    hy.to_csv(f'{DATE}_test_history.csv')\r\n",
      "    hy.to_json(f'{DATE}_test_history.json')\r\n",
      "    \r\n",
      "    final_model.save(FINAL_MODEL_PATH)\r\n",
      "    print(\"Successful Run!\")\r\n"
     ]
    }
   ],
   "source": [
    "! cat task.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05ce3db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try:\r\n",
      "    import keras_tuner as kt\r\n",
      "except Exception as exc:\r\n",
      "    print(\"Importing Keras tuner appears to be unsuccesful. \"\r\n",
      "          \"keras tuner may need to be installed $ pip install -q -U \"\r\n",
      "          \"keras-tuner. The auto-ml features are disabled until this is \"\r\n",
      "           \"fixed, but ResidualMLP will work. A more detailed error is: \"\r\n",
      "           f\"{exc}\")\r\n",
      "import tensorflow as tf\r\n",
      "import numpy as np\r\n",
      "import pandas as pd\r\n",
      "import pendulum\r\n",
      "\r\n",
      "# Becomes a layer to convert BW images to RGB\r\n",
      "\r\n",
      "class ResidualMLP:\r\n",
      "    \r\n",
      "    def __init__(self, problem_type = 'classification',\r\n",
      "                      learning_rate = .0007,\r\n",
      "                      minimum_learning_rate = 0.00007,\r\n",
      "                      maximum_learning_rate = 0.7,\r\n",
      "                      number_of_learning_rates_to_try = 5,\r\n",
      "                      input_shape = (32,32,3),\r\n",
      "                      bw_images = False,\r\n",
      "                      base_model = '',\r\n",
      "                      base_model_input_shape = (600,600,3),\r\n",
      "                      base_model_hyperparameters = {},\r\n",
      "                      flatten_after_base_model = True,\r\n",
      "                          # 2D ARRAY: Each row: \r\n",
      "                          # [number_of_layers,\r\n",
      "                          #  first_layer_neurons, \r\n",
      "                          #  decay_of_n_Dense_units_per_layer]\r\n",
      "                      blocks = [[5,400,50]],\r\n",
      "                      minimum_number_of_blocks = 1,\r\n",
      "                      maximum_number_of_blocks = 7,\r\n",
      "                      minimum_number_of_layers_per_block = 1,\r\n",
      "                      maximum_number_of_layers_per_block = 7,\r\n",
      "                      minimum_neurons_per_block_layer = 3,\r\n",
      "                      maximum_neurons_per_block_layer = 30,\r\n",
      "                      n_options_of_neurons_per_layer_to_try = 7,\r\n",
      "                      minimum_neurons_per_block_layer_decay = 1,\r\n",
      "                      maximum_neurons_per_block_layer_decay = 7,\r\n",
      "                      residual_bypass_dense_layers = list(),\r\n",
      "                      b_norm_or_dropout_residual_bypass_layers = 'dropout',\r\n",
      "                      dropout_rate_for_bypass_layers = .35,\r\n",
      "                      minimum_dropout_rate_for_bypass_layers = 0.01,\r\n",
      "                      maximim_dropout_rate_for_bypass_layers = 0.7,\r\n",
      "                      n_options_dropout_rate_for_bypass_layers = 7,\r\n",
      "                      inter_block_layers_per_block = list(),\r\n",
      "                      minimum_inter_block_layers_per_block = 3,\r\n",
      "                      maximum_inter_block_layers_per_block = 30,\r\n",
      "                      n_options_inter_block_layers_per_block = 7,\r\n",
      "                      b_norm_or_dropout_last_layers = 'dropout', # | 'bnorm'\r\n",
      "                      dropout_rate = .2, #\r\n",
      "                      minimum_dropout_rate = 0.01,\r\n",
      "                      maximum_dropout_rate = 0.7,\r\n",
      "                      n_options_dropout_rate = 7,\r\n",
      "                      activation = tf.keras.activations.relu,\r\n",
      "                      final_dense_layers = [75,35],\r\n",
      "                      minimum_final_dense_layers = 0,\r\n",
      "                      maximum_final_dense_layers = 30,\r\n",
      "                      n_options_final_dense_layers = 2,\r\n",
      "                      number_of_classes = 10, # 1 if a regression problem\r\n",
      "                      final_activation = tf.keras.activations.softmax,\r\n",
      "                      loss = tf.keras.losses.CategoricalCrossentropy(\r\n",
      "                          from_logits=False)):\r\n",
      "        self.problem_type = problem_type\r\n",
      "        self.learning_rate = learning_rate\r\n",
      "        self.minimum_learning_rate = minimum_learning_rate\r\n",
      "        self.maximum_learning_rate = maximum_learning_rate\r\n",
      "        self.number_of_learning_rates_to_try = number_of_learning_rates_to_try\r\n",
      "        self.input_shape = input_shape\r\n",
      "        self.bw_images = bw_images\r\n",
      "        self.base_model = base_model\r\n",
      "        self.base_model_input_shape = base_model_input_shape\r\n",
      "        self.flatten_after_base_model = flatten_after_base_model\r\n",
      "        self.blocks = blocks\r\n",
      "        self.minimum_number_of_blocks = minimum_number_of_blocks\r\n",
      "        self.maximum_number_of_blocks = maximum_number_of_blocks\r\n",
      "        self.minimum_number_of_layers_per_block =\\\r\n",
      "            minimum_number_of_layers_per_block\r\n",
      "        self.maximum_number_of_layers_per_block =\\\r\n",
      "            maximum_number_of_layers_per_block\r\n",
      "        self.minimum_neurons_per_block_layer = minimum_neurons_per_block_layer\r\n",
      "        self.maximum_neurons_per_block_layer = maximum_neurons_per_block_layer\r\n",
      "        self.n_options_of_neurons_per_layer_to_try =\\\r\n",
      "            n_options_of_neurons_per_layer_to_try\r\n",
      "        self.minimum_neurons_per_block_layer_decay =\\\r\n",
      "            minimum_neurons_per_block_layer_decay\r\n",
      "        self.maximum_neurons_per_block_layer_decay =\\\r\n",
      "            maximum_neurons_per_block_layer_decay\r\n",
      "        # residual_bypass_dense_layers\r\n",
      "        # Screen for nonsense combinations of the parameters 'blocks' and \r\n",
      "        # 'residual_bypass_dense_layers'\r\n",
      "        if not isinstance(residual_bypass_dense_layers,list):\r\n",
      "            raise ValueError(\"The parameter residual_bypass_dense_layers \"\r\n",
      "                             \"should be one of these 2: 1. a 2d list, one 1d \"\r\n",
      "                             \"list of positive    integers for each list in \"\r\n",
      "                             \"blocks, or 2. an empty list.\")\r\n",
      "        if len(residual_bypass_dense_layers) != 0:\r\n",
      "            if len(blocks) != len(residual_bypass_dense_layers):\r\n",
      "                raise ValueError(\"The parameter 'blocks' and \"\r\n",
      "                                 \"'residual_bypass_dense_layers are 2d \"\r\n",
      "                                 \"arrays' must have the same number of 1d \"\r\n",
      "                                 \"arrays nested within them OR be aan empty \"\r\n",
      "                                 \"list or left default. To fix this error \"\r\n",
      "                                 \"do one of the following: Fix 1: Make them \" \r\n",
      "                                 \"the same number of 1d arrays, for example \"\r\n",
      "                                 \"blocks = [[5,20,2],[5,20,2],[5,20,2]] ...\"\r\n",
      "                                 \" (3 nested 1d arrays) \"\r\n",
      "                                 \"residual_bypass_dense_layers = \"\r\n",
      "                                 \"[[10,7],[],[10]] also 3. \"\r\n",
      "                                 \"the ith array nested in \"\r\n",
      "                                 \"residual_bypass_dense_layers will be \"\r\n",
      "                                 \"associated with the i_th block in blocks. \"\r\n",
      "                                 \" each j_th item in the i_th nested 1d \"\r\n",
      "                                 \"array will make one dense layer in the \"\r\n",
      "                                 \"tensor that bypasses the main block of \"\r\n",
      "                                 \"dense layers in the ith block in the \"\r\n",
      "                                 \"residual MLP. This is probably a bit \"\r\n",
      "                                 \"confusing to read. Please refer to the \"\r\n",
      "                                 \"tutorials and documentation. Note \"\r\n",
      "                                 \"the order I referred to the tutorials and \"\r\n",
      "                                 \"documentation in. Fix 2: leave \"\r\n",
      "                                 \"residual_bypass_dense_layers default / \"\r\n",
      "                                 \"set it to an empty list.\")\r\n",
      "            else:\r\n",
      "                self.residual_bypass_dense_layers =\\\r\n",
      "                    residual_bypass_dense_layers\r\n",
      "        else:\r\n",
      "            self.residual_bypass_dense_layers = [[] for block in blocks]\r\n",
      "        self.b_norm_or_dropout_residual_bypass_layers =\\\r\n",
      "            b_norm_or_dropout_residual_bypass_layers\r\n",
      "        self.dropout_rate_for_bypass_layers = dropout_rate_for_bypass_layers\r\n",
      "        self.minimum_dropout_rate_for_bypass_layers =\\\r\n",
      "            minimum_dropout_rate_for_bypass_layers\r\n",
      "        self.maximim_dropout_rate_for_bypass_layers =\\\r\n",
      "            maximim_dropout_rate_for_bypass_layers\r\n",
      "        self.n_options_dropout_rate_for_bypass_layers =\\\r\n",
      "            n_options_dropout_rate_for_bypass_layers\r\n",
      "        self.inter_block_layers_per_block = inter_block_layers_per_block\r\n",
      "        self.minimum_inter_block_layers_per_block =\\\r\n",
      "            minimum_inter_block_layers_per_block\r\n",
      "        self.maximum_inter_block_layers_per_block =\\\r\n",
      "            maximum_inter_block_layers_per_block\r\n",
      "        self.n_options_inter_block_layers_per_block =\\\r\n",
      "            n_options_inter_block_layers_per_block\r\n",
      "        self.b_norm_or_dropout_last_layers = b_norm_or_dropout_last_layers\r\n",
      "        self.dropout_rate  = dropout_rate\r\n",
      "        self.minimum_dropout_rate = minimum_dropout_rate\r\n",
      "        self.maximum_dropout_rate = maximum_dropout_rate\r\n",
      "        self.n_options_dropout_rate = n_options_dropout_rate\r\n",
      "        self.activation = activation\r\n",
      "        self.final_dense_layers = final_dense_layers\r\n",
      "        self.minimum_final_dense_layers = minimum_final_dense_layers\r\n",
      "        self.maximum_final_dense_layers = maximum_final_dense_layers\r\n",
      "        self.n_options_final_dense_layers = n_options_final_dense_layers\r\n",
      "        self.number_of_classes = number_of_classes\r\n",
      "        self.final_activation = final_activation\r\n",
      "        self.loss = loss\r\n",
      "        \r\n",
      "    \r\n",
      "    def grayscale_to_rgb(images, channel_axis=-1):\r\n",
      "        images= tf.expand_dims(images, axis=channel_axis)\r\n",
      "        tiling = [1] * 4    # 4 dimensions: B, H, W, C\r\n",
      "        tiling[channel_axis] *= 3\r\n",
      "        images= tf.tile(images, tiling)\r\n",
      "        im = tf.keras.preprocessing.image.smart_resize(images,(224,224))\r\n",
      "        return im\r\n",
      "\r\n",
      "\r\n",
      "    # builds and compiles a tandem model given these params and\r\n",
      "    # selected base model:\r\n",
      "    def make_tandem_model(self):\r\n",
      "        if self.problem_type == 'classification':\r\n",
      "            precision = tf.keras.metrics.Precision(),\r\n",
      "            recall = tf.keras.metrics.Recall()\r\n",
      "            accuracy = tf.keras.metrics.Accuracy()\r\n",
      "        if self.problem_type == 'classification' and\\\r\n",
      "                self.number_of_classes > 1:\r\n",
      "            metrics = [tf.keras.metrics.TopKCategoricalAccuracy(\r\n",
      "                k = k,\r\n",
      "                name=f'top_{k}_'\r\n",
      "                'categorical_'\r\n",
      "                'accuracy',\r\n",
      "                dtype=None)\r\n",
      "                           for k in np.arange(1,self.number_of_classes)\\\r\n",
      "                               if k < 10]\r\n",
      "            metrics.append(precision)\r\n",
      "            metrics.append(recall)\r\n",
      "            metrics.append(accuracy)\r\n",
      "        elif self.problem_type == 'classification' and\\\r\n",
      "                self.number_of_classes == 1:    \r\n",
      "            metrics = [precision, recall, accuracy]\r\n",
      "        else:\r\n",
      "            rmse = tf.keras.metrics.RootMeanSquaredError()\r\n",
      "            mae = tf.keras.metrics.MeanAbsoluteError()\r\n",
      "            metrics = [rmse, mae]\r\n",
      "    \r\n",
      "        inp = tf.keras.layers.Input(shape = self.input_shape) \r\n",
      "        # Start with input layer that fits. \r\n",
      "        # The keras fucntional API requires an explicit input layer\r\n",
      "        if self.bw_images:\r\n",
      "            x = self.grayscale_to_rgb(inp)\r\n",
      "        else:\r\n",
      "            x = inp\r\n",
      "        if self.base_model != '':\r\n",
      "            x = tf.keras.layers.Resizing(self.base_model_input_shape[0],\r\n",
      "                                         self.base_model_input_shape[1])(x)\r\n",
      "            x = self.base_model(x)\r\n",
      "        if self.flatten_after_base_model:\r\n",
      "            x = tf.keras.layers.Flatten()(x)\r\n",
      "        initializer = tf.keras.initializers.GlorotNormal()\r\n",
      "        for bl in np.arange(len(self.blocks)):\r\n",
      "            block = self.blocks[bl]\r\n",
      "            bypass_block = self.residual_bypass_dense_layers[bl]\r\n",
      "            \r\n",
      "            \r\n",
      "            x = tf.keras.layers.Dense(block[1],\r\n",
      "                                      self.activation,\r\n",
      "                                      kernel_initializer=initializer)(x)\r\n",
      "            y = x\r\n",
      "            x = tf.keras.layers.BatchNormalization()(x)\r\n",
      "            # x proceeds sequentially to the \r\n",
      "            # next Dense layer.\r\n",
      "            \r\n",
      "            if self.b_norm_or_dropout_residual_bypass_layers == 'dropout':\r\n",
      "                y = tf.keras.layers\\\r\n",
      "                    .Dropout(self.dropout_rate_for_bypass_layers)(y)\r\n",
      "            elif self.b_norm_or_dropout_residual_bypass_layers == 'bnorm':\r\n",
      "                y = tf.keras.layers.BatchNormalization()(y)\r\n",
      "            else:\r\n",
      "                raise ValueError(\"The parameter: \"\r\n",
      "                                 \"'b_norm_or_dropout_residual_bypass_\"\r\n",
      "                                 \"layers'\"\r\n",
      "                                 \" must be left default '', or be \"\r\n",
      "                                 \"'dropout' or may be 'bnorm'.\")\r\n",
      "            for bypass_layer in bypass_block:\r\n",
      "                y = tf.keras.layers.Dense(bypass_layer,\r\n",
      "                                          self.activation,\r\n",
      "                                          kernel_initializer=initializer)(y)\r\n",
      "                if self.b_norm_or_dropout_residual_bypass_layers == 'dropout':\r\n",
      "                    y = tf.keras.layers\\\r\n",
      "                        .Dropout(self.dropout_rate_for_bypass_layers)(y)\r\n",
      "                elif self.b_norm_or_dropout_residual_bypass_layers == 'bnorm':\r\n",
      "                    y = tf.keras.layers.BatchNormalization()(y)\r\n",
      "                else:\r\n",
      "                    raise ValueError(\"The parameter: \"\r\n",
      "                                     \"'b_norm_or_dropout_residual_bypass_\"\r\n",
      "                                     \"layers' must be left default '', or be \"\r\n",
      "                                     \"'dropout' or may be 'bnorm'.\")\r\n",
      "            # y does NOT proceed sequentially\r\n",
      "            # to the next layer. This bypasses \r\n",
      "            # several layers and give a memory \r\n",
      "            # that attenuates some of the \r\n",
      "            # deleterious effects of a deeper \r\n",
      "            # network and lets us capture more \r\n",
      "            # complex interactions before \r\n",
      "            # overfitting becomes an issue than \r\n",
      "            # the textbook sequential multi - \r\n",
      "            # layer perceptron ...\r\n",
      "            for j in np.arange(block[0]): \r\n",
      "                x = tf.keras.layers.Dense(block[1] - block[2] * j,\r\n",
      "                                          self.activation,\r\n",
      "                                          kernel_initializer=initializer)(x) \r\n",
      "                x = tf.keras.layers.BatchNormalization()(x)\r\n",
      "    \r\n",
      "            x = tf.keras.layers.Concatenate(axis=1)([x, y])\r\n",
      "            \r\n",
      "            if bl != np.arange(len(self.blocks)).max():\r\n",
      "                for inter_block_layer in self.inter_block_layers_per_block:\r\n",
      "                    x = tf.keras.layers.Dense(inter_block_layer,\r\n",
      "                                          self.activation,\r\n",
      "                                          kernel_initializer=initializer)(x)\r\n",
      "                    x = tf.keras.layers.BatchNormalization()(x)\r\n",
      "    \r\n",
      "        for i in self.final_dense_layers:\r\n",
      "            x = tf.keras.layers.Dense(i,\r\n",
      "                                      self.activation,\r\n",
      "                                      kernel_initializer=initializer)(x) \r\n",
      "            if self.b_norm_or_dropout_last_layers == 'bnorm':\r\n",
      "                x = tf.keras.layers.BatchNormalization()(x)\r\n",
      "            elif self.b_norm_or_dropout_last_layers == 'dropout':\r\n",
      "                x = tf.keras.layers.Dropout(self.dropout_rate)(x)\r\n",
      "            else:\r\n",
      "                raise ValueError(\"For b_norm_or_dropout_last_layers, \" \r\n",
      "                                 \"you must pick either 'dropout' or 'bnorm'\")\r\n",
      "        out = tf.keras.layers.Dense(self.number_of_classes,\r\n",
      "                                    self.final_activation,\r\n",
      "                                    kernel_initializer=initializer)(x)\r\n",
      "    \r\n",
      "        # Declare the graph for our model ...\r\n",
      "        modelo_final = tf.keras.Model(inputs=inp,outputs = out)\r\n",
      "        \r\n",
      "        modelo_final\\\r\n",
      "            .compile(optimizer=\\\r\n",
      "                     tf.keras.optimizers.Adam(\r\n",
      "                         learning_rate=self.learning_rate, \r\n",
      "                         clipnorm=1.0),\r\n",
      "                         loss=self.loss, \r\n",
      "                         metrics=metrics)\r\n",
      "        return modelo_final\r\n",
      "    \r\n",
      "    \r\n",
      "    def parse_block(self,number_of_blocks,\r\n",
      "                        layers_per_block,\r\n",
      "                        neurons_per_block_layer,\r\n",
      "                        neurons_per_block_layer_decay):\r\n",
      "        blocks_0 = []\r\n",
      "        for i in np.arange(number_of_blocks):\r\n",
      "            block_0 = [layers_per_block,\r\n",
      "                       neurons_per_block_layer,\r\n",
      "                       neurons_per_block_layer_decay]\r\n",
      "            blocks_0.append(block_0)\r\n",
      "        return blocks_0\r\n",
      "    \r\n",
      "    def build_auto_residual_mlp(self,hp):\r\n",
      "        \r\n",
      "        self.learning_rate = hp.Float(name='learning_rate',\r\n",
      "                                      min_value=self.minimum_learning_rate, \r\n",
      "                                      max_value=self.maximum_learning_rate,\r\n",
      "                                      sampling='log')\r\n",
      "\r\n",
      "        permutations_of_blocks_array = np.array([[i,j,k,l]\r\n",
      "         for i in np.arange(self.minimum_number_of_blocks,\r\n",
      "                            self.maximum_number_of_blocks + 1)\r\n",
      "         for j in np.arange(self.minimum_number_of_layers_per_block,\r\n",
      "                            self.maximum_number_of_layers_per_block + 1)\r\n",
      "         for k in np.linspace(self.minimum_neurons_per_block_layer,\r\n",
      "                              self.maximum_neurons_per_block_layer, \r\n",
      "                              self.n_options_of_neurons_per_layer_to_try,\r\n",
      "                              dtype=int)\r\n",
      "         for l in np.arange(self.minimum_neurons_per_block_layer_decay,\r\n",
      "                            self.maximum_neurons_per_block_layer_decay + 1)])\r\n",
      "        \r\n",
      "        permutations_of_blocks_df = pd.DataFrame(permutations_of_blocks_array)\r\n",
      "        permutations_of_blocks_df.columns = ['number_of_blocks',\r\n",
      "                                             'layers_per_block',\r\n",
      "                                             'neurons_per_block_layer',\r\n",
      "                                             'neurons_per_block_layer_decay']\r\n",
      "        print(\"All permutations:\")\r\n",
      "        print(permutations_of_blocks_df)\r\n",
      "\r\n",
      "        # Filter out any invalid permutations that would try creating Dense\r\n",
      "        # layers with 0 units or a negative number of units.\r\n",
      "        permutations_of_blocks_df['valid_block'] =\\\r\n",
      "            permutations_of_blocks_df['neurons_per_block_layer'] >\\\r\n",
      "            permutations_of_blocks_df[\"layers_per_block\"] *\\\r\n",
      "            permutations_of_blocks_df[\"neurons_per_block_layer_decay\"]\r\n",
      "        valid_permutations_df =\\\r\n",
      "            permutations_of_blocks_df.query(\"valid_block == True\")\\\r\n",
      "            .reset_index(drop=True)\r\n",
      "        print(\"Valid permutations\")\r\n",
      "        print(valid_permutations_df)\r\n",
      "        \r\n",
      "        list_of_blocks_args = list()\r\n",
      "        for i in np.arange(valid_permutations_df.shape[0]):\r\n",
      "            blocks_arg = self.parse_block(\r\n",
      "                valid_permutations_df.loc[i]['number_of_blocks'],\r\n",
      "                valid_permutations_df.loc[i]['layers_per_block'],\r\n",
      "                valid_permutations_df.loc[i]['neurons_per_block_layer'],\r\n",
      "                valid_permutations_df.loc[i]['neurons_per_block_layer_decay'])\r\n",
      "            list_of_blocks_args.append(blocks_arg)\r\n",
      "        \r\n",
      "        valid_permutations_df['blocks'] = list_of_blocks_args\r\n",
      "        \r\n",
      "        print(\"Valid permutations with blocks column\")\r\n",
      "        print(valid_permutations_df)\r\n",
      " \r\n",
      "        valid_permutations_df.sort_values(['layers_per_block',\r\n",
      "                                           'neurons_per_block_layer'],\r\n",
      "                                            ascending=True)\\\r\n",
      "            .reset_index(drop=True)\r\n",
      "        \r\n",
      "        # for reeference, the list of block options is saved as a csv\r\n",
      "        date = pendulum.now().__str__()[:16].replace(\"T\",\"_\").replace(\":\",\"_\")\r\n",
      "        valid_permutations_df.to_csv(f'{date}_blocks_permutations.csv')\r\n",
      "        \r\n",
      "        list_to_choose_blocks_option_from =\\\r\n",
      "            [int(i) for i in np.arange(valid_permutations_df.shape[0])]\r\n",
      "        blocks_index_chosen = hp.Choice(\r\n",
      "                    name='blocks',\r\n",
      "                    values=list_to_choose_blocks_option_from,\r\n",
      "                    ordered=True)\r\n",
      "        \r\n",
      "        \r\n",
      "        self.blocks = valid_permutations_df.loc[blocks_index_chosen]['blocks']\r\n",
      "        print(self.blocks)\r\n",
      "        \r\n",
      "        bypass_layers_units = hp.Choice(name='bypass_layers_units',\r\n",
      "                        values=[int(i) \r\n",
      "                                for i in np.linspace(\r\n",
      "                                self.minimum_neurons_per_block_layer ,\r\n",
      "                                self.maximum_neurons_per_block_layer,\r\n",
      "                                self.n_options_of_neurons_per_layer_to_try,\r\n",
      "                                dtype=int)],\r\n",
      "                        ordered=True)\r\n",
      "        \r\n",
      "        if bypass_layers_units == 0:\r\n",
      "            self.residual_bypass_dense_layers =\\\r\n",
      "                [list() for _ in np.arange(len(self.blocks))]\r\n",
      "        else:\r\n",
      "            self.residual_bypass_dense_layers =\\\r\n",
      "                [[bypass_layers_units] for _ in np.arange(len(self.blocks))]\r\n",
      "        \r\n",
      "        inter_block_layers_per_block_options =\\\r\n",
      "            [int(i) for i in\r\n",
      "                np.linspace(self.minimum_inter_block_layers_per_block,\r\n",
      "                            self.maximum_inter_block_layers_per_block,\r\n",
      "                            self.n_options_inter_block_layers_per_block,\r\n",
      "                            dtype=int)]\r\n",
      "        inter_block_layers_per_block_choice =\\\r\n",
      "            hp.Choice(name='inter_block_layers',\r\n",
      "                      values=inter_block_layers_per_block_options,\r\n",
      "                      ordered=True)\r\n",
      "\r\n",
      "        self.inter_block_layers_per_block = list()\r\n",
      "        if inter_block_layers_per_block_choice > 1:\r\n",
      "            for i in range(len(self.blocks) -1):\r\n",
      "                self.inter_block_layers_per_block\\\r\n",
      "                    .append(inter_block_layers_per_block_choice)\r\n",
      "\r\n",
      "        final_dense_layers_options = \\\r\n",
      "            [int(i) for i in np.linspace(self.minimum_final_dense_layers,\r\n",
      "                                         self.maximum_final_dense_layers,\r\n",
      "                                         self.n_options_final_dense_layers,\r\n",
      "                                         dtype=int)]\r\n",
      "        final_dense_layers_choice = hp.Choice(\r\n",
      "                                            name='final_dense_layers',\r\n",
      "                                            values=final_dense_layers_options,\r\n",
      "                                            ordered=True)\r\n",
      "        if final_dense_layers_choice == 0:\r\n",
      "            self.final_dense_layers = []\r\n",
      "        else:\r\n",
      "            self.final_dense_layers = [final_dense_layers_choice]\r\n",
      "\r\n",
      "        self.b_norm_or_dropout_residual_bypass_layers =\\\r\n",
      "            hp.Choice(name=\"b_norm_or_dropout_residual_bypass_layers\",\r\n",
      "                      values=['dropout','bnorm'],\r\n",
      "                      ordered=False)\r\n",
      "        dropout_rate_for_bypass_layers_choices =\\\r\n",
      "            [float(i) for i in\r\n",
      "                np.linspace(self.minimum_dropout_rate_for_bypass_layers,\r\n",
      "                        self.maximim_dropout_rate_for_bypass_layers,\r\n",
      "                        self.n_options_dropout_rate_for_bypass_layers,\r\n",
      "                        dtype=float)]\r\n",
      "        self.dropout_rate_for_bypass_layers =\\\r\n",
      "            hp.Choice(name='dropout_rate_for_bypass_layers',\r\n",
      "                      values=dropout_rate_for_bypass_layers_choices,\r\n",
      "                      ordered=True)\r\n",
      "\r\n",
      "        self.b_norm_or_dropout_last_layers =\\\r\n",
      "                        hp.Choice(name='b_norm_or_dropout_last_layers',\r\n",
      "                                  values=['dropout','bnorm'],\r\n",
      "                                  ordered=False)\r\n",
      "        \r\n",
      "        dropout_rate_options = [float(i) for i in \r\n",
      "                                np.linspace(self.minimum_dropout_rate, \r\n",
      "                                            self.maximum_dropout_rate,\r\n",
      "                                            self.n_options_dropout_rate,\r\n",
      "                                            dtype=float)]\r\n",
      "        self.dropout_rate = hp.Choice(name='dropout_rate',\r\n",
      "                                      values=dropout_rate_options,\r\n",
      "                                      ordered=True)\r\n",
      "\r\n",
      "        if self.problem_type == 'classification':\r\n",
      "            precision = tf.keras.metrics.Precision(),\r\n",
      "            recall = tf.keras.metrics.Recall()\r\n",
      "            accuracy = tf.keras.metrics.Accuracy()\r\n",
      "        if self.problem_type == 'classification' and\\\r\n",
      "                self.number_of_classes > 1:\r\n",
      "            metrics = [tf.keras.metrics.TopKCategoricalAccuracy(\r\n",
      "                k = k,\r\n",
      "                name=f'top_{k}_'\r\n",
      "                'categorical_'\r\n",
      "                'accuracy',\r\n",
      "                dtype=None)\r\n",
      "                           for k in np.arange(1,self.number_of_classes)\\\r\n",
      "                               if k < 10]\r\n",
      "            metrics.append(precision)\r\n",
      "            metrics.append(recall)\r\n",
      "            metrics.append(accuracy)\r\n",
      "        elif self.problem_type == 'classification' and\\\r\n",
      "                self.number_of_classes == 1:    \r\n",
      "            metrics = [precision, recall, accuracy]\r\n",
      "        else:\r\n",
      "            rmse = tf.keras.metrics.RootMeanSquaredError()\r\n",
      "            mae = tf.keras.metrics.MeanAbsoluteError()\r\n",
      "            metrics = [rmse, mae]\r\n",
      "    \r\n",
      "        inp = tf.keras.layers.Input(shape = self.input_shape) \r\n",
      "        # Start with input layer that fits. \r\n",
      "        # The keras fucntional API requires an explicit input layer\r\n",
      "        if self.bw_images:\r\n",
      "            x = self.grayscale_to_rgb(inp)\r\n",
      "        else:\r\n",
      "            x = inp\r\n",
      "        if self.base_model != '':\r\n",
      "            x = tf.keras.layers.Resizing(self.base_model_input_shape[0],\r\n",
      "                                         self.base_model_input_shape[1])(x)\r\n",
      "            x = self.base_model(x)\r\n",
      "        if self.flatten_after_base_model:\r\n",
      "            x = tf.keras.layers.Flatten()(x)\r\n",
      "        initializer = tf.keras.initializers.GlorotNormal()\r\n",
      "        for bl in np.arange(len(self.blocks)):\r\n",
      "            block = self.blocks[bl]\r\n",
      "            bypass_block = self.residual_bypass_dense_layers[bl]\r\n",
      "            \r\n",
      "            \r\n",
      "            x = tf.keras.layers.Dense(block[1],\r\n",
      "                                      self.activation,\r\n",
      "                                      kernel_initializer=initializer)(x)\r\n",
      "            y = x\r\n",
      "            x = tf.keras.layers.BatchNormalization()(x)\r\n",
      "            # x proceeds sequentially to the \r\n",
      "            # next Dense layer.\r\n",
      "            \r\n",
      "            if self.b_norm_or_dropout_residual_bypass_layers == 'dropout':\r\n",
      "                y = tf.keras.layers\\\r\n",
      "                    .Dropout(self.dropout_rate_for_bypass_layers)(y)\r\n",
      "            elif self.b_norm_or_dropout_residual_bypass_layers == 'bnorm':\r\n",
      "                y = tf.keras.layers.BatchNormalization()(y)\r\n",
      "            else:\r\n",
      "                raise ValueError(\"The parameter: \"\r\n",
      "                                 \"'b_norm_or_dropout_residual_bypass_\"\r\n",
      "                                 \"layers'\"\r\n",
      "                                 \" must be left default '', or be \"\r\n",
      "                                 \"'dropout' or may be 'bnorm'.\")\r\n",
      "            for bypass_layer in bypass_block:\r\n",
      "                y = tf.keras.layers.Dense(bypass_layer,\r\n",
      "                                          self.activation,\r\n",
      "                                          kernel_initializer=initializer)(y)\r\n",
      "                if self.b_norm_or_dropout_residual_bypass_layers == 'dropout':\r\n",
      "                    y = tf.keras.layers\\\r\n",
      "                        .Dropout(self.dropout_rate_for_bypass_layers)(y)\r\n",
      "                elif self.b_norm_or_dropout_residual_bypass_layers == 'bnorm':\r\n",
      "                    y = tf.keras.layers.BatchNormalization()(y)\r\n",
      "                else:\r\n",
      "                    raise ValueError(\"The parameter: \"\r\n",
      "                                     \"'b_norm_or_dropout_residual_bypass_\"\r\n",
      "                                     \"layers' must be left default '', or be \"\r\n",
      "                                     \"'dropout' or may be 'bnorm'.\")\r\n",
      "            # y does NOT proceed sequentially\r\n",
      "            # to the next layer. This bypasses \r\n",
      "            # several layers and give a memory \r\n",
      "            # that attenuates some of the \r\n",
      "            # deleterious effects of a deeper \r\n",
      "            # network and lets us capture more \r\n",
      "            # complex interactions before \r\n",
      "            # overfitting becomes an issue than \r\n",
      "            # the textbook sequential multi - \r\n",
      "            # layer perceptron ...\r\n",
      "            for j in np.arange(block[0]): \r\n",
      "                x = tf.keras.layers.Dense(block[1] - block[2] * j,\r\n",
      "                                          self.activation,\r\n",
      "                                          kernel_initializer=initializer)(x) \r\n",
      "                x = tf.keras.layers.BatchNormalization()(x)\r\n",
      "    \r\n",
      "            x = tf.keras.layers.Concatenate(axis=1)([x, y])\r\n",
      "            \r\n",
      "            if bl != np.arange(len(self.blocks)).max():\r\n",
      "                for inter_block_layer in self.inter_block_layers_per_block:\r\n",
      "                    x = tf.keras.layers.Dense(inter_block_layer,\r\n",
      "                                          self.activation,\r\n",
      "                                          kernel_initializer=initializer)(x)\r\n",
      "                    x = tf.keras.layers.BatchNormalization()(x)\r\n",
      "    \r\n",
      "        for i in self.final_dense_layers:\r\n",
      "            x = tf.keras.layers.Dense(i,\r\n",
      "                                      self.activation,\r\n",
      "                                      kernel_initializer=initializer)(x) \r\n",
      "            if self.b_norm_or_dropout_last_layers == 'bnorm':\r\n",
      "                x = tf.keras.layers.BatchNormalization()(x)\r\n",
      "            elif self.b_norm_or_dropout_last_layers == 'dropout':\r\n",
      "                x = tf.keras.layers.Dropout(self.dropout_rate)(x)\r\n",
      "            else:\r\n",
      "                raise ValueError(\"For b_norm_or_dropout_last_layers, \" \r\n",
      "                                 \"you must pick either 'dropout' or 'bnorm'\")\r\n",
      "        out = tf.keras.layers.Dense(self.number_of_classes,\r\n",
      "                                    self.final_activation,\r\n",
      "                                    kernel_initializer=initializer)(x)\r\n",
      "    \r\n",
      "        # Declare the graph for our model ...\r\n",
      "        modelo_final = tf.keras.Model(inputs=inp,outputs = out)\r\n",
      "        \r\n",
      "        modelo_final\\\r\n",
      "            .compile(optimizer=\\\r\n",
      "                     tf.keras.optimizers.Adam(\r\n",
      "                         learning_rate=self.learning_rate, \r\n",
      "                         clipnorm=1.0),\r\n",
      "                         loss=self.loss, \r\n",
      "                         metrics=metrics)\r\n",
      "        return modelo_final\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! cat residualmlp/residual_mlp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5a597d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're missing a module:\n",
      "No module named 'pendulum'\n",
      "No problem, we install it...\n",
      "Collecting pendulum\n",
      "  Downloading pendulum-2.1.2-cp38-cp38-manylinux1_x86_64.whl (155 kB)\n",
      "Collecting pytzdata>=2020.1\n",
      "  Downloading pytzdata-2020.1-py2.py3-none-any.whl (489 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0,>=2.6 in /usr/local/lib/python3.8/dist-packages (from pendulum) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil<3.0,>=2.6->pendulum) (1.15.0)\n",
      "Installing collected packages: pytzdata, pendulum\n",
      "Successfully installed pendulum-2.1.2 pytzdata-2020.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "WARNING: You are using pip version 21.2.4; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're missing a module:\n",
      "No module named 'pandas'\n",
      "No problem, we install it...\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.4.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.19.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "WARNING: You are using pip version 21.2.4; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're missing a module:\n",
      "No module named 'keras_tuner'\n",
      "No problem, we install it...\n",
      "Collecting keras_tuner\n",
      "  Downloading keras_tuner-1.1.0-py3-none-any.whl (98 kB)\n",
      "Collecting kt-legacy\n",
      "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from keras_tuner) (21.0)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.8/dist-packages (from keras_tuner) (2.6.0)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (from keras_tuner) (7.27.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from keras_tuner) (2.26.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from keras_tuner) (1.19.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from keras_tuner) (1.4.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (3.0.20)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (4.7.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (58.1.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (0.18.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (0.7.5)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (0.1.3)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (0.2.0)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (2.10.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (5.1.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython->keras_tuner) (5.1.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.16->ipython->keras_tuner) (0.8.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect>4.3->ipython->keras_tuner) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras_tuner) (0.2.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->keras_tuner) (2.4.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests->keras_tuner) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->keras_tuner) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->keras_tuner) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->keras_tuner) (3.2)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (1.39.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (2.0.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (0.37.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (0.4.6)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (0.12.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (1.35.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (1.8.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras_tuner) (3.17.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from absl-py>=0.4->tensorboard->keras_tuner) (1.15.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras_tuner) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras_tuner) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras_tuner) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras_tuner) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->keras_tuner) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras_tuner) (3.1.1)\n",
      "Installing collected packages: kt-legacy, keras-tuner\n",
      "Successfully installed keras-tuner-1.1.0 kt-legacy-1.0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "WARNING: You are using pip version 21.2.4; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import argparse\n",
    "# Add parser for common params\n",
    "\n",
    "try:\n",
    "    import pendulum\n",
    "except ModuleNotFoundError as not_found:\n",
    "    print(\"We're missing a module:\")\n",
    "    print(not_found)\n",
    "    print(\"No problem, we install it...\")\n",
    "    subprocess.run(\"pip3 install pendulum\", \n",
    "    \t           shell=True, \n",
    "    \t           check=True)\n",
    "    import pendulum\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ModuleNotFoundError as not_found:\n",
    "    print(\"We're missing a module:\")\n",
    "    print(not_found)\n",
    "    print(\"No problem, we install it...\")\n",
    "    subprocess.run(\"pip3 install pandas\", \n",
    "    \t           shell=True, \n",
    "    \t           check=True)\n",
    "    import pandas as pd\n",
    "\n",
    "try:\n",
    "    import keras_tuner as kt\n",
    "except ModuleNotFoundError as not_found:\n",
    "    print(\"We're missing a module:\")\n",
    "    print(not_found)\n",
    "    print(\"No problem, we install it...\")\n",
    "    subprocess.run(\"pip3 install keras_tuner\", \n",
    "    \t           shell=True, \n",
    "    \t           check=True)\n",
    "    import keras_tuner as kt\n",
    "\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except ModuleNotFoundError as not_found:\n",
    "    print(\"We're missing a module:\")\n",
    "    print(not_found)\n",
    "    print(\"No problem, we install it...\")\n",
    "    subprocess.run(\"pip3 install tensorflow\", \n",
    "    \t           shell=True, \n",
    "    \t           check=True)\n",
    "    import tensorflow as tf\n",
    "\n",
    "from residualmlp.residual_mlp import ResidualMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "327cc72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 8s 0us/step\n",
      "170508288/170498071 [==============================] - 8s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-05 06:30:11.205215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-05 06:30:11.213290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-05 06:30:11.213907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-05 06:30:11.215776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-05 06:30:11.216425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-05 06:30:11.216982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-05 06:30:11.647935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-05 06:30:11.648576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-05 06:30:11.649106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-05 06:30:11.649627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14029 MB memory:  -> device: 0, name: RTX A4000, pci bus id: 0000:00:05.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "cifar = tf.keras.datasets.cifar10.load_data()\n",
    "(x_train, y_train), (x_test, y_test) = cifar\n",
    "\n",
    "y_train_ohe = tf.one_hot([i[0] for i in  y_train],10)\n",
    "indexes_for_rows = tf.range(0,y_train.shape[0])\n",
    "shuffled_indexes = tf.random.shuffle(indexes_for_rows)\n",
    "# selected_indexes = shuffled_indexes[:TRAINING_SET_SIZE]\n",
    "# selected_x_train = x_train[selected_indexes,:,:,:]\n",
    "# selected_y_train_ohe = y_train_ohe.numpy()[selected_indexes,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae428b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_ohe = tf.one_hot([i[0] for i in y_test],10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9849e725",
   "metadata": {},
   "source": [
    "### Below is copied and pasted from task.py to re-instantiate the default settings used for the original hyperband NAS run when task.py was run. (... even though we are not running this as a script and it otherwise doesn't make sense why I have argparse content here... It is just  a shortcut to get te same global settings back.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e36a5991",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--project_name\", type=str, \n",
    "    help=\"Name for this project.\",\n",
    "    default=\"CIFAR10_EfficientNetB7-ResidualMLP_NAS_AUGMENTED_SPACE\")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--training_set_size\", \n",
    "    type=int,\n",
    "    help=\"Training set size (how many observations).\",\n",
    "    default=50000)\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--patience\",\n",
    "    type=int,\n",
    "    help=\"How many epochs with no improved performance before the \"\n",
    "        \"early stopping callback stops further training?\",\n",
    "    default=25)\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--patience_min_delta\",\n",
    "    type=float,\n",
    "    help=\"How sensitive should the early stopping callback be\"\n",
    "        \"  to change?\",\n",
    "    default=0.00001)\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--batch_size\",\n",
    "    type=int,\n",
    "    help=\"How many observations to train with...\",\n",
    "    default=50)\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--max_epochs\",\n",
    "    type=int,\n",
    "    help=\"max_epochs: Integer, the maximum number of epochs to train one \"\n",
    "         \"model. It is recommended to set this to a value slightly higher \"\n",
    "         \"than the expected epochs to convergence for your largest Model, \"\n",
    "         \"and to use early stopping during training (for example, via .\",\n",
    "         default=50)\n",
    "\n",
    "parser.add_argument(\n",
    "        \"--minimum_learning_rate\",\n",
    "        type=float,\n",
    "        help=\"Lowest learning rate to try?\",\n",
    "        default = 0.00007)\n",
    "\n",
    "parser.add_argument(\n",
    "        \"--maximum_learning_rate\",\n",
    "        type=float,\n",
    "        help=\"Highest learning rate to try?\",\n",
    "        default = 0.7)\n",
    "\n",
    "parser.add_argument(\n",
    "        \"--number_of_learning_rates_to_try\",\n",
    "        type=int,\n",
    "        help=\"How many learning rate to try (maximum)?\",\n",
    "        default = 7)\n",
    "\n",
    "parser.add_argument(\n",
    "        \"--minimum_number_of_blocks\",\n",
    "        type=int,\n",
    "        help=\"Minimum number of ResidualMLP blocks in neural architectures \"\n",
    "             \"to try?\",\n",
    "        default = 1)\n",
    "\n",
    "parser.add_argument(\n",
    "        \"--maximum_number_of_blocks\",\n",
    "        type=int,\n",
    "        help=\"Maximum number of ResidualMLP blocks in neural architectures \"\n",
    "              \"to try?\",\n",
    "        default = 8)\n",
    "\n",
    "\n",
    "parser.add_argument(\n",
    "        \"--minimum_number_of_layers_per_block\",\n",
    "        type=int,\n",
    "        help=\"Minimum number of layers to try in each ResidualMLP block in \"\n",
    "             \"the neural architectures to try?\",\n",
    "        default = 1)\n",
    "\n",
    "parser.add_argument(\n",
    "        \"--maximum_number_of_layers_per_block\",\n",
    "        type=int,\n",
    "        help=\"Maximum number of layers to try in each ResidualMLP block in \"\n",
    "             \"the neural architectures to try?\",\n",
    "        default = 8)\n",
    "\n",
    "parser.add_argument(\n",
    "        \"--minimum_neurons_per_block_layer\",\n",
    "        type=int,\n",
    "        help=\"Minimum number of neurons to try in the Dense layers in \"\n",
    "             \"each ResidualMLP block in the neural architectures to tried?\",\n",
    "        default = 30)\n",
    "\n",
    "parser.add_argument(\n",
    "        \"--maximum_neurons_per_block_layer\",\n",
    "        type=int,\n",
    "        help=\"Maximum number of neurons to try in the Dense layers in \"\n",
    "             \"each ResidualMLP block in the neural architectures to tried?\",\n",
    "        default = 130)\n",
    "\n",
    "parser.add_argument(\n",
    "        \"--n_options_of_neurons_per_layer_to_try\",\n",
    "        type=int,\n",
    "        help=\"How many different numbers of neurons (at most) to try in \"\n",
    "             \"the Dense layers try in each ResidualMLP block in the neural \"\n",
    "             \"architectures to tried?\",\n",
    "        default = 7)\n",
    "\n",
    "parser.add_argument(\n",
    "        \"--minimum_neurons_per_block_layer_decay\",\n",
    "        type=int,\n",
    "        help=\"Lowest decay in number of neurons (n less neurons than the \"\n",
    "             \"last layer) to try in the Dense layers try in each \"\n",
    "             \"ResidualMLP block in the neural \"\n",
    "             \"architectures to tried?\",\n",
    "        default = 0)\n",
    "\n",
    "parser.add_argument(\n",
    "        \"--maximum_neurons_per_block_layer_decay\",\n",
    "        type=int,\n",
    "        help=\"Highest decay in number of neurons (n less neurons than the \"\n",
    "             \"last layer) to try in the Dense layers try in each \"\n",
    "             \"ResidualMLP block in the neural \"\n",
    "             \"architectures to tried?\",\n",
    "        default = 50)\n",
    "\n",
    "parser.add_argument(\n",
    "        \"--minimum_dropout_rate_for_bypass_layers\",\n",
    "        type=float,\n",
    "        help=\"Lowest dropout rate to try for dropout layers located in \"\n",
    "             \"residual byass layers?\",\n",
    "        default = 0.01)\n",
    "\n",
    "parser.add_argument(\n",
    "        \"--maximim_dropout_rate_for_bypass_layers\",\n",
    "        type=float,\n",
    "        help=\"Highest dropout rate to try for dropout layers located in \"\n",
    "             \"residual byass layers?\",\n",
    "        default = 0.7)\n",
    "\n",
    "parser.add_argument(\n",
    "        \"--n_options_dropout_rate_for_bypass_layers\",\n",
    "        type=int,\n",
    "        help=\"How many dropout rates to try for dropout layers located in \"\n",
    "             \"residual byass layers?\",\n",
    "        default = 7)\n",
    "\n",
    "parser.add_argument(\n",
    "        \"--minimum_inter_block_layers_per_block\",\n",
    "        type=int,\n",
    "        help=\"Minimum number of neurons per Dense layer for Dense layers \"\n",
    "             \"inserted between ResidualMLP blocks? 0 Will not insert a \"\n",
    "             \"layer if selected.\",\n",
    "        default = 0)\n",
    "\n",
    "parser.add_argument(\n",
    "        \"--maximum_inter_block_layers_per_block\",\n",
    "        type=int,\n",
    "        help=\"Maximum number of neurons per Dense layer for Dense layers \"\n",
    "             \"inserted between ResidualMLP blocks? 0 Will not insert a \"\n",
    "             \"layer if selected.\",\n",
    "        default = 150)\n",
    "\n",
    "parser.add_argument(\n",
    "        \"--n_options_inter_block_layers_per_block\",\n",
    "        type=int,\n",
    "        help=\"How many different numbers of neurons per Dense layer for \"\n",
    "             \"Dense layers inserted between ResidualMLP blocks will we try \"\n",
    "             \"(at most)?\",\n",
    "        default = 7)\n",
    "\n",
    "parser.add_argument(\n",
    "        \"--minimum_dropout_rate\",\n",
    "        type=float,\n",
    "        help=\"Minimum dropout rate for final Dense layers?\",\n",
    "        default = 0.01)\n",
    "\n",
    "parser.add_argument(\n",
    "        \"--maximum_dropout_rate\",\n",
    "        type=float,\n",
    "        help=\"Maximum dropout rate for final Dense layers?\",\n",
    "        default = 0.7)\n",
    "\n",
    "parser.add_argument(\n",
    "        \"--n_options_dropout_rate\",\n",
    "        type=int,\n",
    "        help=\"How many dropout rates (at max) to try for final Dense layers?\",\n",
    "        default = 7)\n",
    "\n",
    "parser.add_argument(\n",
    "        \"--minimum_final_dense_layers\",\n",
    "        type=int,\n",
    "        help=\"Lowest number of neurons to try for final Dense layers, after \"\n",
    "             \"the last ResidualMLP block, before the very last Dense layer \"\n",
    "             \"returning an output? \"\n",
    "             \"(0 doesn't create a layer')\",\n",
    "        default = 0)\n",
    "\n",
    "parser.add_argument(\n",
    "        \"--maximum_final_dense_layers\",\n",
    "        type=int,\n",
    "        help=\"Highest number of neurons to try for final Dense layers, after \"\n",
    "             \"the last ResidualMLP block, before the very last Dense layer \"\n",
    "             \"returning an output? \"\n",
    "             \"(0 doesn't create a layer')\",\n",
    "        default = 150)\n",
    "\n",
    "parser.add_argument(\n",
    "        \"--n_options_final_dense_layers\",\n",
    "        type=int,\n",
    "        help=\"How many options for neurons to try for final Dense layers, \"\n",
    "             \"after the last ResidualMLP block, before the very last \"\n",
    "             \"Dense layer returning an output?\",\n",
    "        default = 7)\n",
    "\n",
    "\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "hparams = args.__dict__\n",
    "\n",
    "# Boilerplate args\n",
    "\n",
    "DATE = pendulum.now().__str__()[:16].replace(\"T\",\"_\").replace(\":\",\"_\")\n",
    "PROJECT_NAME = hparams[\"project_name\"]\n",
    "TRAINING_SET_SIZE = hparams[\"training_set_size\"]\n",
    "\n",
    "# Keras tuner search & fit args\n",
    "\n",
    "PATIENCE = hparams[\"patience\"]\n",
    "PATIENCE_MIN_DELTA = hparams[\"patience_min_delta\"]\n",
    "BATCH_SIZE = hparams[\"batch_size\"]\n",
    "MAX_EPOCHS = hparams[\"max_epochs\"]\n",
    "RESULTS_DIR_FOR_SEARCH =\\\n",
    "    f'{DATE}_{PROJECT_NAME}_SEARCH_RUN'\n",
    "\n",
    "\n",
    "# Base model args\n",
    "\n",
    "BASE_MODEL_INPUT_SHAPE = (600,600,3)\n",
    "\n",
    "# ResidualMLP model args\n",
    "\n",
    "MINIMUM_LEARNING_RATE = hparams[\"minimum_learning_rate\"]\n",
    "MAXIMUM_LEARNING_RATE = hparams[\"maximum_learning_rate\"]\n",
    "NUMBER_OF_LEARNING_RATES_TO_TRY = hparams[\"number_of_learning_rates_to_try\"]\n",
    "MINIMUM_NUMBER_OF_BLOCKS = hparams[\"minimum_number_of_blocks\"]\n",
    "MAXIMUM_NUMBER_OF_BLOCKS = hparams[\"maximum_number_of_blocks\"]\n",
    "MINIMUM_NUMBER_OF_LAYERS_PER_BLOCK =\\\n",
    "    hparams[\"minimum_number_of_layers_per_block\"]\n",
    "MAXIMUM_NUMBER_OF_LAYERS_PER_BLOCK =\\\n",
    "    hparams[\"maximum_number_of_layers_per_block\"]\n",
    "MINIMUM_NEURONS_PER_BLOCK_LAYER = hparams[\"minimum_neurons_per_block_layer\"]\n",
    "MAXIMUM_NEURONS_PER_BLOCK_LAYER = hparams[\"maximum_neurons_per_block_layer\"]\n",
    "N_OPTIONS_OF_NEURONS_PER_LAYER_TO_TRY =\\\n",
    "    hparams[\"n_options_of_neurons_per_layer_to_try\"]\n",
    "MINIMUM_NEURONS_PER_BLOCK_LAYER_DECAY =\\\n",
    "    hparams[\"minimum_neurons_per_block_layer_decay\"]\n",
    "MAXIMUM_NEURONS_PER_BLOCK_LAYER_DECAY =\\\n",
    "    hparams[\"maximum_neurons_per_block_layer_decay\"]\n",
    "MINIMUM_DROPOUT_RATE_FOR_BYPASS_LAYERS =\\\n",
    "    hparams[\"minimum_dropout_rate_for_bypass_layers\"]\n",
    "MAXIMIM_DROPOUT_RATE_FOR_BYPASS_LAYERS =\\\n",
    "    hparams[\"maximim_dropout_rate_for_bypass_layers\"]\n",
    "N_OPTIONS_DROPOUT_RATE_FOR_BYPASS_LAYERS =\\\n",
    "    hparams[\"n_options_dropout_rate_for_bypass_layers\"]\n",
    "MINIMUM_INTER_BLOCK_LAYERS_PER_BLOCK =\\\n",
    "    hparams[\"minimum_inter_block_layers_per_block\"]\n",
    "MAXIMUM_INTER_BLOCK_LAYERS_PER_BLOCK =\\\n",
    "    hparams[\"maximum_inter_block_layers_per_block\"]\n",
    "N_OPTIONS_INTER_BLOCK_LAYERS_PER_BLOCK =\\\n",
    "    hparams[\"n_options_inter_block_layers_per_block\"]\n",
    "MINIMUM_DROPOUT_RATE = hparams[\"minimum_dropout_rate\"]\n",
    "MAXIMUM_DROPOUT_RATE = hparams[\"maximum_dropout_rate\"]\n",
    "N_OPTIONS_DROPOUT_RATE = hparams[\"n_options_dropout_rate\"]\n",
    "MINIMUM_FINAL_DENSE_LAYERS = hparams[\"minimum_final_dense_layers\"]\n",
    "MAXIMUM_FINAL_DENSE_LAYERS = hparams[\"maximum_final_dense_layers\"]\n",
    "N_OPTIONS_FINAL_DENSE_LAYERS = hparams[\"n_options_final_dense_layers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f3e0098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2022-02-05_06_30\n",
      "project_name: CIFAR10_EfficientNetB7-ResidualMLP_NAS_AUGMENTED_SPACE\n",
      "training_set_size: 50000\n",
      "patience: 25\n",
      "PATIENCE_MIN_DELTA: 1e-05\n",
      "BATCH_SIZE: 50\n",
      "MAX_EPOCHS: 50\n",
      "RESULTS_DIR_FOR_SEARCH: 2022-02-05_06_30_CIFAR10_EfficientNetB7-ResidualMLP_NAS_AUGMENTED_SPACE_SEARCH_RUN\n",
      "MINIMUM_LEARNING_RATE 7e-05\n",
      "MAXIMUM_LEARNING_RATE: 0.7\n",
      "NUMBER_OF_LEARNING_RATES_TO_TRY: 7\n",
      "MINIMUM_NUMBER_OF_BLOCKS: 1\n",
      "MAXIMUM_NUMBER_OF_BLOCKS: 8\n",
      "MINIMUM_NUMBER_OF_LAYERS_PER_BLOCK: 1\n",
      "MAXIMUM_NUMBER_OF_LAYERS_PER_BLOCK: 8\n",
      "MINIMUM_NEURONS_PER_BLOCK_LAYER: 30\n",
      "MAXIMUM_NEURONS_PER_BLOCK_LAYER: 130\n",
      "N_OPTIONS_OF_NEURONS_PER_LAYER_TO_TRY: 7\n",
      "MINIMUM_NEURONS_PER_BLOCK_LAYER_DECAY: 0\n",
      "MAXIMUM_NEURONS_PER_BLOCK_LAYER_DECAY: 50\n",
      "MINIMUM_DROPOUT_RATE_FOR_BYPASS_LAYERS: 0.01\n",
      "MAXIMIM_DROPOUT_RATE_FOR_BYPASS_LAYERS: 0.7\n",
      "N_OPTIONS_DROPOUT_RATE_FOR_BYPASS_LAYERS: 7\n",
      "MINIMUM_INTER_BLOCK_LAYERS_PER_BLOCK: 0\n",
      "MAXIMUM_INTER_BLOCK_LAYERS_PER_BLOCK: 150\n",
      "N_OPTIONS_INTER_BLOCK_LAYERS_PER_BLOCK: 7\n",
      "MINIMUM_DROPOUT_RATE: 0.01\n",
      "MAXIMUM_DROPOUT_RATE: 0.7\n",
      "N_OPTIONS_DROPOUT_RATE: 7\n",
      "MINIMUM_FINAL_DENSE_LAYERS: 0\n",
      "MAXIMUM_FINAL_DENSE_LAYERS: 150\n",
      "N_OPTIONS_FINAL_DENSE_LAYERS: 7\n"
     ]
    }
   ],
   "source": [
    "print(f\"Date: {DATE}\")\n",
    "print(f\"project_name: {PROJECT_NAME}\")\n",
    "print(f\"training_set_size: {TRAINING_SET_SIZE}\")\n",
    "print(f\"patience: {PATIENCE}\")\n",
    "print(f\"PATIENCE_MIN_DELTA: {PATIENCE_MIN_DELTA}\")\n",
    "print(f\"BATCH_SIZE: {BATCH_SIZE}\")\n",
    "print(f\"MAX_EPOCHS: {MAX_EPOCHS}\")\n",
    "print(f\"RESULTS_DIR_FOR_SEARCH: {RESULTS_DIR_FOR_SEARCH}\")\n",
    "print(f\"MINIMUM_LEARNING_RATE {MINIMUM_LEARNING_RATE}\")\n",
    "print(f\"MAXIMUM_LEARNING_RATE: {MAXIMUM_LEARNING_RATE}\")\n",
    "print(f\"NUMBER_OF_LEARNING_RATES_TO_TRY: {NUMBER_OF_LEARNING_RATES_TO_TRY}\")\n",
    "print(f\"MINIMUM_NUMBER_OF_BLOCKS: {MINIMUM_NUMBER_OF_BLOCKS}\")\n",
    "print(f\"MAXIMUM_NUMBER_OF_BLOCKS: {MAXIMUM_NUMBER_OF_BLOCKS}\")\n",
    "print(\"MINIMUM_NUMBER_OF_LAYERS_PER_BLOCK: \"\n",
    "      f\"{MINIMUM_NUMBER_OF_LAYERS_PER_BLOCK}\")\n",
    "print(\"MAXIMUM_NUMBER_OF_LAYERS_PER_BLOCK: \"\n",
    "      f\"{MAXIMUM_NUMBER_OF_LAYERS_PER_BLOCK}\")\n",
    "print(f\"MINIMUM_NEURONS_PER_BLOCK_LAYER: {MINIMUM_NEURONS_PER_BLOCK_LAYER}\")\n",
    "print(f\"MAXIMUM_NEURONS_PER_BLOCK_LAYER: {MAXIMUM_NEURONS_PER_BLOCK_LAYER}\")\n",
    "print(\"N_OPTIONS_OF_NEURONS_PER_LAYER_TO_TRY: \"\n",
    "      f\"{N_OPTIONS_OF_NEURONS_PER_LAYER_TO_TRY}\")\n",
    "print(\"MINIMUM_NEURONS_PER_BLOCK_LAYER_DECAY: \"\n",
    "      f\"{MINIMUM_NEURONS_PER_BLOCK_LAYER_DECAY}\")\n",
    "print(\"MAXIMUM_NEURONS_PER_BLOCK_LAYER_DECAY: \"\n",
    "      f\"{MAXIMUM_NEURONS_PER_BLOCK_LAYER_DECAY}\")\n",
    "print(\"MINIMUM_DROPOUT_RATE_FOR_BYPASS_LAYERS: \"\n",
    "      f\"{MINIMUM_DROPOUT_RATE_FOR_BYPASS_LAYERS}\")\n",
    "print(\"MAXIMIM_DROPOUT_RATE_FOR_BYPASS_LAYERS: \"\n",
    "      f\"{MAXIMIM_DROPOUT_RATE_FOR_BYPASS_LAYERS}\")\n",
    "print(\"N_OPTIONS_DROPOUT_RATE_FOR_BYPASS_LAYERS: \"\n",
    "      f\"{N_OPTIONS_DROPOUT_RATE_FOR_BYPASS_LAYERS}\")\n",
    "print(\"MINIMUM_INTER_BLOCK_LAYERS_PER_BLOCK: \"\n",
    "      f\"{MINIMUM_INTER_BLOCK_LAYERS_PER_BLOCK}\")\n",
    "print(\"MAXIMUM_INTER_BLOCK_LAYERS_PER_BLOCK: \"\n",
    "      f\"{MAXIMUM_INTER_BLOCK_LAYERS_PER_BLOCK}\")\n",
    "print(\"N_OPTIONS_INTER_BLOCK_LAYERS_PER_BLOCK: \"\n",
    "      f\"{N_OPTIONS_INTER_BLOCK_LAYERS_PER_BLOCK}\")\n",
    "print(f\"MINIMUM_DROPOUT_RATE: {MINIMUM_DROPOUT_RATE}\")\n",
    "print(f\"MAXIMUM_DROPOUT_RATE: {MAXIMUM_DROPOUT_RATE}\")\n",
    "print(f\"N_OPTIONS_DROPOUT_RATE: {N_OPTIONS_DROPOUT_RATE}\")\n",
    "print(f\"MINIMUM_FINAL_DENSE_LAYERS: {MINIMUM_FINAL_DENSE_LAYERS}\")\n",
    "print(f\"MAXIMUM_FINAL_DENSE_LAYERS: {MAXIMUM_FINAL_DENSE_LAYERS}\")\n",
    "print(f\"N_OPTIONS_FINAL_DENSE_LAYERS: {N_OPTIONS_FINAL_DENSE_LAYERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c6f90",
   "metadata": {},
   "source": [
    "### We instantiate an EfficientNetB7 previously trained on ImageNet, remove the final Dense and BatchNormalization layers after the last Conv2d layer. We set the last Conv2D layer to trainable.  We pass the output of this to a ResidualMLP model of variable hyperparameterized neural architecture. We set the hyperparameter and neural architecture search space to be explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca8dc5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb7.h5\n",
      "268328960/268326632 [==============================] - 12s 0us/step\n",
      "268337152/268326632 [==============================] - 12s 0us/step\n"
     ]
    }
   ],
   "source": [
    "mod_with_fc_raw = tf.keras.applications.efficientnet.EfficientNetB7(\n",
    "    include_top=True, weights='imagenet', input_tensor=None,\n",
    "    input_shape = BASE_MODEL_INPUT_SHAPE, pooling='max', classes=1000\n",
    ")\n",
    "\n",
    "# Make the deepest conv2d layer trainable, leave everything else\n",
    "# as not trainable\n",
    "for layer in mod_with_fc_raw.layers:\n",
    "    layer.trainable = False\n",
    "# Last conv2d layer. This we want to train .\n",
    "mod_with_fc_raw.layers[-6].trainable = True\n",
    "\n",
    "# Create the final base model\n",
    "# (remove the final Dense and BatchNormalization layers ...) \n",
    "efficient_net_b_7_transferable_base_model =\\\n",
    "    tf.keras.Model(inputs=mod_with_fc_raw.layers[0].input, \n",
    "                    outputs=mod_with_fc_raw.layers[-3].output)\n",
    "\n",
    "model_builder = ResidualMLP(\n",
    "                    problem_type= \"classification\",  \n",
    "                    minimum_learning_rate = MINIMUM_LEARNING_RATE, \n",
    "                    maximum_learning_rate = MAXIMUM_LEARNING_RATE, \n",
    "                    number_of_learning_rates_to_try =\n",
    "                        NUMBER_OF_LEARNING_RATES_TO_TRY, \n",
    "                    input_shape = (32,32,3), \n",
    "                    bw_images = False, \n",
    "                    base_model = \n",
    "                        efficient_net_b_7_transferable_base_model, \n",
    "                    base_model_input_shape = BASE_MODEL_INPUT_SHAPE, \n",
    "                    flatten_after_base_model = False, \n",
    "                    minimum_number_of_blocks = MINIMUM_NUMBER_OF_BLOCKS, \n",
    "                    maximum_number_of_blocks = MAXIMUM_NUMBER_OF_BLOCKS, \n",
    "                    minimum_number_of_layers_per_block =\n",
    "                        MINIMUM_NUMBER_OF_LAYERS_PER_BLOCK, \n",
    "                    maximum_number_of_layers_per_block =\n",
    "                        MAXIMUM_NUMBER_OF_LAYERS_PER_BLOCK,\n",
    "                    minimum_neurons_per_block_layer =\n",
    "                        MINIMUM_NEURONS_PER_BLOCK_LAYER, \n",
    "                    maximum_neurons_per_block_layer =\n",
    "                        MAXIMUM_NEURONS_PER_BLOCK_LAYER, \n",
    "                    n_options_of_neurons_per_layer_to_try =\n",
    "                        N_OPTIONS_OF_NEURONS_PER_LAYER_TO_TRY, \n",
    "                    minimum_neurons_per_block_layer_decay =\n",
    "                        MINIMUM_NEURONS_PER_BLOCK_LAYER_DECAY, \n",
    "                    maximum_neurons_per_block_layer_decay = \n",
    "                        MAXIMUM_NEURONS_PER_BLOCK_LAYER_DECAY, \n",
    "                    minimum_dropout_rate_for_bypass_layers =\n",
    "                        MINIMUM_DROPOUT_RATE_FOR_BYPASS_LAYERS, \n",
    "                    maximim_dropout_rate_for_bypass_layers =\n",
    "                        MAXIMIM_DROPOUT_RATE_FOR_BYPASS_LAYERS, \n",
    "                    n_options_dropout_rate_for_bypass_layers =\n",
    "                        N_OPTIONS_DROPOUT_RATE_FOR_BYPASS_LAYERS,\n",
    "                    minimum_inter_block_layers_per_block =\n",
    "                        MINIMUM_INTER_BLOCK_LAYERS_PER_BLOCK, \n",
    "                    maximum_inter_block_layers_per_block =\n",
    "                        MAXIMUM_INTER_BLOCK_LAYERS_PER_BLOCK,\n",
    "                    n_options_inter_block_layers_per_block =\\\n",
    "                        N_OPTIONS_INTER_BLOCK_LAYERS_PER_BLOCK,\n",
    "                    minimum_dropout_rate = MINIMUM_DROPOUT_RATE, \n",
    "                    maximum_dropout_rate = MAXIMUM_DROPOUT_RATE,\n",
    "                    n_options_dropout_rate = N_OPTIONS_DROPOUT_RATE, \n",
    "                    minimum_final_dense_layers =\n",
    "                        MINIMUM_FINAL_DENSE_LAYERS,\n",
    "                    maximum_final_dense_layers =\n",
    "                        MAXIMUM_FINAL_DENSE_LAYERS, \n",
    "                    n_options_final_dense_layers =\n",
    "                        N_OPTIONS_FINAL_DENSE_LAYERS, \n",
    "                    number_of_classes = 10,\n",
    "                    final_activation = tf.keras.activations.softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1fb265",
   "metadata": {},
   "source": [
    "## We restore the hyperband tunerused in task.py and view the summaru of results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcbd3aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project ./untitled_project/oracle.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-05 06:30:27.003881: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2022-02-05 06:30:27.003932: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2022-02-05 06:30:27.003973: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
      "2022-02-05 06:30:27.094328: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2022-02-05 06:30:27.094538: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1749] CUPTI activity buffer flushed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All permutations:\n",
      "       number_of_blocks  layers_per_block  neurons_per_block_layer  \\\n",
      "0                     1                 1                       30   \n",
      "1                     1                 1                       30   \n",
      "2                     1                 1                       30   \n",
      "3                     1                 1                       30   \n",
      "4                     1                 1                       30   \n",
      "...                 ...               ...                      ...   \n",
      "22843                 8                 8                      130   \n",
      "22844                 8                 8                      130   \n",
      "22845                 8                 8                      130   \n",
      "22846                 8                 8                      130   \n",
      "22847                 8                 8                      130   \n",
      "\n",
      "       neurons_per_block_layer_decay  \n",
      "0                                  0  \n",
      "1                                  1  \n",
      "2                                  2  \n",
      "3                                  3  \n",
      "4                                  4  \n",
      "...                              ...  \n",
      "22843                             46  \n",
      "22844                             47  \n",
      "22845                             48  \n",
      "22846                             49  \n",
      "22847                             50  \n",
      "\n",
      "[22848 rows x 4 columns]\n",
      "Valid permutations\n",
      "       number_of_blocks  layers_per_block  neurons_per_block_layer  \\\n",
      "0                     1                 1                       30   \n",
      "1                     1                 1                       30   \n",
      "2                     1                 1                       30   \n",
      "3                     1                 1                       30   \n",
      "4                     1                 1                       30   \n",
      "...                 ...               ...                      ...   \n",
      "10275                 8                 8                      130   \n",
      "10276                 8                 8                      130   \n",
      "10277                 8                 8                      130   \n",
      "10278                 8                 8                      130   \n",
      "10279                 8                 8                      130   \n",
      "\n",
      "       neurons_per_block_layer_decay  valid_block  \n",
      "0                                  0         True  \n",
      "1                                  1         True  \n",
      "2                                  2         True  \n",
      "3                                  3         True  \n",
      "4                                  4         True  \n",
      "...                              ...          ...  \n",
      "10275                             12         True  \n",
      "10276                             13         True  \n",
      "10277                             14         True  \n",
      "10278                             15         True  \n",
      "10279                             16         True  \n",
      "\n",
      "[10280 rows x 5 columns]\n",
      "Valid permutations with blocks column\n",
      "       number_of_blocks  layers_per_block  neurons_per_block_layer  \\\n",
      "0                     1                 1                       30   \n",
      "1                     1                 1                       30   \n",
      "2                     1                 1                       30   \n",
      "3                     1                 1                       30   \n",
      "4                     1                 1                       30   \n",
      "...                 ...               ...                      ...   \n",
      "10275                 8                 8                      130   \n",
      "10276                 8                 8                      130   \n",
      "10277                 8                 8                      130   \n",
      "10278                 8                 8                      130   \n",
      "10279                 8                 8                      130   \n",
      "\n",
      "       neurons_per_block_layer_decay  valid_block  \\\n",
      "0                                  0         True   \n",
      "1                                  1         True   \n",
      "2                                  2         True   \n",
      "3                                  3         True   \n",
      "4                                  4         True   \n",
      "...                              ...          ...   \n",
      "10275                             12         True   \n",
      "10276                             13         True   \n",
      "10277                             14         True   \n",
      "10278                             15         True   \n",
      "10279                             16         True   \n",
      "\n",
      "                                                  blocks  \n",
      "0                                           [[1, 30, 0]]  \n",
      "1                                           [[1, 30, 1]]  \n",
      "2                                           [[1, 30, 2]]  \n",
      "3                                           [[1, 30, 3]]  \n",
      "4                                           [[1, 30, 4]]  \n",
      "...                                                  ...  \n",
      "10275  [[8, 130, 12], [8, 130, 12], [8, 130, 12], [8,...  \n",
      "10276  [[8, 130, 13], [8, 130, 13], [8, 130, 13], [8,...  \n",
      "10277  [[8, 130, 14], [8, 130, 14], [8, 130, 14], [8,...  \n",
      "10278  [[8, 130, 15], [8, 130, 15], [8, 130, 15], [8,...  \n",
      "10279  [[8, 130, 16], [8, 130, 16], [8, 130, 16], [8,...  \n",
      "\n",
      "[10280 rows x 6 columns]\n",
      "[[1, 30, 0]]\n",
      "INFO:tensorflow:Reloading Tuner from ./untitled_project/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATE = pendulum.now().__str__()[:16].replace(\"T\",\"_\").replace(\":\",\"_\")\n",
    "\n",
    "\n",
    "\n",
    "RESULTS_DIR_FOR_SEARCH =\\\n",
    "    f'{DATE}_{PROJECT_NAME}_SEARCH_RUN'\n",
    "\n",
    "logdir_for_search = os.path.join(\"logs\", RESULTS_DIR_FOR_SEARCH + \"_TB\")\n",
    "tensorboard_callback_search =\\\n",
    "    tf.keras.callbacks.TensorBoard(logdir_for_search, histogram_freq=1)\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    model_builder.build_auto_residual_mlp,\n",
    "    objective='val_loss',\n",
    "    max_epochs = MAX_EPOCHS,\n",
    "    hyperband_iterations = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05624c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in ./untitled_project\n",
      "Showing 10 best trials\n",
      "Objective(name='val_loss', direction='min')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.009343337171789316\n",
      "blocks: 399\n",
      "bypass_layers_units: 46\n",
      "inter_block_layers: 25\n",
      "final_dense_layers: 150\n",
      "b_norm_or_dropout_residual_bypass_layers: dropout\n",
      "dropout_rate_for_bypass_layers: 0.24\n",
      "b_norm_or_dropout_last_layers: bnorm\n",
      "dropout_rate: 0.355\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 3\n",
      "tuner/round: 0\n",
      "Score: 0.3171733617782593\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0004635558198557901\n",
      "blocks: 1702\n",
      "bypass_layers_units: 30\n",
      "inter_block_layers: 25\n",
      "final_dense_layers: 25\n",
      "b_norm_or_dropout_residual_bypass_layers: dropout\n",
      "dropout_rate_for_bypass_layers: 0.24\n",
      "b_norm_or_dropout_last_layers: bnorm\n",
      "dropout_rate: 0.12499999999999999\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 3\n",
      "tuner/round: 0\n",
      "Score: 0.328523188829422\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.03967702188815936\n",
      "blocks: 1747\n",
      "bypass_layers_units: 130\n",
      "inter_block_layers: 75\n",
      "final_dense_layers: 0\n",
      "b_norm_or_dropout_residual_bypass_layers: bnorm\n",
      "dropout_rate_for_bypass_layers: 0.355\n",
      "b_norm_or_dropout_last_layers: bnorm\n",
      "dropout_rate: 0.355\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 3\n",
      "tuner/round: 0\n",
      "Score: 0.3632817566394806\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0008526928034881207\n",
      "blocks: 1994\n",
      "bypass_layers_units: 30\n",
      "inter_block_layers: 0\n",
      "final_dense_layers: 125\n",
      "b_norm_or_dropout_residual_bypass_layers: bnorm\n",
      "dropout_rate_for_bypass_layers: 0.01\n",
      "b_norm_or_dropout_last_layers: dropout\n",
      "dropout_rate: 0.47\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 3\n",
      "tuner/round: 0\n",
      "Score: 0.48120883107185364\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.016438365071667294\n",
      "blocks: 3665\n",
      "bypass_layers_units: 130\n",
      "inter_block_layers: 25\n",
      "final_dense_layers: 125\n",
      "b_norm_or_dropout_residual_bypass_layers: bnorm\n",
      "dropout_rate_for_bypass_layers: 0.24\n",
      "b_norm_or_dropout_last_layers: bnorm\n",
      "dropout_rate: 0.355\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 3\n",
      "tuner/round: 0\n",
      "Score: 0.6876320242881775\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0029066594056213328\n",
      "blocks: 6026\n",
      "bypass_layers_units: 30\n",
      "inter_block_layers: 75\n",
      "final_dense_layers: 50\n",
      "b_norm_or_dropout_residual_bypass_layers: bnorm\n",
      "dropout_rate_for_bypass_layers: 0.355\n",
      "b_norm_or_dropout_last_layers: bnorm\n",
      "dropout_rate: 0.01\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 3\n",
      "tuner/round: 0\n",
      "Score: 0.746197521686554\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.004233235668502465\n",
      "blocks: 3951\n",
      "bypass_layers_units: 80\n",
      "inter_block_layers: 125\n",
      "final_dense_layers: 75\n",
      "b_norm_or_dropout_residual_bypass_layers: dropout\n",
      "dropout_rate_for_bypass_layers: 0.7\n",
      "b_norm_or_dropout_last_layers: bnorm\n",
      "dropout_rate: 0.585\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 3\n",
      "tuner/round: 0\n",
      "Score: 1.700502872467041\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.010590476276060884\n",
      "blocks: 9941\n",
      "bypass_layers_units: 96\n",
      "inter_block_layers: 25\n",
      "final_dense_layers: 75\n",
      "b_norm_or_dropout_residual_bypass_layers: dropout\n",
      "dropout_rate_for_bypass_layers: 0.7\n",
      "b_norm_or_dropout_last_layers: bnorm\n",
      "dropout_rate: 0.24\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 3\n",
      "tuner/round: 0\n",
      "Score: 2.305290937423706\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.04869544657951322\n",
      "blocks: 7465\n",
      "bypass_layers_units: 46\n",
      "inter_block_layers: 150\n",
      "final_dense_layers: 125\n",
      "b_norm_or_dropout_residual_bypass_layers: bnorm\n",
      "dropout_rate_for_bypass_layers: 0.24\n",
      "b_norm_or_dropout_last_layers: dropout\n",
      "dropout_rate: 0.24\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 3\n",
      "tuner/round: 0\n",
      "Score: 2.306441068649292\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.09865337709239926\n",
      "blocks: 5800\n",
      "bypass_layers_units: 46\n",
      "inter_block_layers: 150\n",
      "final_dense_layers: 75\n",
      "b_norm_or_dropout_residual_bypass_layers: dropout\n",
      "dropout_rate_for_bypass_layers: 0.7\n",
      "b_norm_or_dropout_last_layers: dropout\n",
      "dropout_rate: 0.01\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 3\n",
      "tuner/round: 0\n",
      "Score: 2.306997299194336\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1d2e98",
   "metadata": {},
   "source": [
    "# Best architecture so far (12 hours of searching on a node with 2 A6000 GPUs and 90 GB RAM): \n",
    "### Residual blocks:  [[2, 63, 28]]\n",
    "1. This means: The Input() layer's output proceeds to on Dense(63) layer.\n",
    "2. The output of this is duplicated into 2 tensors: x and y.\n",
    "3. x proceeds through a block of 2 Dense layers:\n",
    "    1. Dense(63) - then BatchNormalization \n",
    "    2. Dense(33) - then BatchNormalization \n",
    "4. y bypasses this block and proceeds through: \n",
    "    1. One Dense(46) layer in the bypass path followed by \n",
    "    2. Dropout(0.24)\n",
    "4. The outputs of x and y merge in a concat layer.\n",
    "\n",
    "### The output of this goes through:\n",
    "1. One Dense(150) \n",
    "2. a batch notmalization layer\n",
    "3. Dense(10) with softmax. (model's output)\n",
    "\n",
    "### Note that in this architecture, inter_block_layers is ignored, as this only applies to architectures consisting of more than one residual block.\n",
    "\n",
    "Val loss: 0.317\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eee6d2",
   "metadata": {},
   "source": [
    "## Let's examine the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be5e86ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All permutations:\n",
      "       number_of_blocks  layers_per_block  neurons_per_block_layer  \\\n",
      "0                     1                 1                       30   \n",
      "1                     1                 1                       30   \n",
      "2                     1                 1                       30   \n",
      "3                     1                 1                       30   \n",
      "4                     1                 1                       30   \n",
      "...                 ...               ...                      ...   \n",
      "22843                 8                 8                      130   \n",
      "22844                 8                 8                      130   \n",
      "22845                 8                 8                      130   \n",
      "22846                 8                 8                      130   \n",
      "22847                 8                 8                      130   \n",
      "\n",
      "       neurons_per_block_layer_decay  \n",
      "0                                  0  \n",
      "1                                  1  \n",
      "2                                  2  \n",
      "3                                  3  \n",
      "4                                  4  \n",
      "...                              ...  \n",
      "22843                             46  \n",
      "22844                             47  \n",
      "22845                             48  \n",
      "22846                             49  \n",
      "22847                             50  \n",
      "\n",
      "[22848 rows x 4 columns]\n",
      "Valid permutations\n",
      "       number_of_blocks  layers_per_block  neurons_per_block_layer  \\\n",
      "0                     1                 1                       30   \n",
      "1                     1                 1                       30   \n",
      "2                     1                 1                       30   \n",
      "3                     1                 1                       30   \n",
      "4                     1                 1                       30   \n",
      "...                 ...               ...                      ...   \n",
      "10275                 8                 8                      130   \n",
      "10276                 8                 8                      130   \n",
      "10277                 8                 8                      130   \n",
      "10278                 8                 8                      130   \n",
      "10279                 8                 8                      130   \n",
      "\n",
      "       neurons_per_block_layer_decay  valid_block  \n",
      "0                                  0         True  \n",
      "1                                  1         True  \n",
      "2                                  2         True  \n",
      "3                                  3         True  \n",
      "4                                  4         True  \n",
      "...                              ...          ...  \n",
      "10275                             12         True  \n",
      "10276                             13         True  \n",
      "10277                             14         True  \n",
      "10278                             15         True  \n",
      "10279                             16         True  \n",
      "\n",
      "[10280 rows x 5 columns]\n",
      "Valid permutations with blocks column\n",
      "       number_of_blocks  layers_per_block  neurons_per_block_layer  \\\n",
      "0                     1                 1                       30   \n",
      "1                     1                 1                       30   \n",
      "2                     1                 1                       30   \n",
      "3                     1                 1                       30   \n",
      "4                     1                 1                       30   \n",
      "...                 ...               ...                      ...   \n",
      "10275                 8                 8                      130   \n",
      "10276                 8                 8                      130   \n",
      "10277                 8                 8                      130   \n",
      "10278                 8                 8                      130   \n",
      "10279                 8                 8                      130   \n",
      "\n",
      "       neurons_per_block_layer_decay  valid_block  \\\n",
      "0                                  0         True   \n",
      "1                                  1         True   \n",
      "2                                  2         True   \n",
      "3                                  3         True   \n",
      "4                                  4         True   \n",
      "...                              ...          ...   \n",
      "10275                             12         True   \n",
      "10276                             13         True   \n",
      "10277                             14         True   \n",
      "10278                             15         True   \n",
      "10279                             16         True   \n",
      "\n",
      "                                                  blocks  \n",
      "0                                           [[1, 30, 0]]  \n",
      "1                                           [[1, 30, 1]]  \n",
      "2                                           [[1, 30, 2]]  \n",
      "3                                           [[1, 30, 3]]  \n",
      "4                                           [[1, 30, 4]]  \n",
      "...                                                  ...  \n",
      "10275  [[8, 130, 12], [8, 130, 12], [8, 130, 12], [8,...  \n",
      "10276  [[8, 130, 13], [8, 130, 13], [8, 130, 13], [8,...  \n",
      "10277  [[8, 130, 14], [8, 130, 14], [8, 130, 14], [8,...  \n",
      "10278  [[8, 130, 15], [8, 130, 15], [8, 130, 15], [8,...  \n",
      "10279  [[8, 130, 16], [8, 130, 16], [8, 130, 16], [8,...  \n",
      "\n",
      "[10280 rows x 6 columns]\n",
      "[[2, 63, 30]]\n"
     ]
    }
   ],
   "source": [
    "best_model_so_far_2022_02_04 = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a3d833",
   "metadata": {},
   "source": [
    "### Let's look at the details of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c99a0109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "resizing (Resizing)             (None, 600, 600, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model (Functional)              (None, 2560)         64097687    resizing[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 63)           161343      model[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 63)           252         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 63)           4032        batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 63)           252         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 63)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 33)           2112        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 46)           2944        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 33)           132         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 46)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 79)           0           batch_normalization_2[0][0]      \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 150)          12000       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 150)          600         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 10)           1510        batch_normalization_3[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 64,282,864\n",
      "Trainable params: 1,822,959\n",
      "Non-trainable params: 62,459,905\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model_so_far_2022_02_04.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c568f1",
   "metadata": {},
   "source": [
    "### Let's see how this performs on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bba8ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-05 06:30:51.710845: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-02-05 06:30:56.204242: I tensorflow/stream_executor/cuda/cuda_dnn.cc:381] Loaded cuDNN version 8204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  1/313 [..............................] - ETA: 45:00 - loss: 0.3043 - top_1_categorical_accuracy: 0.9375 - top_2_categorical_accuracy: 0.9375 - top_3_categorical_accuracy: 0.9375 - top_4_categorical_accuracy: 0.9688 - top_5_categorical_accuracy: 0.9688 - top_6_categorical_accuracy: 0.9688 - top_7_categorical_accuracy: 0.9688 - top_8_categorical_accuracy: 1.0000 - top_9_categorical_accuracy: 1.0000 - precision: 0.9677 - recall: 0.9375 - accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-05 06:31:00.354400: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 422s 1s/step - loss: 0.3247 - top_1_categorical_accuracy: 0.8986 - top_2_categorical_accuracy: 0.9608 - top_3_categorical_accuracy: 0.9792 - top_4_categorical_accuracy: 0.9888 - top_5_categorical_accuracy: 0.9931 - top_6_categorical_accuracy: 0.9962 - top_7_categorical_accuracy: 0.9973 - top_8_categorical_accuracy: 0.9989 - top_9_categorical_accuracy: 0.9996 - precision: 0.9294 - recall: 0.8670 - accuracy: 1.9000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.32468482851982117,\n",
       " 0.8985999822616577,\n",
       " 0.9607999920845032,\n",
       " 0.979200005531311,\n",
       " 0.9887999892234802,\n",
       " 0.9930999875068665,\n",
       " 0.9962000250816345,\n",
       " 0.9973000288009644,\n",
       " 0.9988999962806702,\n",
       " 0.9995999932289124,\n",
       " 0.929360032081604,\n",
       " 0.8669999837875366,\n",
       " 0.0001900000061141327]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_so_far_2022_02_04.evaluate(x_test,y_test_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8316bf42",
   "metadata": {},
   "source": [
    "## Results:\n",
    "### Test set top-1 categorical accuracy: 0.8985999822616577"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec747b83",
   "metadata": {},
   "source": [
    "# Conclusion and next steps\n",
    "\n",
    " This is a good result, however, it appears I need to keep working and develop an algorithym that does a reasonable 1st pass broad search to narrow down the vast neural architecture and hyperparameter search space (like a random or grid search of a small number of evenly distributed samples of the search space to find hotspots and cold regions in the space), then automatically set up a  hyperband focusing on the spaces in and around these hotspots.  It does appear that a cold - start hyperband search usually takes 50 - 150 trials to find an optimum in the broad search space with a smaller dataset.This works well and is efficient, but it may be best to eliminate areas of the search space That are unlikely to hold an optimal neural architecture and limit the hyperband to spaces found on a computationally efficient first pass search to be likely to hold an optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a6dc1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
